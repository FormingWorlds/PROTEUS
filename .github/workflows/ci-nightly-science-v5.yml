name: CI - Nightly Science Validation (v5)

on:
  push:
    branches:
      - tl/test_ecosystem_v5
  workflow_dispatch:

permissions:
  contents: read
  packages: read

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: formingworlds/proteus

jobs:
  branch-nightly-coverage:
    name: Branch Nightly Coverage (Integration - v5)
    runs-on: ubuntu-latest
    timeout-minutes: 30
    container:
      image: ghcr.io/formingworlds/proteus:tl-test_ecosystem_v5
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
      options: --user root

    steps:
      - name: Checkout branch code
        uses: actions/checkout@v4
        with:
          ref: tl/test_ecosystem_v5
          fetch-depth: 0

      - name: Overlay code onto container
        run: |
          echo "Copying code over container base..."
          rsync -av --exclude='SPIDER' --exclude='socrates' --exclude='petsc' --exclude='AGNI' . /opt/proteus/
          cd /opt/proteus
          git config --global --add safe.directory /opt/proteus
          # Set Julia depot to /opt where there's more disk space
          export JULIA_DEPOT_PATH=/opt/julia_depot
          mkdir -p /opt/julia_depot
          pip install -e ".[develop]" --no-deps

      - name: Read full coverage threshold
        run: |
          cd /opt/proteus
          python - <<'PY' >> "$GITHUB_ENV"
          import pathlib
          import tomllib

          data = tomllib.loads(pathlib.Path("pyproject.toml").read_text())
          val = float(data["tool"]["coverage"]["report"]["fail_under"])
          print(f"FULL_COV_FAIL_UNDER={val}")
          PY

      - name: Download test data
        run: |
          cd /opt/proteus
          # Download all required data based on all_options.toml configuration
          # This ensures all modules (MORS, ARAGOG, AGNI, etc.) have their required data
          # Use Proteus initialization to trigger automatic data download
          python -c "
          import sys
          from pathlib import Path
          from proteus import Proteus

          # Create Proteus instance which will download data on start()
          # But we'll catch the error if data is missing and download it first
          try:
              from proteus.config import read_config_object
              from proteus.utils.data import download_sufficient_data
              config = read_config_object('input/all_options.toml')
              print('Downloading required data for all_options.toml...')
              download_sufficient_data(config, clean=False)
              print('Data download completed.')
          except Exception as e:
              print(f'Warning during data download: {e}')
              # Try to continue anyway - Proteus.start() will also try to download

          # Verify critical data exists
          import os
          fwl_data = os.environ.get('FWL_DATA', '/opt/proteus/fwl_data')
          aragog_path = Path(fwl_data) / 'interior_lookup_tables/1TPa-dK09-elec-free/MgSiO3_Wolf_Bower_2018_1TPa'
          stellar_path = Path(fwl_data) / 'stellar_evolution_tracks/Spada'

          if not aragog_path.exists():
              print(f'Warning: ARAGOG data not found at {aragog_path}')
          if not stellar_path.exists():
              print(f'Warning: Stellar tracks not found at {stellar_path}')
              # Try explicit stellar download
              try:
                  from proteus.utils.data import download_stellar_tracks
                  download_stellar_tracks('Spada')
                  print('Stellar tracks download attempted.')
              except Exception as e2:
                  print(f'Warning: Stellar tracks download failed: {e2}')
          "

      - name: Free up disk space and configure Julia
        run: |
          # Use /opt for Julia depot instead of /tmp (more space in container)
          export JULIA_DEPOT_PATH=/opt/julia_depot
          mkdir -p /opt/julia_depot
          echo "JULIA_DEPOT_PATH=/opt/julia_depot" >> $GITHUB_ENV
          # Check available space in /opt
          df -h /opt

      - name: Run integration coverage (dummy + integration and not slow)
        continue-on-error: true  # Don't fail workflow if some integration tests fail
        run: |
          cd /opt/proteus
          coverage erase
          # Explicitly include dummy suite first
          pytest tests/integration/test_integration_dummy.py \
            -v --tb=short \
            --cov=proteus \
            --cov-fail-under=0 \
            --cov-report=term-missing \
            --cov-report=xml:coverage.xml \
            --cov-report=html \
            --cov-config=pyproject.toml
          # Then full integration (excluding slow, AGNI, and albedo tests requiring external data) and append coverage
          pytest tests/integration \
            -m "integration and not slow" \
            --ignore=tests/integration/test_integration_dummy_agni.py \
            --ignore=tests/integration/test_albedo_lookup.py \
            -v --tb=short \
            --cov=proteus \
            --cov-append \
            --cov-fail-under=0 \
            --cov-report=term-missing \
            --cov-report=xml:coverage.xml \
            --cov-report=html \
            --cov-config=pyproject.toml

      - name: Run slow integration tests (standard config)
        if: always()  # Run even if previous step failed
        continue-on-error: true  # Don't fail workflow if test fails
        run: |
          cd /opt/proteus
          # Run slow integration tests (standard config with all real modules)
          # These require ARAGOG lookup data and may take 10-15 minutes
          pytest tests/integration/test_integration_std_config.py \
            -m "slow" \
            -v --tb=long \
            --cov=proteus \
            --cov-append \
            --cov-fail-under=0 \
            --cov-report=term-missing \
            --cov-report=xml:coverage.xml \
            --cov-report=html \
            --cov-config=pyproject.toml

      - name: Generate coverage JSON
        run: |
          cd /opt/proteus
          coverage json -o coverage-branch-nightly.json || true

      - name: Upload coverage XML and HTML
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: v5-branch-nightly-coverage
          path: |
            /opt/proteus/coverage.xml
            /opt/proteus/htmlcov/
            /opt/proteus/coverage-branch-nightly.json
          retention-days: 14
