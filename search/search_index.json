{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Home","text":"<p>PROTEUS (/\u02c8pro\u028ati\u0259s, PROH-tee-\u0259s) is a modular Python framework that simulates the coupled evolution of the atmospheres and interiors of rocky planets and exoplanets. Inspired by the Greek god of elusive sea change, who could change his form at will, PROTEUS is designed to be flexible and adaptable to a wide range of planetary environments. It can foretell the future, but answers only to those who are capable of asking the right questions.</p> <p> Schematic of PROTEUS components and corresponding modules. </p> <p>Quick links:</p> <ul> <li>Model description -- outline of the code and its structure</li> <li>Installation -- set up PROTEUS on your machine</li> <li>Usage -- run your first simulation</li> <li>Configuration -- customise model parameters</li> <li>Troubleshooting -- common errors and fixes</li> <li>Contributing -- how to change or contribute source code</li> <li>Team -- meet the developers</li> <li>Contact -- get in touch with the developers</li> </ul>"},{"location":"index.html#citation-and-credit","title":"Citation and credit","text":"<p>If you make use of PROTEUS, please reference scientific manuscripts outlined in Bibliography, state the code version used, and include an acknowledgement. We provide a suggested acknowledgement in the contributing page.</p>"},{"location":"CODE_OF_CONDUCT.html","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT.html#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"CODE_OF_CONDUCT.html#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the   overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or   advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email   address, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT.html#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"CODE_OF_CONDUCT.html#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"CODE_OF_CONDUCT.html#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at contact@formingworlds.space. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"CODE_OF_CONDUCT.html#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"CODE_OF_CONDUCT.html#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"CODE_OF_CONDUCT.html#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"CODE_OF_CONDUCT.html#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"CODE_OF_CONDUCT.html#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior,  harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"CODE_OF_CONDUCT.html#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"CONTRIBUTING.html","title":"Contributing guidelines","text":"<p>This page provides an overview on contributing to PROTEUS itself and the ecosystem. Anyone who makes a Pull Request to the <code>main</code> branch should read this document fully first.</p>"},{"location":"CONTRIBUTING.html#licensing-and-credit","title":"Licensing and credit","text":"<p>PROTEUS and its submodules are free software and also open source software. Roughly, this means that the users have the freedom to run, copy, distribute, study, change and improve the software. The term \"free software\" is a matter of liberty, not price or monetary value [ref].</p> <p>PROTEUS is made available under the Apache 2.0 License which permits [ref] commercial use, unlimited distribution, modification of the code elsewhere, and private use. However, there are caveats to these terms: contributors have no liability, the code has no warranty, and a trademark may not be applied [ref].</p> <p>The license only specifies how the material may be used by both developers and non-developers alike.  By default, as a work is created its copyright belongs to the person who created it [ref]. Although in some cases your employer/university may be the copyright holder of work you create [ref]. PROTEUS is not 'owned' by a single entity; the individual parts of the framework are owned by those who made them, and licensed for use and modification by others. More information can be found here and here.</p> <p>The principle purpose of PROTEUS is to generate data to make scientific conclusions and write papers. It is generally expected that the primary author of a paper is the person who contributed the most work to that project. We ask that:</p> <ol> <li>authorship is offered to the Contributors to PROTEUS based on the relevance of their work in a paper,</li> <li>appropriate credit is provided in the Acknowledgements section of the paper,</li> <li>the Maintainers are made aware of when PROTEUS results are used in a scientific paper.</li> </ol> <p>A suggested acknowledgement is:</p> <p>We thank the people who have contributed to PROTEUS and its broader ecosystem for their support and enabling the scientific outputs of this paper. PROTEUS (version XX.XX.XX) may be found online at https://github.com/FormingWorlds</p> <p> In summary: <ul> <li>you generally own all of the code you write and material you create,</li> <li>you give irrevocable permission for the code to be used under the license when distributed,</li> <li>you are requested to offer authorship and give credit in papers as appropriate. </li> </ul> <p>PROTEUS would not exist without the efforts of the wider community. Contributions from research scientists, software developers, students, and many others have made development of the current framework possible. Thank you for your interest in contributing to PROTEUS, and the immense task of simulating the lifetimes of entire planets and stars.</p> <p>\"Alone we can do do so little; together we can do so much.\" - Helen Keller</p>"},{"location":"CONTRIBUTING.html#how-do-i-contribute","title":"How do I contribute?","text":"<p>Contributing to PROTEUS is relatively straightforward. We use Git to manage the source code, and GitHub to host it online. Here is a simple workflow:</p> <ol> <li>Download the code and make sure that it runs on your machine</li> <li>Create a new 'branch' called <code>MY_BRANCH</code> using Git: <code>git checkout -b MY_BRANCH</code></li> <li>Make changes to the code as you so desire</li> <li>Add these changes to the repository: <code>git add .</code></li> <li>Commit these changes with a message: <code>git commit -m MESSAGE</code></li> <li>Push these changes to GitHub: <code>git push -u origin MY_BRANCH</code></li> <li>When you've got a neat set of changes, make a 'pull request' on GitHub here. This makes you a Contributor to the project.</li> <li>One of the Maintainers of the project will review the request.</li> <li>When ready, the changes will be merged into the <code>main</code> branch and are made live!</li> </ol> <p>A series of 'hooks' will check the syntax and validity of your code when committing. With a significant number of people contributing to the codebase, automatic checks are important for preventing programming errors, bugs, stylistic problems, and large files from being committed to the repositories [ref].</p> <p>Currently, the Maintainers of the code are: Harrison Nicholls and Tim Lichtenberg.</p>"},{"location":"CONTRIBUTING.html#development-rules","title":"Development rules","text":""},{"location":"CONTRIBUTING.html#versioning","title":"Versioning","text":"<p>PROTEUS targets Python version 3.12 and is not intended to work on earlier versions of Python. We also target Linux and MacOS as the only supported operating systems. Windows and BSD are not supported.</p> <p>The version of PROTEUS itself is defined using calendar versioning in the format <code>YY.MM.DD</code>. This is defined in the code and in the <code>pyproject.toml</code> metadata file.</p>"},{"location":"CONTRIBUTING.html#code-style","title":"Code style","text":"<ol> <li>Variables should be written in lowercase with underscores: <code>example_variable</code>.</li> <li>Functions should be written in lowercase with underscores: <code>example_function()</code>.</li> <li>Constants should be written in block capitals: <code>CONSTANT_VALUE</code>.</li> <li>Lines of code should avoid being longer than 92 characters where possible</li> <li>Functions should include a docstring where possible, describing the function's purpose and parameters.</li> <li>Indentation deeper than 3 levels should be avoided.</li> </ol>"},{"location":"CONTRIBUTING.html#large-files-output-and-input-data","title":"Large files, output, and input data","text":"<p>Large files should not be committed to the repository. This means that model results, plots, and files you create during analysis should not be be staged and committed to a PROTEUS online branch. Including these (even accidentally) in the repository will make Git operations sluggish and make version control tricky, as Git is only meant for managing text (e.g. code) files.</p> <p>You can make files/folders invisible to Git by prepending <code>nogit_</code> to their names. For example, anything in a folder called <code>nogit_analysis/</code> will be ignored by Git. Large files could then be safely placed in this folder. Model outputs are generated in the <code>output/</code> folder, which is also ignored by Git.</p> <p>PROTEUS relies on input data files that can be potentially large. These are stored outside the Git repository in Zenodo, and duplicated in the OSF. When running PROTEUS, it will automatically download the necessary input data from Zenodo (or from the OSF in case the connection to Zenodo cannot be established) and store it locally inside your FWL data folder defined by the <code>FWL_DATA</code> environment variable in your shell. Placing large files in this folder allows them to be kept on 'storage' file systems on clusters to avoid reaching your allocation limits.</p> <p>If you want to add new input data you can either update a Zenodo record or create a new one.</p> <ol> <li>Updating a record For instance, if you want to add a new stellar spectra, you will update the <code>stellar_spectra/Named</code> record on Zenodo, making sure the file you add is in a consistent format with that of the other files contained in the record.</li> <li>Creating a record If you want instead to add a new model for the absorption properties of the atmosphere, you will create a new Zenodo record named <code>spectral_files/model_name/number_of_bands</code> and upload the model files in a consistent format with that of the other spectral files used in PROTEUS. It is important for clarity and data management that the name of the records follow the organisation of the FWL data repository consistently. The next step is to update/create the python download function in PROTEUS located in <code>src/proteus/utils/data.py</code> that will download the new data. The created Zenodo record number must be provided. Note that sometimes the download function is located in a submodule (e.g. Mors, Aragog\u2026).</li> </ol> <p>Once your new data is uploaded on Zenodo, do not forget to also upload it on the OSF (and update PROTEUS with the OSF record number when necessary). Some checks have been implemented to detect new input data files but note that it might be necessary in some cases to delete your local input data folder (or the data cache in the CI) to enforce the download of the new data.</p>"},{"location":"CONTRIBUTING.html#linting","title":"Linting","text":"<p>Linting is a term for static code analysis to flag programming errors, bugs, stylistic errors and suspicious constructs [ref].</p> <p>PROTEUS uses <code>ruff</code> for linting. The linting rules are defined in <code>pyproject.toml</code>. This check are run automatically via a Github Action: codestyle.</p> <p>You can <code>ruff</code> on locally using one of these commands:</p> <pre><code>ruff check start_proteus.py  # single file\nruff check src/proteus       # directory\nruff check .                 # everything\n</code></pre> <p>If you prepend <code>--fix</code>, it can also fix some issues for you:</p> <pre><code>ruff check . --fix\n</code></pre> <p>You can also use pre-commit to automatically run <code>ruff</code> on every commit, e.g.:</p> <pre><code>pre-commit install\n</code></pre>"},{"location":"CONTRIBUTING.html#running-tests","title":"Running tests","text":"<p>PROTEUS uses pytest to run the tests on the code. Tests are important for ensuring that the code behaves as expected, and for finding bugs/errors as soon as they arise. You can read more about software testing in general here.</p> <p>Our tests are run automatically via a Github Action: CI - Fast PR Checks. You can also run the tests for yourself using the command:</p> <pre><code>pytest\n</code></pre> <p>Or to run a specific test containing a keyword:</p> <pre><code>pytest -k keyword\n</code></pre> <p>To check the 'coverage' of the tests:</p> <pre><code>coverage run -m pytest\ncoverage report  # to output to terminal\ncoverage html    # to generate html report\n</code></pre> <p>The 'coverage' of the tests describes the fraction of the code which is executed while tests are being run. However, care should also be taken to ensure that the output of the tests meets expectations.</p> <p>The test files are located in the <code>tests/</code> folder. The pytest library will automatically find functions contained within Python files in this folder, and run them as tests. A test fails if a function raises an errors or fails an assertion.</p>"},{"location":"CONTRIBUTING.html#building-the-documentation","title":"Building the documentation","text":"<p>The documentation is written in markdown, and uses mkdocs to generate the pages.</p> <p>To build the documentation for yourself:</p> <pre><code>pip install -e '.[docs]'\nmkdocs serve\n</code></pre> <p>This will generate the markdown files and serve them on a local server. You can view documentation while you edit by copy-pasting the displayed URL into your browser (e.g., <code>http://127.0.0.1:8000</code>).</p> <p>You can find the documentation source in the docs directory. If you are adding new pages, make sure to update the listing in the <code>mkdocs.yml</code> under the <code>nav</code> entry.</p> <p>The documentation is hosted on the PROTEUS framework website.</p>"},{"location":"CONTRIBUTING.html#making-a-release","title":"Making a release","text":"<p>The versioning scheme we use is CalVer, in the format <code>YY.MM.DD</code>, without a leading 'v'. This means that releases are made based on the date of the release.</p> <ol> <li>Update requirements files:</li> </ol> <pre><code>python tools/generate_requirements_txt.py\npip-compile -o requirements_full.txt pyproject.toml\n</code></pre> <ol> <li>Bump the version (<code>release</code>/<code>patch</code>) as needed</li> </ol> <pre><code>bump-my-version bump release\n# 24.08.12\n</code></pre> <ol> <li> <p>Commit and push your changes.</p> </li> <li> <p>Make a new release. Make sure to set the tag to the specified version, e.g. <code>24.08.12</code>.</p> </li> <li> <p>The upload to pypi is triggered when a release is published and handled by this workflow.</p> </li> </ol>"},{"location":"ai_usage.html","title":"AI-Assisted Development","text":""},{"location":"ai_usage.html#how-proteus-uses-ai","title":"How PROTEUS Uses AI","text":"<p>PROTEUS leverages AI assistants for software engineering tasks, not scientific content:</p> <ul> <li>Test implementation \u2014 Generating unit tests, expanding coverage, writing fixtures</li> <li>Code security \u2014 Identifying vulnerabilities, reviewing for unsafe patterns</li> <li>Code refactoring \u2014 Improving consistency, streamlining framework structure</li> <li>Automated code reviews \u2014 PR reviews via GitHub Copilot and Cursor Bugbot</li> </ul> <p>AI is not used for: Scientific algorithms, physics implementations, or research decisions. These require domain expertise and human judgment.</p>"},{"location":"ai_usage.html#what-this-document-is-for","title":"What This Document Is For","text":"<p>New to AI coding assistants? This guide explains how to use AI tools (GitHub Copilot, Cursor, Windsurf) safely and effectively with PROTEUS. AI assistants can significantly accelerate development, but require careful use to maintain code quality and security.</p> <p>Key principle: AI is a powerful tool, not a replacement for understanding. Always review AI-generated code before committing.</p>"},{"location":"ai_usage.html#quick-start","title":"Quick Start","text":"<ol> <li>Set up an AI assistant: Install GitHub Copilot, Cursor, or Windsurf</li> <li>Provide context: Point the assistant to <code>.github/copilot-instructions.md</code> (coding guidelines) and <code>.github/copilot-memory.md</code> (project state)</li> <li>Generate code: Use prompts from Test Building for tests</li> <li>Review thoroughly: Check all AI output before committing</li> <li>Run tests: <code>pytest -m \"unit and not skip\"</code> and <code>ruff check</code></li> </ol>"},{"location":"ai_usage.html#githubcopilot-instructionsmd-and-githubcopilot-memorymd","title":".github/copilot-instructions.md and .github/copilot-memory.md","text":"<p>PROTEUS uses two special files to provide AI assistants with project context:</p>"},{"location":"ai_usage.html#githubcopilot-instructionsmd-coding-guidelines","title":".github/copilot-instructions.md \u2014 Coding Guidelines","text":"<p>Purpose: Instructions for AI agents on how to write PROTEUS-compliant code. GitHub Copilot automatically discovers this file; other tools access it via the <code>CLAUDE.md</code> symlink at the project root.</p> <p>Contains: - Project structure and architecture - Coding standards and style rules - Testing requirements and markers - Build commands and validation steps - Common patterns and anti-patterns</p> <p>How to use: GitHub Copilot reads this file automatically. For other AI assistants, add it to the context window or reference it in prompts.</p>"},{"location":"ai_usage.html#githubcopilot-memorymd-project-state","title":".github/copilot-memory.md \u2014 Project State","text":"<p>Purpose: Living document capturing current project state and decisions.</p> <p>Contains: - Recent architectural decisions - Current sprint focus and priorities - Known issues and workarounds - Coverage thresholds and CI status - Lessons learned from past work</p> <p>How to use: Reference when you need context about why things are done a certain way.</p>"},{"location":"ai_usage.html#ide-setup","title":"IDE Setup","text":""},{"location":"ai_usage.html#vs-code-with-github-copilot","title":"VS Code with GitHub Copilot","text":"<ol> <li>Install: VS Code Copilot Extension</li> <li>Enable Copilot Chat: Install Copilot Chat Extension</li> <li>Add context files: In chat, use <code>@workspace</code> to reference project files, or drag <code>.github/copilot-instructions.md</code> into the chat</li> <li>Use <code>.github/copilot-instructions.md</code>: This file automatically provides Copilot with PROTEUS guidelines</li> </ol> <p>Tutorials:</p> <ul> <li>Getting Started with GitHub Copilot</li> <li>Using Copilot Chat in VS Code</li> <li>Copilot Best Practices</li> </ul> <p>Academic License: Students and educators can apply for free GitHub Copilot access: GitHub Education</p>"},{"location":"ai_usage.html#cursor","title":"Cursor","text":"<ol> <li>Install: Download from cursor.sh</li> <li>Open PROTEUS: <code>cursor /path/to/PROTEUS</code></li> <li>Add rules: Cursor reads <code>.cursorrules</code> if present; alternatively, add <code>.github/copilot-instructions.md</code> content to Settings \u2192 Rules</li> <li>Reference files: Use <code>@.github/copilot-instructions.md</code> or <code>@.github/copilot-memory.md</code> in chat to include context</li> </ol>"},{"location":"ai_usage.html#windsurf-cascade","title":"Windsurf (Cascade)","text":"<ol> <li>Install: Download from codeium.com/windsurf</li> <li>Open PROTEUS: Windsurf automatically reads <code>.github/copilot-instructions.md</code> from the workspace</li> <li>Memory system: Windsurf maintains persistent memory across sessions</li> <li>Reference files: Use <code>@file</code> mentions to include specific files in context</li> </ol>"},{"location":"ai_usage.html#ai-for-test-implementation","title":"AI for Test Implementation","text":"<p>AI assistants excel at generating tests when given proper context. PROTEUS has standardized prompts for this purpose.</p>"},{"location":"ai_usage.html#workflow","title":"Workflow","text":"<ol> <li>Open the source file you want to test</li> <li>Open <code>tests/conftest.py</code> to show available fixtures</li> <li>Use the Master Prompt from Test Building:</li> </ol> <pre><code>Act as a Senior Scientific Software Engineer for PROTEUS.\nI need robust unit tests for the open file. Follow these strict guidelines:\n- Use @pytest.mark.unit marker\n- Mock all external dependencies\n- Use pytest.approx() for float comparisons\n- Add docstrings explaining physical scenarios\n</code></pre> <ol> <li>Review the generated tests for:</li> <li>Correct markers (<code>@pytest.mark.unit</code>)</li> <li>Proper mocking (no real I/O or network calls)</li> <li>Physically valid test inputs</li> <li> <p>Clear docstrings</p> </li> <li> <p>Run and verify: <code>pytest tests/&lt;module&gt;/test_&lt;file&gt;.py -v</code></p> </li> </ol>"},{"location":"ai_usage.html#why-ai-tests-work-well","title":"Why AI + Tests Work Well","text":"<ul> <li>Repetitive patterns: Test structures are predictable; AI handles boilerplate</li> <li>Coverage expansion: AI can suggest edge cases you might miss</li> <li>Fixture awareness: AI learns your fixture patterns from <code>conftest.py</code></li> <li>Consistency: AI applies the same style across all tests</li> </ul> <p>See Test Building for detailed prompts and examples.</p>"},{"location":"ai_usage.html#ai-for-code-review","title":"AI for Code Review","text":"<p>Use AI to review your changes before pushing a PR. This catches issues early and reduces review cycles.</p>"},{"location":"ai_usage.html#local-review-workflow","title":"Local Review Workflow","text":"<ol> <li> <p>Stage your changes: <code>git add -p</code> (interactive staging)</p> </li> <li> <p>Generate a diff: <code>git diff --staged &gt; changes.diff</code></p> </li> <li> <p>Ask AI to review:    <pre><code>Review this diff for PROTEUS. Check for:\n- Style violations (should pass ruff)\n- Missing tests for new code\n- Incorrect float comparisons (must use pytest.approx)\n- Security issues (hardcoded paths, secrets)\n- Breaking changes to public APIs\n</code></pre></p> </li> <li> <p>Address feedback before committing</p> </li> </ol>"},{"location":"ai_usage.html#automated-pr-reviews-github","title":"Automated PR Reviews (GitHub)","text":"<p>PROTEUS uses automated AI reviewers on pull requests:</p> <ul> <li>GitHub Copilot \u2014 Reviews code for bugs, security issues, and style</li> <li>Cursor Bugbot \u2014 Analyzes code for potential bugs and improvements</li> </ul> <p>When you open a PR, these bots automatically comment with suggestions. Here's how to handle them:</p>"},{"location":"ai_usage.html#reviewing-bot-comments","title":"Reviewing Bot Comments","text":"<ol> <li>Read each comment carefully \u2014 Bots highlight specific lines with potential issues</li> <li>Evaluate relevance \u2014 Not all suggestions apply; use your judgment</li> <li>Check for false positives \u2014 AI may flag valid code as problematic (especially physics-specific patterns)</li> </ol>"},{"location":"ai_usage.html#responding-to-suggestions","title":"Responding to Suggestions","text":"Action When to Use Accept &amp; implement Suggestion is valid and improves code Dismiss with reason False positive; explain why in a reply Ask for clarification Unclear suggestion; reply to the bot comment Defer to reviewer Uncertain; tag a human reviewer for input"},{"location":"ai_usage.html#common-bot-suggestions","title":"Common Bot Suggestions","text":"<ul> <li>\"Consider adding error handling\" \u2014 Valid if function can fail; dismiss if errors are handled upstream</li> <li>\"Magic number detected\" \u2014 Consider using a named constant; dismiss if value is obvious (e.g., <code>0</code>, <code>1</code>)</li> <li>\"Function too complex\" \u2014 Consider refactoring; may be acceptable for physics calculations</li> <li>\"Missing docstring\" \u2014 Add docstring for public functions; internal helpers may not need one</li> <li>\"Potential security issue\" \u2014 Always investigate; err on the side of caution</li> </ul>"},{"location":"ai_usage.html#best-practices","title":"Best Practices","text":"<ul> <li>Don't ignore all suggestions \u2014 Bots catch real issues</li> <li>Don't accept all suggestions \u2014 Bots make mistakes, especially with scientific code</li> <li>Document dismissals \u2014 Reply explaining why you're not implementing a suggestion</li> <li>Batch responses \u2014 Address all bot comments before requesting human review</li> </ul>"},{"location":"ai_usage.html#what-ai-can-catch","title":"What AI Can Catch","text":"<ul> <li>Style issues: Inconsistent formatting, missing docstrings</li> <li>Common bugs: Off-by-one errors, unhandled edge cases</li> <li>Test gaps: New functions without corresponding tests</li> <li>Security issues: Hardcoded credentials, unsafe file operations</li> <li>API breaks: Changes to function signatures without migration</li> </ul>"},{"location":"ai_usage.html#what-ai-cannot-replace","title":"What AI Cannot Replace","text":"<ul> <li>Domain expertise: AI doesn't understand planetary physics</li> <li>Architectural decisions: Humans decide system design</li> <li>Security audits: Critical security requires human review</li> <li>Final approval: A human must approve all PRs</li> </ul>"},{"location":"ai_usage.html#safety-and-security","title":"Safety and Security","text":""},{"location":"ai_usage.html#critical-rules","title":"\u26a0\ufe0f Critical Rules","text":"<ol> <li>Never share secrets: Don't paste API keys, passwords, or credentials into AI prompts</li> <li>Review all output: AI can generate plausible-looking but incorrect code</li> <li>Verify physics: AI doesn't understand scientific validity\u2014check equations manually</li> <li>Check file operations: AI may suggest destructive file operations (rm, overwrite)</li> <li>Validate external calls: AI may add network requests or subprocess calls</li> </ol>"},{"location":"ai_usage.html#security-checklist","title":"Security Checklist","text":"<p>Before committing AI-generated code:</p> <ul> <li>[ ] No hardcoded paths, credentials, or secrets</li> <li>[ ] No unexpected network requests</li> <li>[ ] No file operations outside expected directories</li> <li>[ ] All tests pass (<code>pytest -m \"unit and not skip\"</code>)</li> <li>[ ] Linting passes (<code>ruff check src/ tests/</code>)</li> <li>[ ] You understand what every line does</li> </ul>"},{"location":"ai_usage.html#maintaining-code-quality","title":"Maintaining Code Quality","text":"<pre><code># Before committing AI-generated code:\nruff check src/ tests/                    # Check style\nruff format src/ tests/                   # Format code\npytest -m \"unit and not skip\"             # Run tests\nbash tools/validate_test_structure.sh     # Validate structure\ngit diff --staged                         # Review changes yourself\n</code></pre>"},{"location":"ai_usage.html#best-practices_1","title":"Best Practices","text":""},{"location":"ai_usage.html#do","title":"Do","text":"<ul> <li>Provide context: Include <code>.github/copilot-instructions.md</code> and relevant source files</li> <li>Be specific: \"Write a unit test for <code>calculate_flux</code> that tests edge case when T=0\"</li> <li>Iterate: Ask AI to refine based on your feedback</li> <li>Learn from output: Use AI suggestions to improve your understanding</li> <li>Attribute appropriately: Note significant AI contributions in commit messages if relevant</li> </ul>"},{"location":"ai_usage.html#dont","title":"Don't","text":"<ul> <li>Blindly accept: Never commit without understanding the code</li> <li>Skip tests: AI-generated code needs testing like any other code</li> <li>Ignore warnings: If AI says \"this might need adjustment,\" investigate</li> <li>Share sensitive data: Keep credentials and private data out of prompts</li> <li>Over-rely: AI is a tool, not a substitute for expertise</li> </ul>"},{"location":"ai_usage.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"ai_usage.html#ai-generates-incorrect-markers","title":"AI generates incorrect markers","text":"<p>Problem: AI uses <code>@pytest.mark.test</code> instead of <code>@pytest.mark.unit</code></p> <p>Solution: Include <code>.github/copilot-instructions.md</code> in context; it specifies valid markers</p>"},{"location":"ai_usage.html#ai-doesnt-know-about-fixtures","title":"AI doesn't know about fixtures","text":"<p>Problem: AI creates fixtures that already exist in <code>conftest.py</code></p> <p>Solution: Always include <code>tests/conftest.py</code> in the context window</p>"},{"location":"ai_usage.html#ai-suggests-outdated-patterns","title":"AI suggests outdated patterns","text":"<p>Problem: AI uses deprecated APIs or old coding patterns</p> <p>Solution: Reference <code>.github/copilot-memory.md</code> for current patterns; specify Python version (3.12)</p>"},{"location":"ai_usage.html#ai-generates-code-that-fails-ci","title":"AI generates code that fails CI","text":"<p>Problem: Generated code passes locally but fails in CI</p> <p>Solution: Run full pre-commit checklist before pushing: <pre><code>ruff check src/ tests/ &amp;&amp; ruff format src/ tests/\npytest -m \"unit and not skip\"\nbash tools/validate_test_structure.sh\n</code></pre></p>"},{"location":"ai_usage.html#references","title":"References","text":"<ul> <li>.github/copilot-instructions.md \u2014 AI coding guidelines for PROTEUS</li> <li>.github/copilot-memory.md \u2014 Project state and decisions</li> <li>Test Building \u2014 Test generation prompts</li> <li>Test Categorization \u2014 Test markers and CI</li> <li>Test Infrastructure \u2014 Coverage and workflows</li> <li>GitHub Copilot Documentation</li> <li>GitHub Education (Academic License)</li> </ul>"},{"location":"bibliography.html","title":"Bibliography","text":"<p>For a full list of papers that use PROTEUS, see the PROTEUS bibliography on SciX.</p> <p>Works describing and further developing PROTEUS</p> <ul> <li>Nicholls et al. (2025a, Monthly Notices of the Royal Astronomical Society) \u2013\u2013 doi:10.1093/mnras/stae2772 \u2013\u2013 arXiv PDF</li> <li>Nicholls et al. (2024, Journal of Geophysical Research: Planets) \u2013\u2013 doi:10.1029/2024JE008576 \u2013\u2013 arXiv PDF</li> <li>Lichtenberg et al. (2021, Journal of Geophysical Research: Planets) \u2013\u2013 doi:10.1029/2020JE006711 \u2013\u2013 arXiv PDF</li> </ul> <pre><code>@ARTICLE{Nicholls_2025_MNRAS,\n       author = {{Nicholls}, Harrison and {Pierrehumbert}, Raymond T. and {Lichtenberg}, Tim and {Soucasse}, Laurent and {Smeets}, Stef},\n        title = \"{Convective shutdown in the atmospheres of lava worlds}\",\n      journal = {\\mnras},\n     keywords = {Astrophysics - Earth and Planetary Astrophysics},\n         year = 2025,\n        month = jan,\n       volume = {536},\n       number = {3},\n        pages = {2957-2971},\n          doi = {10.1093/mnras/stae2772},\narchivePrefix = {arXiv},\n       eprint = {2412.11987},\n primaryClass = {astro-ph.EP},\n       adsurl = {https://ui.adsabs.harvard.edu/abs/2025MNRAS.536.2957N},\n      adsnote = {Provided by the SAO/NASA Astrophysics Data System}\n}\n\n@ARTICLE{Nicholls_2024_JGRP,\n       author = {{Nicholls}, Harrison and {Lichtenberg}, Tim and {Bower}, Dan J. and {Pierrehumbert}, Raymond},\n        title = \"{Magma Ocean Evolution at Arbitrary Redox State}\",\n      journal = {Journal of Geophysical Research (Planets)},\n     keywords = {magma oceans, lava planets, exoplanets, atmospheres, simulation, convection, Astrophysics - Earth and Planetary Astrophysics},\n         year = 2024,\n        month = dec,\n       volume = {129},\n       number = {12},\n        pages = {2024JE008576},\n          doi = {10.1029/2024JE008576},\narchivePrefix = {arXiv},\n       eprint = {2411.19137},\n primaryClass = {astro-ph.EP},\n       adsurl = {https://ui.adsabs.harvard.edu/abs/2024JGRE..12908576N},\n      adsnote = {Provided by the SAO/NASA Astrophysics Data System}\n}\n\n@ARTICLE{Lichtenberg_2021_JGRP,\n       author = {{Lichtenberg}, Tim and {Bower}, Dan J. and {Hammond}, Mark and {Boukrouche}, Ryan and {Sanan}, Patrick and {Tsai}, Shang-Min and {Pierrehumbert}, Raymond T.},\n        title = \"{Vertically Resolved Magma Ocean-Protoatmosphere Evolution: H$_{2}$, H$_{2}$O, CO$_{2}$, CH$_{4}$, CO, O$_{2}$, and N$_{2}$ as Primary Absorbers}\",\n      journal = {Journal of Geophysical Research (Planets)},\n     keywords = {Atmosphere origins, exoplanets, magma oceans, planet composition, planet formation and evolution, planetary surface, Astrophysics - Earth and Planetary Astrophysics, Physics - Atmospheric and Oceanic Physics, Physics - Geophysics},\n         year = 2021,\n        month = feb,\n       volume = {126},\n       number = {2},\n          eid = {e06711},\n        pages = {e06711},\n          doi = {10.1029/2020JE006711},\narchivePrefix = {arXiv},\n       eprint = {2101.10991},\n primaryClass = {astro-ph.EP},\n       adsurl = {https://ui.adsabs.harvard.edu/abs/2021JGRE..12606711L},\n      adsnote = {Provided by the SAO/NASA Astrophysics Data System}\n}\n</code></pre>"},{"location":"config.html","title":"Configuration file","text":"<p>PROTEUS uses TOML to structure its configuration files.</p> <p>All of the parameters required to run the model are listed below with short explanations of their purpose and the values they accept. Configuration files can contain blank lines. Comments are indicated with a <code>#</code> symbol. Whitespace indentation is purely stylistic.</p> <p>Many of the parameters have default values, meaning that you do not have to provide them in the file. Some parameters are conditionally required. For example, if you use the <code>mors</code> stellar evolution module (i.e. <code>star.module == 'mors'</code>), then you are required to also set the variable <code>star.mors.age_now</code>. However, if you instead decided to use the <code>dummy</code> stellar evolution module then the <code>age_now</code> parameter is not required.</p> <p>See the <code>default.toml</code> configuration for a comprehensive example of all possible parameters.</p>"},{"location":"config.html#examples","title":"Examples","text":"<p>Have a look at the input configs for ideas of how to set up your config in practice.</p>"},{"location":"config.html#root-parameters","title":"Root parameters","text":""},{"location":"config.html#proteus.config._config.Config","title":"<code>Config</code>","text":"<p>Root config parameters.</p> <p>Attributes:</p> <ul> <li> <code>version</code>               (<code>str</code>)           \u2013            <p>Version of the configuration file.</p> </li> <li> <code>params</code>               (<code>Params</code>)           \u2013            <p>Parameters for code execution, output files, time-stepping, convergence.</p> </li> <li> <code>star</code>               (<code>Star</code>)           \u2013            <p>Stellar parameters, model selection.</p> </li> <li> <code>orbit</code>               (<code>Orbit</code>)           \u2013            <p>Orbital and star-system parameters.</p> </li> <li> <code>struct</code>               (<code>Struct</code>)           \u2013            <p>Planetary structure calculation (mass, radius).</p> </li> <li> <code>atmos_clim</code>               (<code>AtmosClim</code>)           \u2013            <p>Planetary atmosphere climate parameters, model selection.</p> </li> <li> <code>atmos_chem</code>               (<code>AtmosChem</code>)           \u2013            <p>Planetary atmosphere chemistry parameters, model selection.</p> </li> <li> <code>escape</code>               (<code>Escape</code>)           \u2013            <p>Atmospheric escape parameters, model selection.</p> </li> <li> <code>interior</code>               (<code>Interior</code>)           \u2013            <p>Magma ocean / mantle model parameters, model selection.</p> </li> <li> <code>outgas</code>               (<code>Outgas</code>)           \u2013            <p>Outgassing parameters (fO2, etc) and included volatiles.</p> </li> <li> <code>delivery</code>               (<code>Delivery</code>)           \u2013            <p>Initial volatile inventory, and delivery model selection.</p> </li> <li> <code>observe</code>               (<code>Observe</code>)           \u2013            <p>Synthetic observations.</p> </li> </ul>"},{"location":"config.html#proteus.config._config.Config.write","title":"<code>write(out)</code>","text":"<p>Write configuration to a new TOML file.</p> Source code in <code>src/proteus/config/_config.py</code> <pre><code>def write(self, out: str):\n    \"\"\"\n    Write configuration to a new TOML file.\n    \"\"\"\n\n    # Convert to dictionary\n    cfg = dict(asdict(self))\n\n    # Replace None with \"none\"\n    cfg = dict_replace_none(cfg)\n\n    # Write to TOML file\n    with open(out, 'w') as hdl:\n        tomlkit.dump(cfg, hdl)\n</code></pre>"},{"location":"config.html#general-parameters","title":"General parameters","text":"<p>This module describes the parameters for the data location, data output, and logging. It also defines stopping criteria.</p>"},{"location":"config.html#proteus.config._params.OutputParams","title":"<code>OutputParams</code>","text":"<p>Parameters for output files and logging</p> <p>Attributes:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>Path to output folder relative to <code>PROTEUS/output/</code>.</p> </li> <li> <code>logging</code>               (<code>str</code>)           \u2013            <p>Log verbosity. Choices: 'INFO', 'DEBUG', 'ERROR', 'WARNING'.</p> </li> <li> <code>plot_fmt</code>               (<code>str</code>)           \u2013            <p>Plotting output file format. Choices: \"png\", \"pdf\".</p> </li> <li> <code>write_mod</code>               (<code>int</code>)           \u2013            <p>Write CSV frequency. 0: wait until completion. n: every n iterations.</p> </li> <li> <code>plot_mod</code>               (<code>int | None</code>)           \u2013            <p>Plotting frequency. 0: wait until completion. n: every n iterations. None: never plot.</p> </li> <li> <code>archive_mod</code>               (<code>int | None</code>)           \u2013            <p>Archive frequency. 0: wait until completion. n: every n iterations. None: never archive.</p> </li> <li> <code>remove_sf</code>               (<code>bool</code>)           \u2013            <p>Remove SOCRATES spectral files after model terminates.</p> </li> </ul>"},{"location":"config.html#proteus.config._params.DtProportional","title":"<code>DtProportional</code>","text":"<p>Parameters used to configure the proportional time-stepping scheme.</p> <p>Attributes:</p> <ul> <li> <code>propconst</code>               (<code>float</code>)           \u2013            <p>Proportionality constant.</p> </li> </ul>"},{"location":"config.html#proteus.config._params.DtAdaptive","title":"<code>DtAdaptive</code>","text":"<p>Parameters used to configure the adaptive time-stepping scheme.</p> <p>Attributes:</p> <ul> <li> <code>atol</code>               (<code>float</code>)           \u2013            <p>Absolute tolerance on time-step size [yr].</p> </li> <li> <code>rtol</code>               (<code>float</code>)           \u2013            <p>Relative tolerance on time-step size [dimensionless].</p> </li> </ul>"},{"location":"config.html#proteus.config._params.TimeStepParams","title":"<code>TimeStepParams</code>","text":"<p>Parameters for time-stepping parameters</p> <p>Attributes:</p> <ul> <li> <code>minimum</code>               (<code>float</code>)           \u2013            <p>Minimum absolute time-step size [yr].</p> </li> <li> <code>minimum_rel</code>               (<code>float</code>)           \u2013            <p>Minimum relative time-step size [dimensionless].</p> </li> <li> <code>maximum</code>               (<code>float</code>)           \u2013            <p>Maximum time-step size [yr].</p> </li> <li> <code>initial</code>               (<code>float</code>)           \u2013            <p>Initial time-step size [yr].</p> </li> <li> <code>starspec</code>               (<code>float</code>)           \u2013            <p>Maximum interval at which to recalculate the stellar spectrum [yr].</p> </li> <li> <code>starinst</code>               (<code>float</code>)           \u2013            <p>Maximum interval at which to recalculate instellation flux [yr].</p> </li> <li> <code>method</code>               (<code>str</code>)           \u2013            <p>Time-stepping method. Choices: 'proportional', 'adaptive', 'maximum'.</p> </li> <li> <code>proportional</code>               (<code>DtProportional</code>)           \u2013            <p>Parameters used to configure the proportional time-stepping scheme.</p> </li> <li> <code>adaptive</code>               (<code>DtAdaptive</code>)           \u2013            <p>Parameters used to configure the adaptive time-stepping scheme.</p> </li> </ul>"},{"location":"config.html#proteus.config._params.StopIters","title":"<code>StopIters</code>","text":"<p>Parameters for iteration number criteria.</p> <p>Attributes:</p> <ul> <li> <code>enabled</code>               (<code>bool</code>)           \u2013            <p>Enable criteria if True</p> </li> <li> <code>minimum</code>               (<code>int</code>)           \u2013            <p>Minimum number of iterations.</p> </li> <li> <code>maximum</code>               (<code>int</code>)           \u2013            <p>Maximum number of iterations.</p> </li> </ul>"},{"location":"config.html#proteus.config._params.StopTime","title":"<code>StopTime</code>","text":"<p>Parameters for maximum time criteria.</p> <p>Attributes:</p> <ul> <li> <code>enabled</code>               (<code>bool</code>)           \u2013            <p>Enable criteria if True</p> </li> <li> <code>minimum</code>               (<code>float</code>)           \u2013            <p>Model will absolutely not terminate until at least this time is reached [yr].</p> </li> <li> <code>maximum</code>               (<code>float</code>)           \u2013            <p>Model will terminate when this time is reached [yr].</p> </li> </ul>"},{"location":"config.html#proteus.config._params.StopSolid","title":"<code>StopSolid</code>","text":"<p>Parameters for solidification criteria.</p> <p>Attributes:</p> <ul> <li> <code>enabled</code>               (<code>bool</code>)           \u2013            <p>Enable criteria if True.</p> </li> <li> <code>phi_crit</code>               (<code>float</code>)           \u2013            <p>Model will terminate when global melt fraction is less than this value [dimensionless].</p> </li> </ul>"},{"location":"config.html#proteus.config._params.StopRadeqm","title":"<code>StopRadeqm</code>","text":"<p>Parameters for radiative equilibrium stopping criteria.</p> <p>Attributes:</p> <ul> <li> <code>enabled</code>               (<code>bool</code>)           \u2013            <p>Enable criteria if True</p> </li> <li> <code>atol</code>               (<code>float</code>)           \u2013            <p>Absolute tolerance on energy balance [W m-2].</p> </li> <li> <code>rtol</code>               (<code>float</code>)           \u2013            <p>Relative tolerance on energy balance.</p> </li> </ul>"},{"location":"config.html#proteus.config._params.StopEscape","title":"<code>StopEscape</code>","text":"<p>Parameters for escape stopping criteria.</p> <p>Attributes:</p> <ul> <li> <code>enabled</code>               (<code>bool</code>)           \u2013            <p>Enable criteria if True</p> </li> <li> <code>p_stop</code>               (<code>float</code>)           \u2013            <p>Model will terminate when surface pressure is less than this value [bar].</p> </li> </ul>"},{"location":"config.html#proteus.config._params.StopDisint","title":"<code>StopDisint</code>","text":"<p>Parameters for planet disintegration stopping criteria.</p> <p>Attributes:</p> <ul> <li> <code>enabled</code>               (<code>bool</code>)           \u2013            <p>Enable all planet disintegration criteria if True</p> </li> <li> <code>roche_enabled</code>               (<code>bool</code>)           \u2013            <p>Disable Roche limit criterion</p> </li> <li> <code>offset_roche</code>               (<code>float</code>)           \u2013            <p>Absolute correction (+/-) to (increase/decrease) calculated Roche limit [m].</p> </li> <li> <code>spin_enabled</code>               (<code>bool</code>)           \u2013            <p>Disable Breakup period criterion</p> </li> <li> <code>offset_spin</code>               (<code>float</code>)           \u2013            <p>Absolute correction (+/-) to (increase/decrease) calculated Breakup period [s].</p> </li> </ul>"},{"location":"config.html#proteus.config._params.StopParams","title":"<code>StopParams</code>","text":"<p>Parameters for termination criteria.</p> <p>Attributes:</p> <ul> <li> <code>strict</code>               (<code>bool</code>)           \u2013            <p>Require termination criteria to be satisfied twice before the model exits.</p> </li> <li> <code>iters</code>               (<code>StopIters</code>)           \u2013            <p>Parameters for iteration number criteria.</p> </li> <li> <code>time</code>               (<code>StopTime</code>)           \u2013            <p>Parameters for maximum time criteria.</p> </li> <li> <code>solid</code>               (<code>StopSolid</code>)           \u2013            <p>Parameters for solidification criteria.</p> </li> <li> <code>radeqm</code>               (<code>StopRadeqm</code>)           \u2013            <p>Parameters for radiative equilibrium criteria.</p> </li> <li> <code>escape</code>               (<code>StopEscape</code>)           \u2013            <p>Parameters for escape criteria.</p> </li> <li> <code>disint</code>               (<code>StopDisint</code>)           \u2013            <p>Parameters for planet disintegration criteria.</p> </li> </ul>"},{"location":"config.html#proteus.config._params.Params","title":"<code>Params</code>","text":"<p>Parameters for code execution, output files, time-stepping, convergence.</p> <p>Attributes:</p> <ul> <li> <code>out</code>               (<code>OutputParams</code>)           \u2013            <p>Parameters for data / logging output.</p> </li> <li> <code>dt</code>               (<code>TimeStepParams</code>)           \u2013            <p>Parameters for time-stepping.</p> </li> <li> <code>stop</code>               (<code>StopParams</code>)           \u2013            <p>Parameters for stopping criteria.</p> </li> </ul>"},{"location":"config.html#stellar-evolution","title":"Stellar evolution","text":""},{"location":"config.html#proteus.config._star.Mors","title":"<code>Mors</code>","text":"<p>Module parameters for MORS module.</p> <p>Attributes:</p> <ul> <li> <code>rot_pcntle</code>               (<code>float</code>)           \u2013            <p>Rotation, as percentile of stellar population.</p> </li> <li> <code>rot_period</code>               (<code>float</code>)           \u2013            <p>Rotation rate [days].</p> </li> <li> <code>tracks</code>               (<code>str</code>)           \u2013            <p>Stellar evolution track to be used. Choices: 'spada', 'baraffe'.</p> </li> <li> <code>age_now</code>               (<code>float</code>)           \u2013            <p>Observed estimated age of the star [Gyr].</p> </li> <li> <code>star_name</code>               (<code>str</code>)           \u2013            <p>Name of the star, to find appropriate stellar spectrum. See documentation.</p> </li> <li> <code>star_path</code>               (<code>str</code>)           \u2013            <p>Path to custom stellar spectra. If 'none', star_name will be used to find spectra in default locations.</p> </li> <li> <code>spectrum_source</code>               (<code>str</code>)           \u2013            <p>Source of stellar spectra. Choices: 'solar', 'muscles', 'phoenix', 'none'.</p> </li> <li> <code>phoenix_FeH</code>               (<code>float</code>)           \u2013            <p>Stellar metallicity [Fe/H] to be used for PHOENIX synthetic spectra, if spectrum_source is 'phoenix'.</p> </li> <li> <code>phoenix_alpha</code>               (<code>float</code>)           \u2013            <p>Alpha-element enhancement [alpha/Fe] to be used for PHOENIX synthetic spectra, if spectrum_source is 'phoenix'.</p> </li> <li> <code>phoenix_radius</code>               (<code>float</code>)           \u2013            <p>Stellar radius [R_sun]. If 'none', radius will be calculated using mors' stellar tracks, if spectrum_source is 'phoenix'.</p> </li> <li> <code>phoenix_log_g</code>               (<code>float</code>)           \u2013            <p>Surface gravity [cgs]. If 'none', log g will be calculated will be calculated using mors' stellar tracks, if spectrum_source is 'phoenix'.</p> </li> <li> <code>phoenix_Teff</code>               (<code>float</code>)           \u2013            <p>Effective temperature [K]. If 'none', Teff will be calculated will be calculated using mors' stellar tracks, if spectrum_source is 'phoenix'.</p> </li> </ul>"},{"location":"config.html#proteus.config._star.StarDummy","title":"<code>StarDummy</code>","text":"<p>Dummy star module.</p> <p>Attributes:</p> <ul> <li> <code>radius</code>               (<code>float</code>)           \u2013            <p>Observed radius [R_sun].</p> </li> <li> <code>calculate_radius</code>               (<code>bool</code>)           \u2013            <p>Calculate the radius using empirical mass-luminosity and mass-radius relation</p> </li> <li> <code>Teff</code>               (<code>float</code>)           \u2013            <p>Observed effective temperature [K].</p> </li> </ul>"},{"location":"config.html#proteus.config._star.Star","title":"<code>Star</code>","text":"<p>Stellar parameters, model selection.</p> <p>You can find useful reference data in the documentation.</p> <p>Attributes:</p> <ul> <li> <code>bol_scale</code>               (<code>float</code>)           \u2013            <p>Scale factor to increase the luminosity.</p> </li> <li> <code>mass</code>               (<code>float</code>)           \u2013            <p>Stellar mass [M_sun]. Note that for Mors, it should be between 0.1 and 1.25 solar masses. Values outside of the valid range will be clipped.</p> </li> <li> <code>age_ini</code>               (<code>float</code>)           \u2013            <p>Age of system at model initialisation [Gyr].</p> </li> <li> <code>module</code>               (<code>str | None</code>)           \u2013            <p>Select star module to use.</p> </li> <li> <code>mors</code>               (<code>Mors</code>)           \u2013            <p>Parameters for MORS module.</p> </li> <li> <code>dummy</code>               (<code>StarDummy</code>)           \u2013            <p>Parameters for the dummy star module</p> </li> </ul>"},{"location":"config.html#orbital-evolution-and-tides","title":"Orbital evolution and tides","text":""},{"location":"config.html#proteus.config._orbit.OrbitDummy","title":"<code>OrbitDummy</code>","text":"<p>Dummy orbit/tidal heating module.</p> <p>Uses a fixed tidal heating power density and love number.</p> <p>Attributes:</p> <ul> <li> <code>H_tide</code>               (<code>float</code>)           \u2013            <p>Fixed global heating rate from tides [W kg-1].</p> </li> <li> <code>Phi_tide</code>               (<code>str</code>)           \u2013            <p>Inequality which, if locally true, determines in which regions tides are applied.</p> </li> <li> <code>Imk2</code>               (<code>float</code>)           \u2013            <p>Imaginary part of k2 Love number, which is usually negative.</p> </li> </ul>"},{"location":"config.html#proteus.config._orbit.Lovepy","title":"<code>Lovepy</code>","text":"<p>Lovepy tides module.</p> <p>Attributes:</p> <ul> <li> <code>visc_thresh</code>               (<code>float</code>)           \u2013            <p>Minimum viscosity required for heating [Pa s].</p> </li> <li> <code>ncalc</code>               (<code>int</code>)           \u2013            <p>Number of interpoltaed interior levels to use for solving tidal heating rates.</p> </li> </ul>"},{"location":"config.html#proteus.config._orbit.Orbit","title":"<code>Orbit</code>","text":"<p>Planetary and satellite orbital parameters.</p> <p>Includes initial conditions, and options for enabling dynamical evolution.</p> <p>Attributes:</p> <ul> <li> <code>semimajoraxis</code>               (<code>float</code>)           \u2013            <p>Initial semi-major axis of the planet's orbit [AU].</p> </li> <li> <code>eccentricity</code>               (<code>float</code>)           \u2013            <p>Initial Eccentricity of the planet's orbit.</p> </li> <li> <code>instellation_method</code>               (<code>str</code>)           \u2013            <p>Whether to use the semi-major axis ('sma') or instellation flux ('inst') to define the planet's initial orbit</p> </li> <li> <code>instellationflux</code>               (<code>float</code>)           \u2013            <p>Instellation flux initially received by the planet in Earth units.</p> </li> <li> <code>zenith_angle</code>               (<code>float</code>)           \u2013            <p>Characteristic angle of incoming stellar radiation, relative to the zenith [deg].</p> </li> <li> <code>s0_factor</code>               (<code>float</code>)           \u2013            <p>Scale factor applies to incoming stellar radiation to represent planetary rotation.</p> </li> <li> <code>evolve</code>               (<code>bool</code>)           \u2013            <p>Allow the planet's orbit to evolve based on eccentricity tides?</p> </li> <li> <code>axial_period</code>               (<code>float | None</code>)           \u2013            <p>Planet initial day length [hours], will use orbital period if value is None.</p> </li> <li> <code>satellite</code>               (<code>bool</code>)           \u2013            <p>Model a satellite (moon) orbiting the planet and solve for its orbit?</p> </li> <li> <code>semimajoraxis_sat</code>               (<code>float</code>)           \u2013            <p>Satellit initial semi-major axis  [m]</p> </li> <li> <code>module</code>               (<code>str | None</code>)           \u2013            <p>Select orbit module to use. Choices: 'none', 'dummy', 'lovepy'.</p> </li> </ul>"},{"location":"config.html#interior-structure","title":"Interior structure","text":""},{"location":"config.html#proteus.config._struct.Zalmoxis","title":"<code>Zalmoxis</code>","text":"<p>Parameters for Zalmoxis module.</p> <p>Attributes:</p> <ul> <li> <code>EOSchoice</code>               (<code>str</code>)           \u2013            <p>EOS choice of Zalmoxis. Choices: \"Tabulated:iron/silicate\", \"Tabulated:iron/Tdep_silicate\", \"Tabulated:water\".</p> </li> <li> <code>coremassfrac</code>               (<code>float</code>)           \u2013            <p>Fraction of the planet's interior mass corresponding to the core.</p> </li> <li> <code>mantle_mass_fraction</code>               (<code>float</code>)           \u2013            <p>Fraction of the planet's interior mass corresponding to the mantle (needed for modeling more than 2 layers).</p> </li> <li> <code>weight_iron_frac</code>               (<code>float</code>)           \u2013            <p>Fraction of the planet's mass that is iron.</p> </li> <li> <code>temperature_mode</code>               (<code>str</code>)           \u2013            <p>Choice of input temperature profile: \"isothermal\", \"linear\", \"prescribed\".</p> </li> <li> <code>surface_temperature</code>               (<code>float</code>)           \u2013            <p>Surface temperature (K), required for temperature_mode=\"isothermal\" or \"linear\", ignored otherwise.</p> </li> <li> <code>center_temperature</code>               (<code>float</code>)           \u2013            <p>Center temperature (K), required for temperature_mode=\"linear\", ignored otherwise.</p> </li> <li> <code>temperature_profile_file</code>               (<code>Optional[str]</code>)           \u2013            <p>Filename containing a prescribed temperature profile, required for temperature_mode=\"prescribed\".</p> </li> <li> <code>num_levels</code>               (<code>int</code>)           \u2013            <p>Number of Zalmoxis radius layers.</p> </li> <li> <code>max_iterations_outer</code>               (<code>int</code>)           \u2013            <p>Maximum number of iterations for the outer loop.</p> </li> <li> <code>tolerance_outer</code>               (<code>float</code>)           \u2013            <p>Convergence tolerance for the outer loop [kg].</p> </li> <li> <code>max_iterations_inner</code>               (<code>int</code>)           \u2013            <p>Maximum number of iterations for the inner loop.</p> </li> <li> <code>tolerance_inner</code>               (<code>float</code>)           \u2013            <p>Convergence tolerance for the inner loop [kg/m^3].</p> </li> <li> <code>relative_tolerance</code>               (<code>float</code>)           \u2013            <p>Relative tolerance for solve_ivp.</p> </li> <li> <code>absolute_tolerance</code>               (<code>float</code>)           \u2013            <p>Absolute tolerance for solve_ivp.</p> </li> <li> <code>maximum_step</code>               (<code>float</code>)           \u2013            <p>Maximum integration step size for solve_ivp (m).</p> </li> <li> <code>adaptive_radial_fraction</code>               (<code>float</code>)           \u2013            <p>Fraction (0\u20131) of the radial domain defining where solve_ivp transitions from adaptive integration to fixed-step integration when using the temperature-dependent <code>\"Tabulated:iron/Tdep_silicate\"</code> EOS.</p> </li> <li> <code>max_center_pressure_guess</code>               (<code>float</code>)           \u2013            <p>Maximum pressure guess at the center of the planet based on the \"Tabulated:iron/Tdep_silicate\" EOS files (Pa).</p> </li> <li> <code>target_surface_pressure</code>               (<code>float</code>)           \u2013            <p>Target surface pressure for the pressure adjustment [Pa].</p> </li> <li> <code>pressure_tolerance</code>               (<code>float</code>)           \u2013            <p>Convergence tolerance for the pressure adjustment [Pa].</p> </li> <li> <code>max_iterations_pressure</code>               (<code>int</code>)           \u2013            <p>Maximum number of iterations for the pressure adjustment.</p> </li> <li> <code>pressure_adjustment_factor</code>               (<code>float</code>)           \u2013            <p>Reduction factor for adjusting the pressure in the pressure adjustment.</p> </li> <li> <code>verbose</code>               (<code>bool</code>)           \u2013            <p>If true, logs detailed convergence info and warnings; if false, only essential messages are shown (true/false).</p> </li> <li> <code>iteration_profiles_enabled</code>               (<code>bool</code>)           \u2013            <p>If true, writes pressure and density profiles for each iteration to files (true/false).</p> </li> </ul>"},{"location":"config.html#proteus.config._struct.Struct","title":"<code>Struct</code>","text":"<p>Planetary structure (mass, radius).</p> <p>Attributes:</p> <ul> <li> <code>corefrac</code>               (<code>float</code>)           \u2013            <p>Fraction of the planet's interior radius corresponding to the core.</p> </li> <li> <code>module</code>               (<code>str</code>)           \u2013            <p>Module for solving the planet's interior structure. Choices: 'self', 'zalmoxis'.</p> </li> <li> <code>zalmoxis</code>               (<code>Zalmoxis or None</code>)           \u2013            <p>Zalmoxis parameters if module is 'zalmoxis'.</p> </li> <li> <code>mass_tot</code>               (<code>float</code>)           \u2013            <p>Total mass of the planet [M_earth]</p> </li> <li> <code>radius_int</code>               (<code>float</code>)           \u2013            <p>Radius of the atmosphere-mantle boundary [R_earth]</p> </li> <li> <code>core_density</code>               (<code>float</code>)           \u2013            <p>Density of the planet's core [kg m-3]</p> </li> <li> <code>core_heatcap</code>               (<code>float</code>)           \u2013            <p>Specific heat capacity of the planet's core [J kg-1 K-1]</p> </li> <li> <code>module</code>               (<code>str</code>)           \u2013            <p>Module for solving the planet's interior structure. Choices: 'self', 'zalmoxis'.</p> </li> </ul>"},{"location":"config.html#proteus.config._struct.Struct.set_by","title":"<code>set_by</code>  <code>property</code>","text":"<p>How is the structure set?</p>"},{"location":"config.html#magma-ocean-and-planetary-interior","title":"Magma ocean and planetary interior","text":""},{"location":"config.html#proteus.config._interior.Spider","title":"<code>Spider</code>","text":"<p>Parameters for SPIDER module.</p> <p>Attributes:</p> <ul> <li> <code>num_levels</code>               (<code>int</code>)           \u2013            <p>Number of SPIDER grid levels.</p> </li> <li> <code>mixing_length</code>               (<code>int</code>)           \u2013            <p>Parameterisation used to determine convective mixing length.</p> </li> <li> <code>tolerance</code>               (<code>float</code>)           \u2013            <p>Absolute solver tolerance.</p> </li> <li> <code>tolerance_rel</code>               (<code>float</code>)           \u2013            <p>Relative solver tolerance.</p> </li> <li> <code>tsurf_atol</code>               (<code>float</code>)           \u2013            <p>Absolute tolerance on change in T_mantle during a single interior iteration.</p> </li> <li> <code>tsurf_rtol</code>               (<code>float</code>)           \u2013            <p>Relative tolerance on change in T_mantle during a single interior iteration.</p> </li> <li> <code>ini_entropy</code>               (<code>float</code>)           \u2013            <p>Initial specific surface entropy [J K-1 kg-1].</p> </li> <li> <code>ini_dsdr</code>               (<code>float</code>)           \u2013            <p>Initial interior specific entropy gradient [J K-1 kg-1 m-1].</p> </li> <li> <code>solver_type</code>               (<code>str</code>)           \u2013            <p>Numerical integrator. Choices: 'adams', 'bdf'.</p> </li> <li> <code>conduction</code>               (<code>bool</code>)           \u2013            <p>Whether to include conductive heat flux in the model.</p> </li> <li> <code>convection</code>               (<code>bool</code>)           \u2013            <p>Whether to include convective heat flux in the model.</p> </li> <li> <code>gravitational_separation</code>               (<code>bool</code>)           \u2013            <p>Whether to include gravitational separation flux in the model.</p> </li> <li> <code>mixing</code>               (<code>bool</code>)           \u2013            <p>Whether to include mixing flux in the model.</p> </li> <li> <code>matprop_smooth_width</code>               (<code>float</code>)           \u2013            <p>Window width, in melt-fraction, for smoothing properties across liquidus and solidus</p> </li> </ul>"},{"location":"config.html#proteus.config._interior.Aragog","title":"<code>Aragog</code>","text":"<p>Parameters for Aragog module.</p> <p>Attributes:</p> <ul> <li> <code>logging</code>               (<code>str</code>)           \u2013            <p>Log verbosity of Aragog. Choices: 'INFO', 'DEBUG', 'ERROR', 'WARNING'.</p> </li> <li> <code>num_levels</code>               (<code>int</code>)           \u2013            <p>Number of Aragog grid levels (basic mesh).</p> </li> <li> <code>initial_condition</code>               (<code>int</code>)           \u2013            <p>How to define the intial temperature profile (1: linear, 2: user defined, 3: adiabat)</p> </li> <li> <code>tolerance</code>               (<code>float</code>)           \u2013            <p>Solver tolerance.</p> </li> <li> <code>ini_tmagma</code>               (<code>float</code>)           \u2013            <p>Initial magma surface temperature [K].</p> </li> <li> <code>basal_temperature</code>               (<code>float</code>)           \u2013            <p>Temperature at the base of the mantle (if using a linear temperature profile to start)</p> </li> <li> <code>init_file</code>               (<code>str</code>)           \u2013            <p>File containing the initial temperature file for aragog</p> </li> <li> <code>inner_boundary_condition</code>               (<code>int</code>)           \u2013            <p>Type of inner boundary condition. Choices:  1 (core cooling), 2 (prescribed heat flux), 3 (prescribed temperature).</p> </li> <li> <code>inner_boundary_value</code>               (<code>float</code>)           \u2013            <p>Value of the inner boundary condition, either temperature or heat flux, depending on the chosen condition.</p> </li> <li> <code>conduction</code>               (<code>bool</code>)           \u2013            <p>Whether to include conductive heat flux in the model. Default is True.</p> </li> <li> <code>convection</code>               (<code>bool</code>)           \u2013            <p>Whether to include convective heat flux in the model. Default is True.</p> </li> <li> <code>gravitational_separation</code>               (<code>bool</code>)           \u2013            <p>Whether to include gravitational separation flux in the model. Default is False.</p> </li> <li> <code>mixing</code>               (<code>bool</code>)           \u2013            <p>Whether to include mixing flux in the model. Default is False.</p> </li> <li> <code>dilatation</code>               (<code>bool</code>)           \u2013            <p>Whether to include dilatation source term in the model. Default is False.</p> </li> <li> <code>mass_coordinates</code>               (<code>bool</code>)           \u2013            <p>Whether to use mass coordinates in the model. Default is False.</p> </li> <li> <code>tsurf_poststep_change</code>               (<code>float</code>)           \u2013            <p>Maximum change in surface temperature allowed during a single interior iteration [K].</p> </li> <li> <code>event_triggering</code>               (<code>bool</code>)           \u2013            <p>Whether to include event triggering in the solver. Default is True.</p> </li> <li> <code>bulk_modulus</code>               (<code>float</code>)           \u2013            <p>Adiabatic bulk modulus AW-EOS parameter [Pa].</p> </li> </ul>"},{"location":"config.html#proteus.config._interior.InteriorDummy","title":"<code>InteriorDummy</code>","text":"<p>Parameters for Dummy interior module.</p> <p>Attributes:</p> <ul> <li> <code>ini_tmagma</code>               (<code>float</code>)           \u2013            <p>Initial magma surface temperature [K].</p> </li> <li> <code>tmagma_atol</code>               (<code>float</code>)           \u2013            <p>Max absolute change in surface temperature [K] during a single iteration.</p> </li> <li> <code>tmagma_rtol</code>               (<code>float</code>)           \u2013            <p>Max relative change in surface temperature [K] during a single iteration.</p> </li> <li> <code>mantle_rho</code>               (<code>float</code>)           \u2013            <p>Mantle mass density [kg m-3].</p> </li> <li> <code>mantle_cp</code>               (<code>float</code>)           \u2013            <p>Mantle specific heat capacity [J kg-1 K-1]</p> </li> <li> <code>mantle_tliq</code>               (<code>float</code>)           \u2013            <p>Mantle liquidus temperature [K]</p> </li> <li> <code>mantle_tsol</code>               (<code>float</code>)           \u2013            <p>Mantle solidus temperature [K]</p> </li> <li> <code>H_radio</code>               (<code>float</code>)           \u2013            <p>Constant radiogenic heating rate [W kg-1]</p> </li> </ul>"},{"location":"config.html#proteus.config._interior.Interior","title":"<code>Interior</code>","text":"<p>Magma ocean model selection and parameters.</p> <p>Attributes:</p> <ul> <li> <code>grain_size</code>               (<code>float</code>)           \u2013            <p>Crystal settling grain size [m].</p> </li> <li> <code>F_initial</code>               (<code>float</code>)           \u2013            <p>Initial heat flux guess [W m-2].</p> </li> <li> <code>radiogenic_heat</code>               (<code>bool</code>)           \u2013            <p>Include radiogenic heat production?</p> </li> <li> <code>tidal_heat</code>               (<code>bool</code>)           \u2013            <p>Include tidal heating?</p> </li> <li> <code>rheo_phi_loc</code>               (<code>float</code>)           \u2013            <p>Centre of rheological transition in terms of melt fraction</p> </li> <li> <code>rheo_phi_wid</code>               (<code>float</code>)           \u2013            <p>Width of rheological transition in terms of melt fraction</p> </li> <li> <code>module</code>               (<code>str</code>)           \u2013            <p>Module for simulating the magma ocean. Choices: 'spider', 'aragog', 'dummy'.</p> </li> <li> <code>spider</code>               (<code>Spider</code>)           \u2013            <p>Parameters for running the SPIDER module.</p> </li> <li> <code>aragog</code>               (<code>Aragog</code>)           \u2013            <p>Parameters for running the aragog module.</p> </li> <li> <code>dummy</code>               (<code>Dummy</code>)           \u2013            <p>Parameters for running the dummy module.</p> </li> <li> <code>melting_dir</code>               (<code>str</code>)           \u2013            <p>Set of melting curves to use in the model.</p> </li> <li> <code>lookup_dir</code>               (<code>str</code>)           \u2013            <p>Set of lookup data files to use in the model (e.g. equations of state).</p> </li> </ul>"},{"location":"config.html#atmosphere-climate","title":"Atmosphere climate","text":""},{"location":"config.html#proteus.config._atmos_clim.Agni","title":"<code>Agni</code>","text":"<p>AGNI atmosphere module.</p> <p>Attributes:</p> <ul> <li> <code>verbosity</code>               (<code>int</code>)           \u2013            <p>Logging and output verbosity for agni (0:none, 1:info, 2:debug)</p> </li> <li> <code>p_top</code>               (<code>float</code>)           \u2013            <p>Top of atmosphere grid pressure [bar].</p> </li> <li> <code>p_obs</code>               (<code>float</code>)           \u2013            <p>Pressure level probed by observations [bar]</p> </li> <li> <code>spectral_group</code>               (<code>str</code>)           \u2013            <p>Spectral file codename defining the gas opacities to be included. See documentation.</p> </li> <li> <code>spectral_bands</code>               (<code>str</code>)           \u2013            <p>Number of wavenumer bands in k-table. See documentation.</p> </li> <li> <code>surf_material</code>               (<code>str</code>)           \u2013            <p>File name for material used to set surface single-scattering properties, relative to FWL data directory. Set to 'greybody' to use <code>surf_greyalbedo</code>. See documentation for potential options.</p> </li> <li> <code>num_levels</code>               (<code>str</code>)           \u2013            <p>Number of atmospheric grid levels.</p> </li> <li> <code>chemistry</code>               (<code>str | None</code>)           \u2013            <p>Treatment of self-consistent atmospheric chemsitry. Choices: \"none\", \"eq\".</p> </li> <li> <code>solve_energy</code>               (<code>bool</code>)           \u2013            <p>Solve for an energy-conserving atmosphere solution.</p> </li> <li> <code>solution_atol</code>               (<code>float</code>)           \u2013            <p>Absolute tolerance on the atmosphere solution.</p> </li> <li> <code>solution_rtol</code>               (<code>float</code>)           \u2013            <p>Relative tolerance on the atmosphere solution.</p> </li> <li> <code>overlap_method</code>               (<code>str</code>)           \u2013            <p>Gas overlap method. Choices: random overlap (\"ro\"), RO with resorting+rebinning (\"rorr\"), equivalent extinction (\"ee\").</p> </li> <li> <code>surf_roughness</code>               (<code>float</code>)           \u2013            <p>Characteristic surface roughness scale [metres].</p> </li> <li> <code>surf_windspeed</code>               (<code>float</code>)           \u2013            <p>Characteristic surface wind speed [m/s].</p> </li> <li> <code>phs_timescale</code>               (<code>float</code>)           \u2013            <p>Characteristic timescale of phase changes [seconds].</p> </li> <li> <code>evap_efficiency</code>               (<code>bool</code>)           \u2013            <p>Efficiency of raindrop re-evaporation (0 to 1).</p> </li> <li> <code>rainout</code>               (<code>bool</code>)           \u2013            <p>Enable volatile condensation and evaporation in the atmosphere.</p> </li> <li> <code>oceans</code>               (<code>bool</code>)           \u2013            <p>Enable volatile ocean formation at the surface.</p> </li> <li> <code>latent_heat</code>               (<code>bool</code>)           \u2013            <p>Account for latent heat from condense/evap when solving temperature profile. Requires <code>condensation=true</code>.</p> </li> <li> <code>convection</code>               (<code>bool</code>)           \u2013            <p>Account for convective heat transport, using MLT.</p> </li> <li> <code>conduction</code>               (<code>bool</code>)           \u2013            <p>Account for conductive heat transport, using Fourier's law.</p> </li> <li> <code>sens_heat</code>               (<code>bool</code>)           \u2013            <p>Include sensible heat flux at surface</p> </li> <li> <code>real_gas</code>               (<code>bool</code>)           \u2013            <p>Use real gas equations of state in atmosphere, where possible.</p> </li> <li> <code>psurf_thresh</code>               (<code>float</code>)           \u2013            <p>Use the transparent-atmosphere solver when P_surf is less than this value [bar].</p> </li> <li> <code>dx_max</code>               (<code>float</code>)           \u2013            <p>Nominal maximum step size to T(p) during the solver process, although this is dynamic.</p> </li> <li> <code>dx_max_ini</code>               (<code>float</code>)           \u2013            <p>Initial maximum step size to T(p) when AGNI is called in the first few PROTEUS loops.</p> </li> <li> <code>max_steps</code>               (<code>int</code>)           \u2013            <p>Maximum number of iterations before giving up.</p> </li> <li> <code>perturb_all</code>               (<code>bool</code>)           \u2013            <p>Recalculate entire jacobian matrix at every iteration?</p> </li> <li> <code>mlt_criterion</code>               (<code>str</code>)           \u2013            <p>Convection criterion. Options: (l)edoux, (s)chwarzschild.</p> </li> <li> <code>fastchem_floor</code>               (<code>float</code>)           \u2013            <p>Minimum temperature allowed to be sent to FC</p> </li> <li> <code>fastchem_maxiter_chem</code>               (<code>int</code>)           \u2013            <p>Maximum FC iterations (chemistry)</p> </li> <li> <code>fastchem_maxiter_solv</code>               (<code>int</code>)           \u2013            <p>Maximum FC iterations (internal solver)</p> </li> <li> <code>fastchem_xtol_chem</code>               (<code>float</code>)           \u2013            <p>FC solver tolerance (chemistry)</p> </li> <li> <code>fastchem_xtol_elem</code>               (<code>float</code>)           \u2013            <p>FC solver tolerance (elemental)</p> </li> <li> <code>ini_profile</code>               (<code>str</code>)           \u2013            <p>Shape of initial T(p) guess: 'loglinear', 'isothermal', 'dry_adiabat', 'analytic'.</p> </li> <li> <code>ls_default</code>               (<code>int</code>)           \u2013            <p>Default linesearch method. 0: disabled, 1: goldensection, 2: backtracking.</p> </li> </ul>"},{"location":"config.html#proteus.config._atmos_clim.Janus","title":"<code>Janus</code>","text":"<p>JANUS atmosphere module.</p> <p>Attributes:</p> <ul> <li> <code>p_top</code>               (<code>float</code>)           \u2013            <p>Top of atmosphere grid pressure [bar].</p> </li> <li> <code>p_obs</code>               (<code>float</code>)           \u2013            <p>Pressure level probed by observations [bar]</p> </li> <li> <code>spectral_group</code>               (<code>str</code>)           \u2013            <p>Spectral file codename defining the gas opacities to be included. See documentation.</p> </li> <li> <code>spectral_bands</code>               (<code>str</code>)           \u2013            <p>Number of wavenumer bands in k-table. See documentation.</p> </li> <li> <code>F_atm_bc</code>               (<code>int</code>)           \u2013            <p>Measure outgoing flux using value at TOA (0) or surface (1).</p> </li> <li> <code>num_levels</code>               (<code>int</code>)           \u2013            <p>Number of atmospheric grid levels.</p> </li> <li> <code>tropopause</code>               (<code>str | None</code>)           \u2013            <p>Scheme for determining tropopause location. Choices: \"none\", \"skin\", \"dynamic\".</p> </li> <li> <code>overlap_method</code>               (<code>str</code>)           \u2013            <p>Gas overlap method. Choices: random overlap (\"ro\"), RO with resorting+rebinning (\"rorr\"), equivalent extinction (\"ee\").</p> </li> </ul>"},{"location":"config.html#proteus.config._atmos_clim.Dummy","title":"<code>Dummy</code>","text":"<p>Dummy atmosphere module.</p> <p>A parametrised model of the atmosphere designed for debugging. The greenhouse effect is captured by <code>gamma</code> which produces a transparent atmosphere when 0, and a completely opaque atmosphere when 1. The height of the atmosphere equals the scale height times the <code>height_factor</code> variable.</p> <p>Attributes:</p> <ul> <li> <code>gamma</code>               (<code>float</code>)           \u2013            <p>Atmosphere opacity factor between 0 and 1.</p> </li> <li> <code>height_factor</code>               (<code>float</code>)           \u2013            <p>A multiplying factor applied to the ideal-gas scale height.</p> </li> </ul>"},{"location":"config.html#proteus.config._atmos_clim.AtmosClim","title":"<code>AtmosClim</code>","text":"<p>Atmosphere parameters, model selection.</p> <p>Attributes:</p> <ul> <li> <code>prevent_warming</code>               (<code>bool</code>)           \u2013            <p>When True, require the planet to monotonically cool over time.</p> </li> <li> <code>surface_d</code>               (<code>float</code>)           \u2013            <p>Conductive skin thickness [m],</p> </li> <li> <code>surface_k</code>               (<code>float</code>)           \u2013            <p>Conductive skin thermal conductivity [W m-1 K-1].</p> </li> <li> <code>cloud_enabled</code>               (<code>bool</code>)           \u2013            <p>Enable water cloud radiative effects.</p> </li> <li> <code>cloud_alpha</code>               (<code>float</code>)           \u2013            <p>Condensate retention fraction (0 =&gt; full rainout, 1 =&gt; fully retained).</p> </li> <li> <code>surf_state</code>               (<code>str</code>)           \u2013            <p>Surface energy balance scheme. Choices: \"mixed_layer\", \"fixed\", \"skin\".</p> </li> <li> <code>surf_greyalbedo</code>               (<code>float</code>)           \u2013            <p>Grey surface albedo.</p> </li> <li> <code>albedo_pl</code>               (<code>float | str</code>)           \u2013            <p>Planetary bond albedo used to emulate scattering. Can be float (0 to 1) or str (path to CSV file containing lookup data).</p> </li> <li> <code>rayleigh</code>               (<code>bool</code>)           \u2013            <p>Include Rayleigh scattering in the radiative transfer calculations.</p> </li> <li> <code>tmp_minimum</code>               (<code>float</code>)           \u2013            <p>Minimum temperature throughout the atmosphere [K].</p> </li> <li> <code>tmp_maximum</code>               (<code>float</code>)           \u2013            <p>Maximum temperature throughout the atmosphere [K].</p> </li> <li> <code>module</code>               (<code>str</code>)           \u2013            <p>Which atmosphere module to use.</p> </li> <li> <code>agni</code>               (<code>Agni</code>)           \u2013            <p>Config parameters for AGNI atmosphere module</p> </li> <li> <code>janus</code>               (<code>Janus</code>)           \u2013            <p>Config parameters for JANUS atmosphere module</p> </li> <li> <code>dummy</code>               (<code>Dummy</code>)           \u2013            <p>Config parameters for dummy atmosphere module</p> </li> </ul>"},{"location":"config.html#proteus.config._atmos_clim.AtmosClim.surf_state_int","title":"<code>surf_state_int</code>  <code>property</code>","text":"<p>Return integer surface boundary condition for agni.</p>"},{"location":"config.html#proteus.config._atmos_clim.AtmosClim.albedo_from_file","title":"<code>albedo_from_file</code>  <code>property</code>","text":"<p>Is albedo set by lookup table or not?</p>"},{"location":"config.html#atmospheric-escape","title":"Atmospheric escape","text":""},{"location":"config.html#proteus.config._escape.Zephyrus","title":"<code>Zephyrus</code>","text":"<p>Parameters for Zephyrus module.</p> <p>Attributes:</p> <ul> <li> <code>Pxuv</code>               (<code>float</code>)           \u2013            <p>Pressure at which XUV radiation become opaque in the planetary atmosphere [bar]</p> </li> <li> <code>efficiency</code>               (<code>float</code>)           \u2013            <p>Escape efficiency factor</p> </li> <li> <code>tidal</code>               (<code>bool</code>)           \u2013            <p>Tidal contribution enabled</p> </li> </ul>"},{"location":"config.html#proteus.config._escape.EscapeDummy","title":"<code>EscapeDummy</code>","text":"<p>Dummy module.</p> <p>Attributes:</p> <ul> <li> <code>rate</code>               (<code>float</code>)           \u2013            <p>Bulk unfractionated escape rate [kg s-1]</p> </li> </ul>"},{"location":"config.html#proteus.config._escape.EscapeBoreas","title":"<code>EscapeBoreas</code>","text":"<p>BOREAS escape module.</p> <p>Attributes:</p> <ul> <li> <code>fractionate</code>               (<code>bool</code>)           \u2013            <p>Enable elemental fractionation in outflow?</p> </li> <li> <code>efficiency</code>               (<code>float</code>)           \u2013            <p>Energy efficiency factor.</p> </li> <li> <code>sigma_H</code>               (<code>float</code>)           \u2013            <p>Absorption cross-section of H in XUV [cm2]</p> </li> <li> <code>sigma_O</code>               (<code>float</code>)           \u2013            <p>Absorption cross-section of O in XUV [cm2]</p> </li> <li> <code>sigma_C</code>               (<code>float</code>)           \u2013            <p>Absorption cross-section of C in XUV [cm2]</p> </li> <li> <code>sigma_N</code>               (<code>float</code>)           \u2013            <p>Absorption cross-section of N in XUV [cm2]</p> </li> <li> <code>sigma_S</code>               (<code>float</code>)           \u2013            <p>Absorption cross-section of S in XUV [cm2]</p> </li> <li> <code>kappa_H2O</code>               (<code>float</code>)           \u2013            <p>Grey H2O opacity in IR [cm2 g-1]</p> </li> <li> <code>kappa_H2</code>               (<code>float</code>)           \u2013            <p>Grey H2 opacity in IR [cm2 g-1]</p> </li> <li> <code>kappa_O2</code>               (<code>float</code>)           \u2013            <p>Grey O2 opacity in IR [cm2 g-1]</p> </li> <li> <code>kappa_CO2</code>               (<code>float</code>)           \u2013            <p>Grey CO2 opacity in IR [cm2 g-1]</p> </li> <li> <code>kappa_CO</code>               (<code>float</code>)           \u2013            <p>Grey CO opacity in IR [cm2 g-1]</p> </li> <li> <code>kappa_CH4</code>               (<code>float</code>)           \u2013            <p>Grey CH4 opacity in IR [cm2 g-1]</p> </li> <li> <code>kappa_N2</code>               (<code>float</code>)           \u2013            <p>Grey N2 opacity in IR [cm2 g-1]</p> </li> <li> <code>kappa_NH3</code>               (<code>float</code>)           \u2013            <p>Grey NH3 opacity in IR [cm2 g-1]</p> </li> <li> <code>kappa_H2S</code>               (<code>float</code>)           \u2013            <p>Grey H2S opacity in IR [cm2 g-1]</p> </li> <li> <code>kappa_SO2</code>               (<code>float</code>)           \u2013            <p>Grey SO2 opacity in IR [cm2 g-1]</p> </li> <li> <code>kappa_S2</code>               (<code>float</code>)           \u2013            <p>Grey S2 opacity in IR [cm2 g-1]</p> </li> </ul>"},{"location":"config.html#proteus.config._escape.Escape","title":"<code>Escape</code>","text":"<p>Escape parameters and module selection.</p> <p>Attributes:</p> <ul> <li> <code>reservoir</code>               (<code>str</code>)           \u2013            <p>Escaping composition when not doing fractionation. Choices: bulk, outgas, pxuv.</p> </li> <li> <code>module</code>               (<code>str | None</code>)           \u2013            <p>Escape module to use. Choices: None, \"dummy\", \"zephyrus\", \"boreas\".</p> </li> <li> <code>zephyrus</code>               (<code>Zephyrus</code>)           \u2013            <p>Parameters for zephyrus module.</p> </li> <li> <code>dummy</code>               (<code>EscapeDummy</code>)           \u2013            <p>Parameters for dummy escape module.</p> </li> <li> <code>boreas</code>               (<code>EscapeBoreas</code>)           \u2013            <p>Parameters for BOREAS escape module.</p> </li> </ul>"},{"location":"config.html#proteus.config._escape.Escape.xuv_defined_by_radius","title":"<code>xuv_defined_by_radius</code>  <code>property</code>","text":"<p>Does Rxuv define the escape level?</p> <p>If the escape level is defined by constant Pxuv, then return False. This depends on the escape module used. BOREAS calculates both Pxuv and Rxuv, while the default assumes that Pxuv is constant, which is used to find Rxuv from the r(p) profile.</p>"},{"location":"config.html#atmospheric-chemistry","title":"Atmospheric chemistry","text":""},{"location":"config.html#proteus.config._atmos_chem.Vulcan","title":"<code>Vulcan</code>","text":"<p>VULCAN chemistry module.</p> <p>Attributes:</p> <ul> <li> <code>clip_fl</code>               (<code>float</code>)           \u2013            <p>Stellar flux floor [ergs cm-2 s-1 nm-1].</p> </li> <li> <code>clip_vmr</code>               (<code>float</code>)           \u2013            <p>Neglect species with surface VMR &lt; clip_vmr.</p> </li> <li> <code>make_funs</code>               (<code>bool</code>)           \u2013            <p>Make functions from chemical network.</p> </li> <li> <code>ini_mix</code>               (<code>str</code>)           \u2013            <p>Initial mixing ratios. Options: profile, outgas.</p> </li> <li> <code>fix_surf</code>               (<code>bool</code>)           \u2013            <p>Fix the surface mixing ratios based on outgassed composition.</p> </li> <li> <code>network</code>               (<code>str</code>)           \u2013            <p>Chemical network. Options: CHO, NCHO, SNCHO.</p> </li> <li> <code>save_frames</code>               (<code>bool</code>)           \u2013            <p>Save simulation state as plots.</p> </li> <li> <code>yconv_cri</code>               (<code>float</code>)           \u2013            <p>Steady state - max change in mixing ratio over test period</p> </li> <li> <code>slope_cri</code>               (<code>float</code>)           \u2013            <p>Steady state - max rate of change of mixing ratio over test period</p> </li> </ul>"},{"location":"config.html#proteus.config._atmos_chem.AtmosChem","title":"<code>AtmosChem</code>","text":"<p>Atmosphere chemistry parameters, model selection.</p> <p>Attributes:</p> <ul> <li> <code>module</code>               (<code>str</code>)           \u2013            <p>Chemistry module</p> </li> <li> <code>vulcan</code>               (<code>Vulcan</code>)           \u2013            <p>VULCAN  module options</p> </li> <li> <code>when</code>               (<code>str</code>)           \u2013            <p>When to run the chemistry module. Options: manually, offline, online.</p> </li> <li> <code>photo_on</code>               (<code>bool</code>)           \u2013            <p>Use photochemistry.</p> </li> <li> <code>Kzz_on</code>               (<code>bool</code>)           \u2013            <p>Use Kzz.</p> </li> <li> <code>Kzz_const</code>               (<code>float</code>)           \u2013            <p>Constant Kzz value [cm2/s]. If 'none', Kzz is read from NetCDF file.</p> </li> <li> <code>moldiff_on</code>               (<code>bool</code>)           \u2013            <p>Use molecular diffusion.</p> </li> <li> <code>updraft_const</code>               (<code>float</code>)           \u2013            <p>Updraft velocity [cm/s].</p> </li> </ul>"},{"location":"config.html#volatile-outgassing","title":"Volatile outgassing","text":""},{"location":"config.html#proteus.config._outgas.Calliope","title":"<code>Calliope</code>","text":"<p>Module parameters for Calliope.</p> <p>Attributes:</p> <ul> <li> <code>T_floor</code>               (<code>float</code>)           \u2013            <p>Temperature floor applied to chemistry calculation [K].</p> </li> <li> <code>include_H2O</code>               (<code>bool</code>)           \u2013            <p>If True, include H2O.</p> </li> <li> <code>include_CO2</code>               (<code>bool</code>)           \u2013            <p>If True, include CO2.</p> </li> <li> <code>include_N2</code>               (<code>bool</code>)           \u2013            <p>If True, include N2.</p> </li> <li> <code>include_S2</code>               (<code>bool</code>)           \u2013            <p>If True, include S2.</p> </li> <li> <code>include_SO2</code>               (<code>bool</code>)           \u2013            <p>If True, include SO2.</p> </li> <li> <code>include_H2S</code>               (<code>bool</code>)           \u2013            <p>If True, include H2S.</p> </li> <li> <code>include_NH3</code>               (<code>bool</code>)           \u2013            <p>If True, include NH3.</p> </li> <li> <code>include_H2</code>               (<code>bool</code>)           \u2013            <p>If True, include H2.</p> </li> <li> <code>include_CH4</code>               (<code>bool</code>)           \u2013            <p>If True, include CH4.</p> </li> <li> <code>include_CO</code>               (<code>bool</code>)           \u2013            <p>If True, include CO.</p> </li> <li> <code>rtol</code>               (<code>float</code>)           \u2013            <p>Relative tolerance on solver for mass conservation.</p> </li> <li> <code>xtol</code>               (<code>float</code>)           \u2013            <p>Absolute tolerance on solver for mass conservation.</p> </li> <li> <code>solubility</code>               (<code>bool</code>)           \u2013            <p>Enable solubility of volatiles into melt.</p> </li> </ul>"},{"location":"config.html#proteus.config._outgas.Calliope.is_included","title":"<code>is_included(vol)</code>","text":"<p>Helper method for getting flag if <code>vol</code> is included in outgassing.</p> Source code in <code>src/proteus/config/_outgas.py</code> <pre><code>def is_included(self, vol: str) -&gt; bool:\n    \"\"\"Helper method for getting flag if `vol` is included in outgassing.\"\"\"\n    return getattr(self, f'include_{vol}')\n</code></pre>"},{"location":"config.html#proteus.config._outgas.Atmodeller","title":"<code>Atmodeller</code>","text":"<p>Module parameters for Atmodeller.</p> <p>Attributes:</p> <ul> <li> <code>some_parameter</code>               (<code>str</code>)           \u2013            <p>Not used currently.</p> </li> </ul>"},{"location":"config.html#proteus.config._outgas.Outgas","title":"<code>Outgas</code>","text":"<p>Outgassing parameters (fO2) and included volatiles.</p> <p>Attributes:</p> <ul> <li> <code>fO2_shift_IW</code>               (<code>float</code>)           \u2013            <p>Homogeneous oxygen fugacity in the magma ocean used to represent redox state (log10 units relative to Iron-Wustite).</p> </li> <li> <code>module</code>               (<code>str</code>)           \u2013            <p>Outgassing module to be used. Choices: 'calliope' only.</p> </li> <li> <code>mass_thresh</code>               (<code>float</code>)           \u2013            <p>Minimum threshold for element mass [kg]. Inventories below this are set to zero.</p> </li> <li> <code>calliope</code>               (<code>Calliope</code>)           \u2013            <p>Parameters for CALLIOPE module.</p> </li> <li> <code>atmodeller</code>               (<code>Atmodeller</code>)           \u2013            <p>Parameters for atmodeller module.</p> </li> </ul>"},{"location":"config.html#elemental-delivery-and-accretion","title":"Elemental delivery and accretion","text":""},{"location":"config.html#proteus.config._delivery.Elements","title":"<code>Elements</code>","text":"<p>Initial volatile inventory by planetary bulk element abundances.</p> <p>There are various ways to set these. You can specify a metallicity relative to solar alongside a total hydrogen abundance by providing <code>use_metallicity=True</code>.</p> <p>Instead of metallicity, provide the abundance of each element with either specific mass ratio relative to hydrogen or in terms of the concentration in the mantle. For X in {C, N, S}: only XH_ratio or X_ppmw should be used at any one time.</p> <p>Hydrogen abundance is set via either <code>H_oceans</code>, which is the number of oceans of hydrogen in the planet's mantle at initialisation (assumed to be fully molten). Or, you can set the hydrogen abundance in ppm relative to the mantle mass with <code>H_ppmw</code>. For hydrogen: only H_oceans or H_ppmw should be used at any one time.</p> <p>Attributes:</p> <ul> <li> <code>H_oceans</code>               (<code>float</code>)           \u2013            <p>Absolute hydrogen inventory, units of equivalent Earth oceans.</p> </li> <li> <code>H_kg</code>               (<code>float</code>)           \u2013            <p>Absolute hydrogen inventory, kg.</p> </li> <li> <code>H_ppmw</code>               (<code>float</code>)           \u2013            <p>Relative hydrogen inventory, ppmw relative to mantle mass.</p> </li> <li> <code>use_metallicity</code>               (<code>bool</code>)           \u2013            <p>Whether or not to specify the elemental abundances in terms of solar metallicity</p> </li> <li> <code>metallicity</code>               (<code>float</code>)           \u2013            <p>Elemental metallicity relative to solar metallicity, by mass</p> </li> <li> <code>CH_ratio</code>               (<code>float</code>)           \u2013            <p>Carbon metallicity. C/H mass ratio in combined mantle+atmosphere system.</p> </li> <li> <code>C_kg</code>               (<code>float</code>)           \u2013            <p>Absolute carbon inventory, kg.</p> </li> <li> <code>C_ppmw</code>               (<code>float</code>)           \u2013            <p>Relative carbon inventory, ppmw relative to mantle mass.</p> </li> <li> <code>NH_ratio</code>               (<code>float</code>)           \u2013            <p>Nitrogen metallicity. N/H mass ratio in combined mantle+atmosphere system.</p> </li> <li> <code>N_kg</code>               (<code>float</code>)           \u2013            <p>Absolute nitrogen inventory, kg.</p> </li> <li> <code>N_ppmw</code>               (<code>float</code>)           \u2013            <p>Relative nitrogen inventory, ppmw relative to mantle mass.</p> </li> <li> <code>SH_ratio</code>               (<code>float</code>)           \u2013            <p>Sulfur metallicity. C/H mass ratio in combined mantle+atmosphere system.</p> </li> <li> <code>S_kg</code>               (<code>float</code>)           \u2013            <p>Absolute sulfur inventory, kg.</p> </li> <li> <code>S_ppmw</code>               (<code>float</code>)           \u2013            <p>Absolute sulfur inventory, ppmw relative to mantle mass.</p> </li> </ul>"},{"location":"config.html#proteus.config._delivery.Volatiles","title":"<code>Volatiles</code>","text":"<p>Initial volatile inventory set by partial pressures in atmosphere.</p> <p>Attributes:</p> <ul> <li> <code>H2O</code>               (<code>float</code>)           \u2013            <p>Initial atmospheric partial surface pressure of H2O [bar].</p> </li> <li> <code>CO2</code>               (<code>float</code>)           \u2013            <p>Initial atmospheric partial surface pressure of CO2 [bar].</p> </li> <li> <code>N2</code>               (<code>float</code>)           \u2013            <p>Initial atmospheric partial surface pressure of N2 [bar].</p> </li> <li> <code>S2</code>               (<code>float</code>)           \u2013            <p>Initial atmospheric partial surface pressure of S2 [bar].</p> </li> <li> <code>SO2</code>               (<code>float</code>)           \u2013            <p>Initial atmospheric partial surface pressure of SO2 [bar].</p> </li> <li> <code>H2S</code>               (<code>float</code>)           \u2013            <p>Initial atmospheric partial surface pressure of H2S [bar].</p> </li> <li> <code>NH3</code>               (<code>float</code>)           \u2013            <p>Initial atmospheric partial surface pressure of NH3 [bar].</p> </li> <li> <code>H2</code>               (<code>float</code>)           \u2013            <p>Initial atmospheric partial surface pressure of H2 [bar].</p> </li> <li> <code>CH4</code>               (<code>float</code>)           \u2013            <p>Initial atmospheric partial surface pressure of CH4 [bar].</p> </li> <li> <code>CO</code>               (<code>float</code>)           \u2013            <p>Initial atmospheric partial surface pressure of CO [bar].</p> </li> </ul>"},{"location":"config.html#proteus.config._delivery.Volatiles.get_pressure","title":"<code>get_pressure(s)</code>","text":"<p>Helper method for getting the pressure for <code>vol</code> by string.</p> Source code in <code>src/proteus/config/_delivery.py</code> <pre><code>def get_pressure(self, s: str) -&gt; float:\n    \"\"\"Helper method for getting the pressure for `vol` by string.\"\"\"\n    return getattr(self, s)\n</code></pre>"},{"location":"config.html#proteus.config._delivery.Delivery","title":"<code>Delivery</code>","text":"<p>Initial volatile inventory, radionuclide concentration, and delivery model selection.</p> <p>Attributes:</p> <ul> <li> <code>initial</code>               (<code>str</code>)           \u2013            <p>Method by which to set the initial volatile inventory to use. Options: 'volatiles', 'elements'.</p> </li> <li> <code>module</code>               (<code>str</code>)           \u2013            <p>Delivery module to use (Not used as of yet).</p> </li> <li> <code>elements</code>               (<code>Elements</code>)           \u2013            <p>Parameters used when setting volatile inventory by element abundances.</p> </li> <li> <code>volatiles</code>               (<code>Volatiles</code>)           \u2013            <p>Parameters used when setting volatile inventory by partial pressures.</p> </li> <li> <code>radio_tref</code>               (<code>float</code>)           \u2013            <p>Reference age for setting radioactive decay [Gyr].</p> </li> <li> <code>radio_U</code>               (<code>float</code>)           \u2013            <p>Concentration (ppmw) of uranium at reference age of <code>t=radio_tref</code></p> </li> <li> <code>radio_K</code>               (<code>float</code>)           \u2013            <p>Concentration (ppmw) of potassium at reference age of <code>t=radio_tref</code></p> </li> <li> <code>radio_Th</code>               (<code>float</code>)           \u2013            <p>Concentration (ppmw) of thorium at reference age of <code>t=radio_tref</code></p> </li> </ul>"},{"location":"config.html#synthetic-observations","title":"Synthetic observations","text":"For developers: adding a new parameter <p>So, you are developing a new model and want to add some parameters? Follow these steps:</p> <ol> <li>Decide on a good parameter name (e.g. <code>my_star_var</code>), and under which section to place it (e.g. <code>star</code>).    Add the new variable to the config submodule.</li> <li>Add the type for your variable, e.g. float, int, str.    You can also add complex types, please check the code for inspiration.</li> <li>Add a validator!    If your variable has a maximum value (e.g. 10), you can add a validator to make sure    that any values above 10 are rejected: <code>my_star_var: float = field(validator=attrs.validators.le(10))</code></li> <li>Add a description for your new variable under <code>Attributes</code> in the docstring.    The documentation uses the description to generate this documentation.</li> <li>Update the example input configs.    Proteus checks tests all input configs in this directory are valid.</li> <li>Use your parameter in your code, i.e.: <code>config.star.my_star_var</code></li> </ol> src/proteus/config/_star.py<pre><code>class Star:\n    \"\"\"Stellar parameters.\n\n    Attributes\n    ----------\n    my_star_var: float\n        Star variable, must be 10 or lower!\n    \"\"\"\n    my_star_var: float = field(validator=attrs.validators.le(10))\n</code></pre> <p>Proteus uses attrs for its parameter handling. Please see the examples for more information how to work with attrs.</p>"},{"location":"config.html#proteus.config._observe.Platon","title":"<code>Platon</code>","text":"<p>Parameters for the PLATON module.</p> <p>Attributes:</p> <ul> <li> <code>downsample</code>               (<code>int</code>)           \u2013            <p>Downsample binning factor for the spectrum.</p> </li> <li> <code>clip_vmr</code>               (<code>float</code>)           \u2013            <p>Minimum VMR for a species to be included in the radiative transfer.</p> </li> </ul>"},{"location":"config.html#proteus.config._observe.Observe","title":"<code>Observe</code>","text":"<p>Synthetic observations.</p> <p>synthesis: str     Module to use for calculating synthetic spectra.</p>"},{"location":"contact.html","title":"Contact","text":"<p>We encourage you to reach out! Choose the most appropriate channel below.</p> Channel Use for GitHub Discussions Questions, installation help, feature suggestions GitHub Issues Bug reports, specific feature requests proteus_dev@formingworlds.space General enquiries"},{"location":"data.html","title":"Reference data","text":""},{"location":"data.html#contents","title":"Contents","text":"<ul> <li>Stellar spectra</li> <li>Using solar spectra</li> <li>Using a (Mega-)MUSCLES observed spectrum</li> <li>Using a PHOENIX synthetic spectrum</li> <li>Using a custom stellar spectrum</li> <li>Surfaces</li> <li>Exoplanet population data</li> <li>Mass-radius relations</li> </ul>"},{"location":"data.html#automatic-data-download","title":"Automatic data download","text":"<p>PROTEUS automatically downloads large reference data files from Zenodo on first run. This includes stellar spectra, opacities, equations of state, and other tabulated data.</p> <ul> <li>Token is optional: The code works without a Zenodo API token, using public access.</li> <li>Higher limits: Optional API token provides higher rate limits if you download often.</li> <li>OSF Fallback: If Zenodo is unavailable, data downloads automatically from OSF.</li> </ul> <p>To optionally configure your token, see the Troubleshooting guide.</p>"},{"location":"data.html#stellar-spectra","title":"Stellar spectra","text":"<p>PROTEUS can use:</p> <ul> <li>Observed spectra from MUSCLES / Mega-MUSCLES and NREL (for the Sun)</li> <li>Synthetic PHOENIX spectra</li> </ul> <p>By default, observed spectra are searched in:</p> <ul> <li><code>$FWL_DATA/stellar_spectra/MUSCLES</code>  \u2013 MUSCLES / Mega-MUSCLES stars</li> <li><code>$FWL_DATA/stellar_spectra/solar</code>    \u2013 solar spectra (modern, past, future)</li> </ul> <p>To see which files you have installed, run for example:</p> <pre><code>ls $FWL_DATA/stellar_spectra/MUSCLES\nls $FWL_DATA/stellar_spectra/solar\n</code></pre> <p>For PHOENIX spectra, files are stored under: <code>$FWL_DATA/stellar_spectra/PHOENIX</code>, where each subdirectory corresponds to a metallicity / alpha combination, e.g. FeH-0.5_alpha+0.0/.</p>"},{"location":"data.html#using-solar-spectra","title":"Using solar spectra","text":"<p>To use the modern NREL observed spectrum of the Sun, set <code>spectrum_source = \"solar\"</code> or do not set the parameter at all, and set <code>star_name = \"sun\"</code>.</p> <p>To use a different solar spectrum, for example a 'young sun' spectrum, there are VPL spectra available by Claire et al. (2012). In this case, set <code>spectrum_source = \"solar\"</code> and choose one of the following options for <code>star_name</code>:</p> <ul> <li><code>Sun0.6Ga</code> A young sun of 0.6 Gyr ago (age ~ 4.0 Gyr)</li> <li><code>Sun1.8Ga</code> A young sun of 1.8 Gyr ago (age ~ 2.8 Gyr)</li> <li><code>Sun2.4Ga</code> A young sun of 2.4 Gyr ago (age ~ 2.2 Gyr)</li> <li><code>Sun2.7Ga</code> A young sun of 2.7 Gyr ago (age ~ 1.9 Gyr)</li> <li><code>Sun3.8Ga</code> A young sun of 3.8 Gyr ago (age ~ 800 Myr)</li> <li><code>Sun4.4Ga</code> A young sun of 4.4 Gyr ago (age ~ 200 Myr)</li> <li><code>Sun5.6Gyr</code> A future sun of an age of 5.6 Gyr</li> <li><code>SunModern</code> A modern solar spectrum, can be chosen as an alternative to the NREL spectrum listed above.</li> </ul>"},{"location":"data.html#the-sun","title":"The Sun","text":"<ul> <li>Star name: <code>sun</code></li> <li>URL: https://en.wikipedia.org/wiki/Sun</li> <li>Spectral type:  G2V</li> <li>Teff:  5772 K</li> <li>Age:   4.6 Gyr</li> <li>Luminosity:   1.0 L\u2609</li> <li>Mass:     1.0 M\u2609</li> <li>Radius:     1.0 R\u2609</li> <li>Source: Gueymard 2003, NREL, Claire et al. 2012</li> </ul>"},{"location":"data.html#using-a-mega-muscles-observed-spectrum","title":"Using a (Mega-)MUSCLES observed spectrum","text":"<p>PROTEUS can use observed stellar spectra from the MUSCLES and Mega-MUSCLES surveys. To use one of these observed spectra:</p> <ul> <li>Set <code>spectrum_source = \"muscles\"</code></li> <li>Set <code>star_name</code> to one of the keys listed below (these names must match the installed spectrum filenames / dataset keys; many targets have multiple common aliases, but PROTEUS expects the specific <code>star_name</code> string shown here).</li> </ul> <p>By default, PROTEUS looks for MUSCLES/Mega-MUSCLES spectra under:</p> <ul> <li><code>$FWL_DATA/stellar_spectra/MUSCLES</code></li> </ul> <p>Files are saved as <code>&lt;star_name&gt;.txt</code>, e.g. <code>trappist-1.txt</code>.</p> Full star catalog"},{"location":"data.html#epsilon-eridani","title":"Epsilon Eridani","text":"<ul> <li>Star name: <code>v-eps-eri</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/eps%20Eri</li> <li>Spectral type: K2V</li> <li>Teff: 5020 K</li> <li>Age: 400\u2013800 Myr</li> <li>Luminosity: 0.32 L\u2609</li> <li>Mass: 0.82 M\u2609</li> <li>Radius: 0.759 R\u2609</li> <li>Source: MUSCLES</li> </ul>"},{"location":"data.html#gj-1132","title":"GJ 1132","text":"<ul> <li>Star name: <code>gj1132</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/GJ%201132</li> <li>Spectral type: M4.5 V</li> <li>Teff: 3229 K</li> <li>Age: ?</li> <li>Luminosity: 0.005 L\u2609</li> <li>Mass: 0.195 M\u2609</li> <li>Radius: 0.221 R\u2609</li> <li>Source: Mega-MUSCLES</li> </ul>"},{"location":"data.html#gj-1214","title":"GJ 1214","text":"<ul> <li>Star name: <code>gj1214</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/GJ%201214</li> <li>Spectral type: M4 V</li> <li>Teff: 3100 K</li> <li>Age: 5\u201310 Gyr</li> <li>Luminosity: 0.00351 L\u2609</li> <li>Mass: 0.182 M\u2609</li> <li>Radius: 0.216 R\u2609</li> <li>Source: MUSCLES</li> </ul>"},{"location":"data.html#hd-85512","title":"HD 85512","text":"<ul> <li>Star name: <code>hd85512</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/HD%2085512</li> <li>Spectral type: M0V</li> <li>Teff: ~4400 K</li> <li>Age: ~6 Gyr</li> <li>Luminosity: 0.17 L\u2609</li> <li>Mass: 0.69 M\u2609</li> <li>Radius: 0.69 R\u2609</li> <li>Source: MUSCLES</li> </ul>"},{"location":"data.html#hd-97658","title":"HD 97658","text":"<ul> <li>Star name: <code>hd97658</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/HD%2097658%20b#planet_HD-97658-b_collapsible</li> <li>Spectral type: K1V</li> <li>Teff: 5212 K</li> <li>Age: 3.9 Gyr</li> <li>Luminosity: 0.351 L\u2609</li> <li>Mass: 0.773 M\u2609</li> <li>Radius: 0.728 R\u2609</li> <li>Source: MUSCLES</li> </ul>"},{"location":"data.html#l-98-59","title":"L 98-59","text":"<ul> <li>Star name: <code>l-98-59</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/L%2098-59</li> <li>Spectral type: M3V</li> <li>Teff: 3415 K</li> <li>Age: ~5 Gyr</li> <li>Luminosity: 0.012 L\u2609</li> <li>Mass: 0.292 M\u2609</li> <li>Radius: 0.316 R\u2609</li> <li>Source: Mega-MUSCLES</li> </ul>"},{"location":"data.html#trappist-1","title":"TRAPPIST-1","text":"<ul> <li>Star name: <code>trappist-1</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/TRAPPIST-1</li> <li>Spectral type: M8V</li> <li>Teff: 2566 K</li> <li>Age: ~8 Gyr</li> <li>Luminosity: 0.000553 L\u2609</li> <li>Mass: 0.0898 M\u2609</li> <li>Radius: 0.1192 R\u2609</li> <li>Source: Mega-MUSCLES</li> </ul>"},{"location":"data.html#gj-849","title":"GJ 849","text":"<ul> <li>Star name: <code>gj849</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/GJ%20849</li> <li>Spectral type: M3.5V</li> <li>Teff: 3467 K</li> <li>Age: &gt;3 Gyr</li> <li>Luminosity: 0.02887 L\u2609</li> <li>Mass: 0.45 M\u2609</li> <li>Radius: 0.45 R\u2609</li> <li>Source: Mega-MUSCLES</li> </ul>"},{"location":"data.html#wasp-43","title":"WASP-43","text":"<ul> <li>Star name: <code>wasp-43</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/WASP-43</li> <li>Spectral type: K7V</li> <li>Teff: ~4100 K</li> <li>Age: ~7 Gyr</li> <li>Luminosity: ~0.15 L\u2609</li> <li>Mass: ~0.65 M\u2609</li> <li>Radius: ~0.76 R\u2609</li> <li>Source: MUSCLES extension</li> </ul>"},{"location":"data.html#wasp-77-a","title":"WASP-77 A","text":"<ul> <li>Star name: <code>wasp-77a</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/WASP-77%20A</li> <li>Spectral type: G8V</li> <li>Teff: ~5600 K</li> <li>Age: ~6 Gyr</li> <li>Luminosity: ~0.74 L\u2609</li> <li>Mass: ~0.90 M\u2609</li> <li>Radius: ~0.91 R\u2609</li> <li>Source: MUSCLES extension</li> </ul>"},{"location":"data.html#gj-15-a","title":"GJ 15 A","text":"<ul> <li>Star name: <code>gj15a</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/GJ%2015</li> <li>Spectral type: M1\u2013M2V</li> <li>Teff: ~3700 K</li> <li>Age: ?</li> <li>Luminosity: ~0.021 L\u2609</li> <li>Mass: ~0.40 M\u2609</li> <li>Radius: ~0.38 R\u2609</li> <li>Source: Mega-MUSCLES</li> </ul>"},{"location":"data.html#gj-163","title":"GJ 163","text":"<ul> <li>Star name: <code>gj163</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/gj%20163%20b</li> <li>Spectral type: M3.5V</li> <li>Teff: ~3300\u20133500 K</li> <li>Age: ~2-10 Gyr</li> <li>Luminosity: ~0.02 L\u2609</li> <li>Mass: ~0.40 M\u2609</li> <li>Radius: ~0.41 R\u2609</li> <li>Source: Mega-MUSCLES</li> </ul>"},{"location":"data.html#gj-176-hd-285968","title":"GJ 176 (HD 285968)","text":"<ul> <li>Star name: <code>gj176</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/HD%20285968</li> <li>Spectral type: M2.5V</li> <li>Teff: ~3700 K</li> <li>Age: ~4 Gyr</li> <li>Luminosity: 0.034 L\u2609</li> <li>Mass: 0.51 M\u2609</li> <li>Radius: 0.48 R\u2609</li> <li>Source: MUSCLES</li> </ul>"},{"location":"data.html#gj-436","title":"GJ 436","text":"<ul> <li>Star name: <code>gj436</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/GJ%20436</li> <li>Spectral type: M2.5-M3V</li> <li>Teff: ~3600 K</li> <li>Age: ~6\u201315 Gyr</li> <li>Luminosity: 0.023 L\u2609</li> <li>Mass: 0.44 M\u2609</li> <li>Radius: 0.42 R\u2609</li> <li>Source: MUSCLES</li> </ul>"},{"location":"data.html#gj-551-proxima-centauri","title":"GJ 551 (Proxima Centauri)","text":"<ul> <li>Star name: <code>gj551</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/alpha%20Cen</li> <li>Spectral type: M5.5Ve</li> <li>Teff: ~2900-3000 K</li> <li>Age: ~4.8 Gyr</li> <li>Luminosity: 0.0015 L\u2609</li> <li>Mass: 0.12 M\u2609</li> <li>Radius: 0.14 R\u2609</li> <li>Source: MUSCLES</li> </ul>"},{"location":"data.html#gj-581","title":"GJ 581","text":"<ul> <li>Star name: <code>gj581</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/GJ%20581</li> <li>Spectral type: M3V</li> <li>Teff: ~3500 K</li> <li>Age: ~4 Gyr</li> <li>Luminosity: ~0.012 L\u2609</li> <li>Mass: ~0.30 M\u2609</li> <li>Radius: ~0.30 R\u2609</li> <li>Source: MUSCLES</li> </ul>"},{"location":"data.html#gj-649","title":"GJ 649","text":"<ul> <li>Star name: <code>gj649</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/GJ%20649</li> <li>Spectral type: M1-M2V</li> <li>Teff: ~3700 K</li> <li>Age: ?</li> <li>Luminosity: ~0.044 L\u2609</li> <li>Mass: 0.51 M\u2609</li> <li>Radius: 0.50 R\u2609</li> <li>Source: Mega-MUSCLES</li> </ul>"},{"location":"data.html#gj-667-c","title":"GJ 667 C","text":"<ul> <li>Star name: <code>gj667c</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/GJ%20667C</li> <li>Spectral type: M1.5V</li> <li>Teff: ~3700 K</li> <li>Age: &gt;2 Gyr</li> <li>Luminosity: ~0.014 L\u2609</li> <li>Mass: ~0.33 M\u2609</li> <li>Radius: ~0.32\u20130.42 R\u2609</li> <li>Source: MUSCLES</li> </ul>"},{"location":"data.html#gj-674","title":"GJ 674","text":"<ul> <li>Star name: <code>gj674</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/GJ%20674</li> <li>Spectral type: M2.5V</li> <li>Teff: ~3400\u20133600 K</li> <li>Age: ~0.5\u20133 Gyr</li> <li>Luminosity: 0.017 L\u2609</li> <li>Mass: 0.35 M\u2609</li> <li>Radius: 0.36 R\u2609</li> <li>Source: Mega-MUSCLES</li> </ul>"},{"location":"data.html#gj-676-a","title":"GJ 676 A","text":"<ul> <li>Star name: <code>gj676a</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/GJ%20676A</li> <li>Spectral type: M0V</li> <li>Teff: ~3800\u20134000 K</li> <li>Age: ?</li> <li>Luminosity: 0.083 L\u2609</li> <li>Mass: 0.63 M\u2609</li> <li>Radius: 0.65 R\u2609</li> <li>Source: Mega-MUSCLES</li> </ul>"},{"location":"data.html#gj-699-barnards-star","title":"GJ 699 (Barnard\u2019s Star)","text":"<ul> <li>Star name: <code>gj699</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/barnard's%20star</li> <li>Spectral type: M3.5-4.0V</li> <li>Teff: ~3200-3300 K</li> <li>Age: ~10 Gyr</li> <li>Luminosity: 0.0035 L\u2609</li> <li>Mass: ~0.16 M\u2609</li> <li>Radius: ~0.19 R\u2609</li> <li>Source: Mega-MUSCLES</li> </ul>"},{"location":"data.html#gj-729-ross-154","title":"GJ 729 (Ross 154)","text":"<ul> <li>Star name: <code>gj729</code></li> <li>URL: https://simbad.cds.unistra.fr/simbad/sim-id?Ident=GJ+729</li> <li>Spectral type: M3.5V</li> <li>Teff: ~3200\u20133300 K</li> <li>Age: &lt;1\u20132 Gyr</li> <li>Luminosity: ~0.004\u20130.005 L\u2609</li> <li>Mass: ~0.18 M\u2609</li> <li>Radius: ~0.20 R\u2609</li> <li>Source: Mega-MUSCLES</li> </ul>"},{"location":"data.html#gj-832","title":"GJ 832","text":"<ul> <li>Star name: <code>gj832</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/HIP%20106440</li> <li>Spectral type: M1-M3V</li> <li>Teff: ~3500-3500 K</li> <li>Age: ~4\u201312 Gyr</li> <li>Luminosity: ~0.03 L\u2609</li> <li>Mass: 0.45 M\u2609</li> <li>Radius: 0.45 R\u2609</li> <li>Source: MUSCLES</li> </ul>"},{"location":"data.html#gj-876","title":"GJ 876","text":"<ul> <li>Star name: <code>gj876</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/GJ%20876</li> <li>Spectral type: M2-4V</li> <li>Teff: ~3200\u20133300 K</li> <li>Age: ~2\u201315 Gyr</li> <li>Luminosity: ~0.013 L\u2609</li> <li>Mass: ~0.37 M\u2609</li> <li>Radius: ~0.37 R\u2609</li> <li>Source: MUSCLES</li> </ul>"},{"location":"data.html#hat-p-12","title":"HAT-P-12","text":"<ul> <li>Star name: <code>hat-p-12</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/HAT-P-12</li> <li>Spectral type: K4V</li> <li>Teff: 4500-4800 K</li> <li>Age: 2-14 Gyr</li> <li>Luminosity: ~0.20 L\u2609</li> <li>Mass: 0.74 M\u2609</li> <li>Radius: 0.70 R\u2609</li> <li>Source: MUSCLES extension</li> </ul>"},{"location":"data.html#hat-p-26","title":"HAT-P-26","text":"<ul> <li>Star name: <code>hat-p-26</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/HAT-P-26</li> <li>Spectral type: K1V</li> <li>Teff: ~5050 K</li> <li>Age: 4-12 Gyr</li> <li>Luminosity: ~0.44 L\u2609</li> <li>Mass: 0.85 M\u2609</li> <li>Radius: 0.86 R\u2609</li> <li>Source: MUSCLES extension</li> </ul>"},{"location":"data.html#hd-149026","title":"HD 149026","text":"<ul> <li>Star name: <code>hd-149026</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/HD%20149026%20b#planet_HD-149026</li> <li>Spectral type: G0V</li> <li>Teff: ~6100-6200 K</li> <li>Age: ~2-3 Gyr</li> <li>Luminosity: ~2.6 L\u2609</li> <li>Mass: ~1.14 M\u2609</li> <li>Radius: ~1.46 R\u2609</li> <li>Source: MUSCLES extension</li> </ul>"},{"location":"data.html#hd-40307","title":"HD 40307","text":"<ul> <li>Star name: <code>hd40307</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/HD%2040307</li> <li>Spectral type: K2.5V</li> <li>Teff: ~4800-5000 K</li> <li>Age: ~2\u20135 Gyr</li> <li>Luminosity: ~0.22 L\u2609</li> <li>Mass: ~0.79 M\u2609</li> <li>Radius: ~0.71 R\u2609</li> <li>Source: MUSCLES</li> </ul>"},{"location":"data.html#l-678-39-gj-357","title":"L 678-39 (GJ 357)","text":"<ul> <li>Star name: <code>l-678-39</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/GJ%20357</li> <li>Spectral type: M2.5V</li> <li>Teff: ~3500 K</li> <li>Age: ?</li> <li>Luminosity: 0.0017 L\u2609</li> <li>Mass: ~0.34 M\u2609</li> <li>Radius: ~0.34 R\u2609</li> <li>Source: MUSCLES extension</li> </ul>"},{"location":"data.html#l-980-5","title":"L 980-5","text":"<ul> <li>Star name: <code>l-980-5</code></li> <li>URL: https://simbad.u-strasbg.fr/simbad/sim-id?Ident=l+980-5</li> <li>Spectral type: M4V</li> <li>Teff: ?</li> <li>Age: ?</li> <li>Luminosity: ?</li> <li>Mass: ?</li> <li>Radius: ?</li> <li>Source: Mega-MUSCLES</li> </ul>"},{"location":"data.html#lhs-2686","title":"LHS 2686","text":"<ul> <li>Star name: <code>lhs-2686</code></li> <li>URL: https://simbad.cds.unistra.fr/simbad/sim-id?Ident=LHS+2686</li> <li>Spectral type: M5V</li> <li>Teff: ?</li> <li>Age: ?</li> <li>Luminosity: ?</li> <li>Mass: ?</li> <li>Radius: ?</li> <li>Source: Mega-MUSCLES</li> </ul>"},{"location":"data.html#lp-791-18","title":"LP 791-18","text":"<ul> <li>Star name: <code>lp-791-18</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/LP%20791-18</li> <li>Spectral type: M6V</li> <li>Teff: ~2960 K</li> <li>Age: &gt; 0.5 Gyr</li> <li>Luminosity: ~0.002 L\u2609</li> <li>Mass: 0.139 M\u2609</li> <li>Radius: 0.182 R\u2609</li> <li>Source: MUSCLES extension</li> </ul>"},{"location":"data.html#toi-193-ltt-9779","title":"TOI-193 (LTT 9779)","text":"<ul> <li>Star name: <code>toi-193</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/LTT%209779</li> <li>Spectral type: G7V</li> <li>Teff: ~5400-5500 K</li> <li>Age: ~2 Gyr</li> <li>Luminosity: ~ 0.7 L\u2609</li> <li>Mass: ~1.0 M\u2609</li> <li>Radius: ~0.95 R\u2609</li> <li>Source: MUSCLES extension</li> </ul>"},{"location":"data.html#wasp-127","title":"WASP-127","text":"<ul> <li>Star name: <code>wasp-127</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/WASP-127</li> <li>Spectral type: G5 (subgiant-like)</li> <li>Teff: ~5600-5800 K</li> <li>Age: ~10\u201312 Gyr</li> <li>Luminosity: ~1.8 L\u2609</li> <li>Mass: ~0.95\u20131.10 M\u2609</li> <li>Radius: ~1.3 R\u2609</li> <li>Source: MUSCLES extension</li> </ul>"},{"location":"data.html#wasp-17","title":"WASP-17","text":"<ul> <li>Star name: <code>wasp-17</code></li> <li>URL: https://exoplanetarchive.ipac.caltech.edu/overview/WASP-17</li> <li>Spectral type: F4\u2013F6V</li> <li>Teff: ~6550 K</li> <li>Age: ~3 Gyr</li> <li>Luminosity: ~4 L\u2609</li> <li>Mass: 1.35 M\u2609</li> <li>Radius: 1.57 R\u2609</li> <li>Source: MUSCLES extension</li> </ul>"},{"location":"data.html#using-a-phoenix-synthetic-spectrum","title":"Using a PHOENIX synthetic spectrum","text":"<p>To use a Med-Res PHOENIX synthetic spectrum, set:</p> <ul> <li><code>spectrum_source = \"phoenix\"</code></li> </ul> <p>PHOENIX spectra are stored under: - <code>$FWL_DATA/stellar_spectra/PHOENIX/</code></p> <p>Each subdirectory corresponds to a metallicity / alpha combination, e.g. <code>FeH-0.5_alpha+0.0/</code>.</p>"},{"location":"data.html#parameters","title":"Parameters","text":"<p>PHOENIX models are defined on a grid in:</p> <ul> <li><code>Teff</code> (K) \u2014 effective temperature</li> <li><code>log_g</code> (dex) \u2014 surface gravity</li> <li><code>FeH</code> (dex) \u2014 metallicity</li> <li><code>alpha</code> (dex) \u2014 alpha enhancement</li> </ul> <p>In addition, PROTEUS needs the stellar radius:</p> <ul> <li><code>radius</code> (R\u2609)</li> </ul> <p>This is used to scale the model spectrum (surface flux) to the flux at 1 AU.</p> <p>You can set these under <code>star.mors</code> in your config file.</p> <p>Defaults / fallbacks</p> <ul> <li>If <code>FeH</code> and/or <code>alpha</code> are not set, they default to solar (0.0).</li> <li>If <code>Teff</code>, <code>log_g</code>, and/or <code>radius</code> are not set, they are estimated by the stellar evolution module (<code>mors</code>) from the stellar mass (if provided).</li> </ul>"},{"location":"data.html#using-a-custom-stellar-spectrum","title":"Using a custom stellar spectrum","text":"<p>If you prefer to use your own stellar spectrum, you can input its filepath under the parameter <code>star_path</code>. Make sure you input its absolute path, even if it is in the $FWL_DATA directory. NOTE: this parameter will override all other stellar spectrum config parameters!</p> <p>There are a few things to take into account when using a custom stellar spectrum:</p> <ul> <li>The file should be a two-column ASCII file, with the first column the wavelength in nm, and the second column the flux in erg/s/cm^2/nm. Headers should be indicated with <code>#</code>.</li> <li>The spectrum must be scaled to 1 AU.</li> </ul>"},{"location":"data.html#surfaces","title":"Surfaces","text":"<p>Single-scattering albedo data taken from Hammond et al., (2024): Zenodo data. Available options can be found by running the command: <pre><code>ls $FWL_DATA/surface_albedos/Hammond24\n</code></pre></p>"},{"location":"data.html#exoplanet-population-data","title":"Exoplanet population data","text":"<p>These are obtained from the DACE PlanetS database (Parc et al., 2024).</p>"},{"location":"data.html#mass-radius-relations","title":"Mass-radius relations","text":"<p>These are obtained from Zeng et al., 2019.</p>"},{"location":"docker_ci_architecture.html","title":"Docker-Based CI/CD Architecture for PROTEUS","text":""},{"location":"docker_ci_architecture.html#what-this-document-is-for","title":"What This Document Is For","text":"<p>New to PROTEUS CI? This document explains how our Docker-based testing infrastructure works. Docker containers provide a consistent environment with pre-compiled physics modules, making CI runs fast and reproducible.</p> <p>Key concept: Instead of compiling SOCRATES, AGNI, PETSc, and SPIDER on every CI run (~60 min), we use a pre-built Docker image (~5 min startup).</p> <p>For test markers and categories, see Test Categorization. For coverage workflows, see Test Infrastructure. For writing tests, see Test Building.</p>"},{"location":"docker_ci_architecture.html#overview","title":"Overview","text":"<p>This architecture solves slow compilation times by using a pre-built Docker image containing the full PROTEUS environment with compiled physics modules. The image is built on demand and used by all CI/CD workflows.</p>"},{"location":"docker_ci_architecture.html#architecture-components","title":"Architecture Components","text":""},{"location":"docker_ci_architecture.html#1-dockerfile","title":"1. Dockerfile","text":"<p>Location: <code>/Dockerfile</code></p> <p>Purpose: Define the pre-built environment with all dependencies and compiled physics modules.</p> <p>Key Features: - Base: Python 3.12 on Debian Bookworm (slim) - System dependencies: gfortran, make, cmake, git, NetCDF libraries - Julia installation via official installer - Compiles all physics modules:   - SOCRATES (radiative transfer)   - PETSc (numerical computing)   - SPIDER (interior evolution)   - AGNI (radiative-convective atmosphere) - Installs Python packages from <code>pyproject.toml</code> - Optimized for size with cache cleanup</p> <p>Environment Variables: <pre><code>FWL_DATA=/opt/proteus/fwl_data\nRAD_DIR=/opt/proteus/socrates\nAGNI_DIR=/opt/proteus/AGNI\nPETSC_DIR=/opt/proteus/petsc\nPETSC_ARCH=arch-linux-c-opt\nPROTEUS_DIR=/opt/proteus\n</code></pre></p>"},{"location":"docker_ci_architecture.html#2-docker-buildyml-the-updater","title":"2. docker-build.yml (The Updater)","text":"<p>Location: <code>.github/workflows/docker-build.yml</code></p> <p>Purpose: Build and push the Docker image to GitHub Container Registry.</p> <p>Triggers: - Schedule: Nightly at 02:00 UTC - Push to <code>main</code> when dependencies change:   - <code>pyproject.toml</code>   - <code>environment.yml</code>   - <code>Dockerfile</code>   - <code>tools/get_*.sh</code> scripts</p> <p>Output: <code>ghcr.io/formingworlds/proteus:latest</code></p> <p>Tags: - <code>latest</code> (on main branch) - <code>&lt;branch&gt;-&lt;sha&gt;</code> (commit-specific) - <code>nightly-YYYYMMDD</code> (daily builds)</p> <p>Optimization: - BuildKit cache for faster rebuilds - Layer caching from previous builds - Multi-stage optimization potential</p>"},{"location":"docker_ci_architecture.html#3-ci-pr-checksyml-fast-feedback","title":"3. ci-pr-checks.yml (Fast Feedback)","text":"<p>Location: <code>.github/workflows/ci-pr-checks.yml</code></p> <p>Purpose: Fast PR validation using pre-built Docker image.</p> <p>Triggers: - Pull requests to <code>main</code> - Push to <code>main</code> - Manual dispatch</p> <p>Strategy: 1. Container: Runs inside <code>ghcr.io/formingworlds/proteus:latest</code> (or branch-specific tag) 2. Threshold check: Prevents coverage decreases vs main 3. Code Overlay: Overlays PR code onto container (excludes compiled modules) 4. Structure validation: <code>tools/validate_test_structure.sh</code> 5. Sequential testing: Unit \u2192 Smart rebuild \u2192 Smoke 6. Coverage coordination: Downloads nightly artifact for estimated total</p> <p>Jobs:</p> <ul> <li>unit-tests (Linux, Docker): Unit + smoke tests with coverage, coverage validation</li> <li>macos-unit-tests (macOS): Unit tests only (no compiled binaries available)</li> <li>lint: Code style checks with ruff</li> <li>summary: Aggregates results from all jobs into a unified report</li> </ul> <p>Linux job steps (in order):</p> <ol> <li>Prevent threshold decreases \u2014 Fails if <code>fail_under</code> decreased vs main</li> <li>Overlay PR code \u2014 <code>rsync</code> excludes SPIDER, SOCRATES, PETSc, AGNI</li> <li>Validate test structure \u2014 Ensures <code>tests/</code> mirrors <code>src/proteus/</code></li> <li>Run unit tests \u2014 <code>pytest -m \"unit and not skip\"</code> with coverage</li> <li>Smart rebuild \u2014 Recompile SOCRATES/AGNI only if sources changed</li> <li>Run smoke tests \u2014 <code>pytest -m \"smoke and not skip\"</code> (appends coverage)</li> <li>Download nightly coverage \u2014 For estimated total calculation</li> <li>Check staleness \u2014 Fails if nightly artifact &gt;48h old</li> <li>Validate coverage \u2014 Grace period of 0.3% for drops</li> <li>Diff-cover \u2014 80% coverage required on changed lines</li> </ol> <p>Coverage coordination: - Fast gate threshold from <code>[tool.proteus.coverage_fast] fail_under</code> (see <code>pyproject.toml</code>) - Estimated total = union of PR lines + nightly integration lines - Grace period allows \u22640.3% drop with warning - Diff-cover enforces 80% on changed lines</p> <p>See Test Categorization for marker details and Test Infrastructure for coverage thresholds.</p> <p>Key Innovation - Smart Rebuild: <pre><code>- name: Smart rebuild of physics modules\n  run: |\n    # Only rebuild if source files changed\n    cd SPIDER\n    make -q || make -j$(nproc)  # -q checks if build is up-to-date\n</code></pre></p> <p>Since the container already has compiled binaries: - If PR changes only Python files: No recompilation needed (~instant) - If PR changes Fortran/C files: Only changed files recompile (~seconds to minutes) - Full compilation avoided (~30-60 minutes saved)</p>"},{"location":"docker_ci_architecture.html#4-ci-nightlyyml-deep-validation","title":"4. ci-nightly.yml (Deep Validation)","text":"<p>Location: <code>.github/workflows/ci-nightly.yml</code></p> <p>Purpose: Comprehensive scientific validation and coverage baseline.</p> <p>Triggers: - Primary: Dispatched by <code>docker-build.yml</code> after the 2am UTC image rebuild - Fallback: Cron at 03:00 UTC (skips if a dispatch run already happened in the last 4 hours) - Manual dispatch</p> <p>Deduplication: A <code>check-already-triggered</code> guard job queries the GitHub API for recent <code>workflow_dispatch</code> runs. If the docker-build workflow already triggered the nightly, the 3am cron skips. On API failure, the cron proceeds as a safe default.</p> <p>Environment: - Sets <code>PROTEUS_CI_NIGHTLY=1</code> \u2014 enables additional smoke tests - Timeout: 240 minutes (4 hours) - Downloads ~200MB minimal data for smoke tests</p> <p>Strategy: 1. Check if already triggered by docker-build (skip if so) 2. Use branch-specific Docker image 3. Overlay code (excludes compiled modules) 4. Download minimal data (spectral files, stellar spectra, lookup tables) 5. Configure Julia environment for Python integration 6. Run all test tiers sequentially 7. Upload aggregate coverage to Codecov 8. Generate coverage artifacts for PR coordination 9. Ratchet coverage threshold on success</p> <p>Test sequence: 1. Unit tests \u2014 <code>pytest -m \"unit and not skip\"</code> with coverage 2. Smoke tests \u2014 <code>pytest -m \"smoke and not skip\"</code> (coverage appended) 3. Integration tests \u2014 <code>pytest -m \"integration and not slow\"</code> (coverage appended) 4. Slow tests \u2014 <code>pytest -m slow</code> (if time permits)</p> <p>Codecov upload: - Uploads combined <code>coverage.xml</code> (unit + smoke + integration) under the <code>nightly</code> flag - Configured in <code>codecov.yml</code> with <code>carryforward: true</code> so data persists across PR evaluations - This is what the Codecov coverage badge in the README reflects</p> <p>Artifacts uploaded: - <code>nightly-coverage/coverage-integration-only.json</code> \u2014 For PR estimated total - <code>nightly-coverage/nightly-timestamp.txt</code> \u2014 For staleness detection - <code>nightly-coverage/coverage-by-type.json</code> \u2014 Breakdown by test type</p> <p>Coverage ratcheting: - Full threshold from <code>[tool.coverage.report] fail_under</code> (see <code>pyproject.toml</code>) - Auto-commits threshold increase on successful main runs</p> <p>See Test Infrastructure for coverage coordination details.</p>"},{"location":"docker_ci_architecture.html#test-markers","title":"Test Markers","text":"<p>Tests are categorized using pytest markers defined in <code>pyproject.toml</code>:</p> <pre><code># Unit test (fast, mocked physics)\n@pytest.mark.unit\ndef test_config_parsing():\n    # Test Python logic without heavy dependencies\n    pass\n\n# Smoke test (quick real binary check)\n@pytest.mark.smoke\ndef test_spider_single_timestep():\n    # Run SPIDER for 1 timestep at low resolution\n    # Ensures binary actually works\n    pass\n\n# Integration test (multi-module)\n@pytest.mark.integration\ndef test_atmosphere_interior_coupling():\n    # Test interaction between JANUS and SPIDER\n    pass\n\n# Slow test (full scientific validation)\n@pytest.mark.slow\ndef test_earth_evolution_1gyr():\n    # Run full 1 Gyr simulation\n    # Validate against known results\n    pass\n</code></pre>"},{"location":"docker_ci_architecture.html#workflow-sequence","title":"Workflow Sequence","text":""},{"location":"docker_ci_architecture.html#nightly-main-branch","title":"Nightly (Main Branch)","text":"<pre><code>02:00 UTC: docker-build.yml\n  \u2193\n  Rebuild Docker image\n  \u2193\n  Trigger ci-nightly.yml via workflow_dispatch\n  \u2193\n03:00 UTC: ci-nightly.yml cron (fallback)\n  \u2193\n  check-already-triggered job\n  \u2193  (skips if dispatch run found in last 4h)\n  Pull Docker image\n  \u2193\n  Overlay code, download data (~200MB)\n  \u2193\n  Run unit tests with coverage\n  \u2193\n  Run smoke tests (PROTEUS_CI_NIGHTLY=1 enables extras)\n  \u2193\n  Run integration tests\n  \u2193\n  Run slow tests (if time permits)\n  \u2193\n  Upload aggregate coverage to Codecov (nightly flag)\n  \u2193\n  Upload nightly-coverage artifact\n  \u2193\n  Ratchet threshold if coverage increased\n</code></pre>"},{"location":"docker_ci_architecture.html#pull-request","title":"Pull Request","text":"<pre><code>PR opened/updated\n  \u2193\nci-pr-checks.yml (3 parallel jobs + summary)\n  \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 unit-tests (Linux)   \u2502 macos-unit-tests \u2502 lint       \u2502\n\u2502 Pull Docker image    \u2502 Setup Python     \u2502 ruff check \u2502\n\u2502 Overlay PR code      \u2502 pip install      \u2502 ruff format\u2502\n\u2502 Validate structure   \u2502 Run unit tests   \u2502            \u2502\n\u2502 Unit tests + cov     \u2502                  \u2502            \u2502\n\u2502 Smart rebuild        \u2502                  \u2502            \u2502\n\u2502 Smoke tests          \u2502                  \u2502            \u2502\n\u2502 Coverage validation  \u2502                  \u2502            \u2502\n\u2502 Diff-cover (80%)     \u2502                  \u2502            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2193\n                    summary job\n                    (unified report)\n                          \u2193\n                  Fast feedback (~10-15 min)\n</code></pre>"},{"location":"docker_ci_architecture.html#benefits","title":"Benefits","text":""},{"location":"docker_ci_architecture.html#speed-improvements","title":"Speed Improvements","text":"<ul> <li>Before: Every PR compiles SOCRATES, PETSc, SPIDER, AGNI (~60 minutes)</li> <li>After: Use pre-built image, smart rebuild only (~5-10 minutes for Python-only changes)</li> <li>Savings: ~50 minutes per PR iteration</li> </ul>"},{"location":"docker_ci_architecture.html#resource-efficiency","title":"Resource Efficiency","text":"<ul> <li>Docker layer caching reduces rebuild time</li> <li>Smart recompilation only builds changed files</li> <li>Parallel job execution where possible</li> </ul>"},{"location":"docker_ci_architecture.html#scientific-rigor","title":"Scientific Rigor","text":"<ul> <li>Nightly comprehensive validation ensures correctness</li> <li>PR checks provide fast feedback without compromising quality</li> <li>Separation of fast unit tests from slow integration tests</li> </ul>"},{"location":"docker_ci_architecture.html#developer-experience","title":"Developer Experience","text":"<ul> <li>Fast PR checks (~10-15 min) enable rapid iteration</li> <li>Clear test markers guide test writing</li> <li>Comprehensive nightly validation catches regressions</li> </ul>"},{"location":"docker_ci_architecture.html#image-maintenance","title":"Image Maintenance","text":""},{"location":"docker_ci_architecture.html#when-docker-image-rebuilds","title":"When Docker Image Rebuilds","text":"<ol> <li>Nightly at 02:00 UTC (scheduled)</li> <li>Changes to <code>pyproject.toml</code> (dependency updates)</li> <li>Changes to <code>environment.yml</code> (conda dependencies)</li> <li>Changes to <code>Dockerfile</code> (build process)</li> <li>Changes to <code>tools/get_*.sh</code> (compilation scripts)</li> </ol>"},{"location":"docker_ci_architecture.html#image-size-management","title":"Image Size Management","text":"<ul> <li>Cleanup layers remove apt cache, Python cache</li> <li>Multi-stage builds potential for further optimization</li> <li>Current estimated size: ~2-3 GB (with compiled modules)</li> </ul>"},{"location":"docker_ci_architecture.html#cache-strategy","title":"Cache Strategy","text":"<ul> <li>BuildKit cache stored in registry</li> <li>Layer caching from previous builds</li> <li>Fast incremental builds</li> </ul>"},{"location":"docker_ci_architecture.html#coverage-coordination","title":"Coverage Coordination","text":"<p>The two-tier coverage system coordinates between nightly and PR workflows:</p> Feature Value Description Fast gate <code>pyproject.toml</code> PR threshold (unit + smoke) Full gate <code>pyproject.toml</code> Nightly threshold (all tests) Grace period 0.3% PRs can merge with small drops Staleness 48h PR fails if nightly too old Diff-cover 80% Required on changed lines <p>How estimated total works: 1. PR runs unit + smoke \u2192 <code>coverage-unit.json</code> 2. Download nightly's <code>coverage-integration-only.json</code> 3. Compute union of covered lines 4. Compare against full threshold</p> <p>See Test Infrastructure for threshold details.</p>"},{"location":"docker_ci_architecture.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"docker_ci_architecture.html#image-build-fails","title":"Image Build Fails","text":"<ul> <li>Check GitHub Actions logs in <code>docker-build.yml</code></li> <li>Verify compilation scripts work locally</li> <li>Test Dockerfile locally: <code>docker build -t proteus-test .</code></li> </ul>"},{"location":"docker_ci_architecture.html#smart-rebuild-not-working","title":"Smart Rebuild Not Working","text":"<ul> <li>Verify make is installed in container</li> <li>Check if Makefiles are copied correctly</li> <li>Manual rebuild: Remove binaries and rebuild</li> </ul>"},{"location":"docker_ci_architecture.html#tests-fail-in-container","title":"Tests Fail in Container","text":"<ul> <li>Test locally with: <code>docker run -it ghcr.io/formingworlds/proteus:latest bash</code></li> <li>Verify environment variables are set</li> <li>Check file permissions</li> </ul>"},{"location":"docker_ci_architecture.html#image-too-large","title":"Image Too Large","text":"<ul> <li>Review cleanup steps in Dockerfile</li> <li>Consider multi-stage builds</li> <li>Analyze layers: <code>docker history ghcr.io/formingworlds/proteus:latest</code></li> </ul>"},{"location":"docker_ci_architecture.html#future-enhancements","title":"Future Enhancements","text":"<ol> <li>Multi-architecture Support: Build for ARM64 (Apple Silicon)</li> <li>Version Tagging: Semantic versioning for stable releases</li> <li>Matrix Testing: Multiple Python versions (3.11, 3.12, 3.13)</li> <li>Performance Profiling: Benchmark tests across versions</li> <li>Artifact Caching: Cache FWL_DATA between runs</li> </ol>"},{"location":"docker_ci_architecture.html#references","title":"References","text":""},{"location":"docker_ci_architecture.html#proteus-documentation","title":"PROTEUS Documentation","text":"<ul> <li>Test Infrastructure \u2014 Coverage workflows, thresholds, troubleshooting</li> <li>Test Categorization \u2014 Test markers, CI pipelines, fixtures</li> <li>Test Building \u2014 Writing tests, prompts, best practices</li> <li>AI-Assisted Development \u2014 Using AI for tests and code review</li> </ul>"},{"location":"docker_ci_architecture.html#external-resources","title":"External Resources","text":"<ul> <li>Docker Best Practices</li> <li>GitHub Actions: Container Jobs</li> <li>pytest Markers</li> <li>coverage.py Documentation</li> </ul>"},{"location":"funding.html","title":"PROTEUS Funding Sources","text":"<p>We are thankful for the continous support of the larger scientific community and funding by the public of fundamental research. The development of PROTEUS has been made possible by the invaluable support of the following organisations:</p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p>"},{"location":"habrok_cluster_guide.html","title":"Habrok Cluster Guide","text":""},{"location":"habrok_cluster_guide.html#access-the-habrok-cluster","title":"Access the Habrok Cluster","text":"<p>You will need a RUG account, with an account name (e.g. <code>p321401</code>) and two-factor authentication set up. To do this, first follow the instructions on the online documentation.</p> <p>We recommend that you also add your public ssh key to Habrok. Doing so allows password-free connectivity: <pre><code>ssh-keygen -t rsa\nssh-copy-id -i ~/.ssh/id_rsa.pub YOUR_USERNAME@login1.hb.hpc.rug.nl\n</code></pre></p> <p>Once you have added your SSH key to Habrok, modify the entry below and insert it into your <code>~/.ssh/config</code> file <pre><code>Host habrok1\n    HostName interactive1.hb.hpc.rug.nl\n    User YOUR_USERNAME\n    IdentityFile ~/.ssh/id_rsa\n    ServerAliveInterval 120\n    ServerAliveCountMax 60\n</code></pre></p>"},{"location":"habrok_cluster_guide.html#configure-environment","title":"Configure environment","text":"<p>Once you are connected to one of the interactive servers, use these steps to configure your environment before running PROTEUS.</p> <ol> <li> <p>We need to configure the correct modules. Run the following commands to set your bashrc file:     <pre><code>echo \"module purge\" &gt;&gt;  \"$HOME/.bashrc\"\n</code></pre> <pre><code>echo \"module load netCDF-Fortran libarchive\"  &gt;&gt;  \"$HOME/.bashrc\"\n</code></pre></p> </li> <li> <p>Log out of Habrok, and then login again</p> </li> <li> <p>You can now follow the usual installation steps here.</p> </li> </ol>"},{"location":"habrok_cluster_guide.html#file-system-partitioning","title":"File system partitioning","text":"<p>The standard size of your home folder is 50 GB. This is sufficient for installing PROTEUS and its submodules, but not for storing output data. You should store output data in your personal folder within <code>/scratch/</code>, which is accessible to the compute nodes. Since <code>/scratch/</code> is frequently emptied, you should then copy important data to <code>/projects/</code> for long term storage.</p> <p>See the information on the HPC wiki for details.</p> <p>The best way to organise this is to create a symbolic link from the PROTEUS output folder to <code>/scratch</code>. Once you have installed PROTEUS, this can be done by running the following commands inside your PROTEUS folder: <pre><code>rm -rf output\nmkdir /scratch/$USER/proteus_output\nln -sf /scratch/$USER/proteus_output output\n</code></pre> Anything written to <code>output/</code> will then be stored inside the <code>/scratch</code> partition.</p>"},{"location":"habrok_cluster_guide.html#resource-limits","title":"Resource limits","text":"<p>Each PROTEUS simulation should be allocated at least 2 GB of memory - ideally 3 GB. Habrok limits job runtime to a maximum of 10 days on the \"regular\" node partition. The parallel and GPU partitions are limited to 5 and 3 days respectively; they should be avoided since PROTEUS will not benefit from these.</p>"},{"location":"habrok_cluster_guide.html#submitting-jobs-and-running-grids-of-simulations","title":"Submitting jobs, and running grids of simulations","text":"<p>There is information on the HPC wiki on how to submit jobs with SLURM.</p> <ul> <li> <p>To submit a generic script, run:     <pre><code>sbatch name_of_your_script.sh\n</code></pre></p> </li> <li> <p>To check the status of your jobs, use:     <pre><code>squeue -u $USER\n</code></pre></p> </li> </ul> <p>See the section on running grids in the PROTEUS usage guide. These instructions will detail how to submit grids to the nodes via SLURM.</p> <p>You can also submit single PROTEUS runs to the nodes. For example: <pre><code>sbatch --mem-per-cpu=3G --time=1440 --wrap \"proteus start -oc input/all_options.toml\"\n</code></pre></p>"},{"location":"inference.html","title":"Asynchronous Bayesian Optimization for PROTEUS","text":"<p>This project implements parallel-asynchronous Bayesian Optimization (BO) for parameter inference using PROTEUS as the  'simulator'. It uses multiple workers to efficiently explore the parameter space and find optimal matches between simulated and observed planetary characteristics. You can also run this BO inference scheme to refine the results of a grid.</p>"},{"location":"inference.html#overview","title":"Overview","text":"<p>The system performs Bayesian optimization to infer planetary formation parameters by:</p> <ol> <li>Running PROTEUS simulations with different parameter combinations</li> <li>Comparing simulated observables (planet radius, mass, transit depth, etc.) with target values</li> <li>Using Gaussian Process surrogates and acquisition functions to guide the search toward optimal parameters</li> <li>Employing multiple parallel workers asynchronously to accelerate the optimization process</li> </ol> Project structure (developer reference) <p>These files are contained within the folder <code>src/proteus/inference/</code>.</p> File Description <code>inference.py</code> Main entry point <code>async_BO.py</code> Parallel BO implementation <code>BO.py</code> Single BO step implementation <code>objective.py</code> PROTEUS interface and objective function <code>plot.py</code> Visualization utilities <code>utils.py</code> Helper functions for inference scheme <code>gen_D_init.py</code> Generate initial data"},{"location":"inference.html#configuration","title":"Configuration","text":"<p>The main configuration is done through a TOML-formatted configuration file. There are two ways to initialise the inference process:</p> <ol> <li>Allowing PROTEUS to randomly sample the parameter space provided in the config.</li> <li>Using the result of a previously-computed grid of models.</li> </ol> <p>To apply case (1), set the config variable <code>init_samps=4</code> to use 4 initial samples. You can choose any number greater than 2, but ideally less than 10. Then set <code>init_grid='none'</code>.</p> <p>If you instead wish to initialise under case (2), where a pre-computed grid provides the initial samples, set the config variable <code>init_grid='outname'</code> where <code>outname</code> is the name of the folder containing the grid inside the shared PROTEUS output folder. Then set <code>init_samps='none'</code>.</p> <p>An example configuration file is shown below.</p> <pre><code># Set seed for reproducibility\nseed = 2\n\n# Path to output folder where inference will be saved (relative to PROTEUS output folder)\noutput = \"infer_demo/\"\n\n# Path to base (reference) config file relative to PROTEUS root folder\nref_config = \"input/demos/dummy.toml\"\n\n# Method for initialising the inference scheme (one of these must be 'none')\ninit_samps = '2'         # Number of random samples if starting from scratch.\ninit_grid  = 'none' # grid_demo/'   # Path pre-computed grid (relative to PROTEUS output folder)\n\n# Parameters for Bayesian optimisation\nn_workers  = 7        # Number of parallel workers\nkernel     = \"MAT\"    # Kernel type for GP, \"RBF\" | \"MAT\"\nacqf       = \"LogEI\"  # Acquisition function, \"UCB\" | \"LogEI\"\nn_steps    = 30       # Total number of evaluations (i.e. BO steps)\nn_restarts = 10       # GP optimization restarts\nn_samples  = 1000     # Raw samples for acquisition optimization\n\n# Parameters to optimize (with bounds)\n[parameters]\n\"struct.mass_tot\" = [0.7, 3.0]\n\"struct.corefrac\" = [0.3, 0.9]\n\"delivery.elements.H_ppmw\" = [6e3, 2e4]\n\"outgas.fO2_shift_IW\" = [-3.0, 5.0]\n\n# Target observables to match by optimisation\n[observables]\n\"R_obs\" = 7.9950245489e6\n\"H2O_vmr\" = 0.41\n</code></pre>"},{"location":"inference.html#usage","title":"Usage","text":"<p>Execute the main optimisation process by using the PROTEUS command-line interface</p> <pre><code>proteus infer --config input/ensembles/example.infer.toml\n</code></pre> <p>In this case, we randomly sample the parameter space to provide a starting point for the optimisation. This process must stay open in order to manage the workers.</p>"},{"location":"inference.html#how-it-works","title":"How It Works","text":""},{"location":"inference.html#objective-function","title":"Objective Function","text":"<p>The system optimizes an objective function that measures how well simulated observables match target values:</p> <pre><code>J = 1 - ||1 - sim/true||\u00b2\n</code></pre> <p>Where <code>sim</code> are the simulated observables and <code>true</code> are the target values. This means that the 'best' value for the objective function is 1. Values closer to 1 represent better fits, while smaller values (including negative ones) are worse fits.</p>"},{"location":"inference.html#parallel-processing","title":"Parallel Processing","text":"<ul> <li>Multiple workers run simultaneously, each performing BO steps</li> <li>Workers share a common dataset but operate independently</li> <li>Lock mechanisms prevent race conditions when updating shared data</li> <li>Each worker tracks \"busy\" locations to avoid redundant evaluations</li> </ul>"},{"location":"inference.html#bayesian-optimization","title":"Bayesian Optimization","text":"<ul> <li>Uses Gaussian Process (GP) models to predict objective values</li> <li>Acquisition function guides exploration-exploitation trade-off on search space</li> <li>Automatic hyperparameter tuning via marginal likelihood optimization</li> </ul> <p>The optimization will run until <code>n_steps</code> evaluations are completed or manually stopped. Results are continuously saved and can be resumed if needed.</p>"},{"location":"inference.html#output","title":"Output","text":"<p>The system generates several outputs in:</p>"},{"location":"inference.html#data-files","title":"Data Files","text":"<ul> <li><code>data.pkl</code>: Final dataset with all evaluated parameters and objectives</li> <li><code>logs.pkl</code>: Detailed logs of each BO step</li> <li><code>Ts.pkl</code>: Timestamps for performance analysis</li> <li><code>init.pkl</code>: Data used as an initial guess for starting the optimisation</li> </ul>"},{"location":"inference.html#plots","title":"Plots","text":"<p>The BO scheme will generate many plots upon completion. Those prefixed with <code>perf_</code> diagnose the performance of the optimisation.</p> <ul> <li><code>perf_parallel.png</code>: Timeline showing parallel worker execution</li> <li><code>perf_timehist.png</code>: Distribution of total evaluation times</li> <li><code>perf_BO_timehist.png</code>: Distribution of BO computation times</li> <li><code>perf_eval_timehist.png</code>: Distribution of PROTEUS evaluation times</li> <li><code>perf_fit_timehist.png</code>: Distribution of GP fitting times</li> <li><code>perf_ac_timehist.png</code>: Distribution of acquisition optimization times</li> <li><code>perf_distance_iters.png</code>: Distance between queries and busy locations</li> <li><code>perf_regret.png</code>: Convergence plots (regret vs time/iterations)</li> <li><code>perf_bestval.png</code>: Best objective value evolution</li> </ul> <p>Plots prefixed with <code>result_</code> show the results of the optimisation.</p> <ul> <li><code>result_correlation.png</code>: Scatter plot observables for each parameter, at each sample.</li> <li><code>result_objective.png</code>: Value of objective <code>J</code> for each parameter, at each sample.</li> </ul>"},{"location":"inference.html#results-summary","title":"Results Summary","text":"<p>The system prints the final results including: - Best found parameters - Corresponding simulated observables - Comparison with target observables</p>"},{"location":"inference.html#customization","title":"Customization","text":""},{"location":"inference.html#adding-new-parameters","title":"Adding New Parameters","text":"<ol> <li>Update the <code>[parameters]</code> section in your inference config file</li> <li>Ensure the parameter names match PROTEUS configuration keys</li> </ol>"},{"location":"inference.html#changing-observables","title":"Changing Observables","text":"<ol> <li>Update the <code>[observables]</code> section with your target values</li> <li>Make sure these observables are output by PROTEUS</li> </ol>"},{"location":"inference.html#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Set <code>n_workers</code> to be less than your CPU core count minus 1</li> <li>The system automatically limits thread usage to prevent oversubscription</li> <li>PROTEUS evaluation time typically dominates total runtime</li> </ul>"},{"location":"installation.html","title":"Installation","text":"<p>Prerequisites</p> <ul> <li>macOS (Intel or Apple Silicon) or Linux</li> <li>~20 GB disk space (conda, Julia, reference data, submodules)</li> <li>Git with SSH key configured (GitHub SSH setup)</li> <li>Internet connection for data downloads</li> <li>Allow ~60 minutes for a full installation including all submodules</li> </ul> <p>These instructions will guide you through the typical installation process. The setup is written for macOS and Linux. Depending on your system settings and installed libraries your procedure may differ. If one or more of the steps below do not work for you we encourage you to first check the Troubleshooting page. If that does not help you further, please contact the developers.</p> <p>macOS users</p> <p>macOS Catalina (10.15) and later uses <code>zsh</code> as the default shell. Replace <code>.bashrc</code> with <code>.zshrc</code> throughout these instructions if you are using the default shell.</p>"},{"location":"installation.html#1-system-pre-configuration","title":"1. System pre-configuration","text":"<p>Setting up PROTEUS and its submodules requires extra steps to be performed before following the rest of this guide. Follow the instructions below depending on your system configuration.</p> <p>Local machine (laptop/desktop): follow the appropriate section in the Local machine guide.</p> <p>Compute cluster: use the dedicated guides:</p> <ul> <li>Kapteyn cluster</li> <li>Habrok cluster</li> <li>Snellius cluster</li> </ul>"},{"location":"installation.html#2-setup-a-python-environment","title":"2. Setup a Python environment","text":"<p>We recommend Python version 3.12 for running PROTEUS. Python is most easily obtained and managed using either miniconda or miniforge.</p> LinuxmacOS <p>Install miniconda:</p> <pre><code>mkdir -p ~/miniconda3\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\nbash ~/miniconda3/miniconda.sh\nrm ~/miniconda3/miniconda.sh\n</code></pre> <p>Choose an install folder where you have plenty of disk space.</p> <p>Install miniforge (recommended for Apple Silicon):</p> <pre><code>curl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh\n</code></pre>"},{"location":"installation.html#3-install-julia","title":"3. Install Julia","text":"<p>Some PROTEUS modules are written in Julia. You should only obtain Julia using the official installer, not via your computer's package manager.</p> <pre><code>curl -fsSL https://install.julialang.org | sh\n</code></pre> <p>Pin Julia to version 1.11</p> <p>Julia 1.12+ is not yet supported due to OpenSSL library incompatibilities with Python. After installing Julia, pin it to version 1.11:</p> <pre><code>juliaup add 1.11\njuliaup default 1.11\n</code></pre> <p>Set the Julia environment variable:</p> bashzsh <pre><code>echo \"export PYTHON_JULIAPKG_EXE=$(which julia)\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <pre><code>echo \"export PYTHON_JULIAPKG_EXE=$(which julia)\" &gt;&gt; ~/.zshrc\nsource ~/.zshrc\n</code></pre>"},{"location":"installation.html#4-create-and-set-environment-variables","title":"4. Create and set environment variables","text":"<p>The environment variable <code>FWL_DATA</code> points to the folder where input data are stored. This variable must always be set, so add it to your shell config file.</p> bashzsh <pre><code>mkdir /your/local/path/FWL_DATA\necho \"export FWL_DATA=/your/local/path/FWL_DATA/\" &gt;&gt; \"$HOME/.bashrc\"\nsource \"$HOME/.bashrc\"\n</code></pre> <pre><code>mkdir /your/local/path/FWL_DATA\necho \"export FWL_DATA=/your/local/path/FWL_DATA/\" &gt;&gt; \"$HOME/.zshrc\"\nsource \"$HOME/.zshrc\"\n</code></pre>"},{"location":"installation.html#5-download-proteus","title":"5. Download PROTEUS","text":"<pre><code>git clone git@github.com:FormingWorlds/PROTEUS.git\ncd PROTEUS\n</code></pre>"},{"location":"installation.html#6-create-a-virtual-environment","title":"6. Create a virtual environment","text":"<pre><code>conda create -n proteus python=3.12\nconda activate proteus\n</code></pre>"},{"location":"installation.html#7-install-socrates-radiative-transfer","title":"7. Install SOCRATES (radiative transfer)","text":"<pre><code>./tools/get_socrates.sh\n</code></pre> <p>The environment variable <code>RAD_DIR</code> must always point to the SOCRATES installation path. Add it to your shell config file:</p> bashzsh <pre><code>echo \"export RAD_DIR=$PWD/socrates/\" &gt;&gt; \"$HOME/.bashrc\"\nsource \"$HOME/.bashrc\"\n</code></pre> <pre><code>echo \"export RAD_DIR=$PWD/socrates/\" &gt;&gt; \"$HOME/.zshrc\"\nsource \"$HOME/.zshrc\"\n</code></pre>"},{"location":"installation.html#8-install-agni-radiative-convective-atmosphere-model","title":"8. Install AGNI (radiative-convective atmosphere model)","text":"<p>Installation steps can be found at the AGNI wiki. They are also reproduced below.</p> <pre><code>git clone git@github.com:nichollsh/AGNI.git\ncd AGNI\nbash src/get_agni.sh 0\ncd ../\n</code></pre> <p>Use this <code>get_agni.sh</code> script to keep AGNI and its data files up to date. AGNI must be available at <code>./AGNI/</code> inside your PROTEUS folder (either a symbolic link or the true location).</p> <p>The argument provided to the script (integer from 0 to 20) indicates which tests AGNI should run. A value of <code>0</code> means the tests are skipped.</p>"},{"location":"installation.html#9-install-submodules-as-editable","title":"9. Install submodules as editable","text":"<p>Clone and install each submodule in editable mode.</p> <p>MORS (stellar evolution):</p> <pre><code>git clone git@github.com:FormingWorlds/MORS\npython -m pip install -e MORS/.\n</code></pre> <p>JANUS (1D convective atmosphere):</p> <pre><code>git clone git@github.com:FormingWorlds/JANUS\npython -m pip install -e JANUS/.\n</code></pre> <p>CALLIOPE (volatile in-/outgassing):</p> <pre><code>git clone git@github.com:FormingWorlds/CALLIOPE\npython -m pip install -e CALLIOPE/.\n</code></pre> <p>ARAGOG (interior thermal evolution):</p> <pre><code>git clone git@github.com:FormingWorlds/aragog.git\npython -m pip install -e aragog/.\n</code></pre> <p>ZEPHYRUS (atmospheric escape):</p> <pre><code>git clone git@github.com:FormingWorlds/ZEPHYRUS\npython -m pip install -e ZEPHYRUS/.\n</code></pre>"},{"location":"installation.html#10-setup-petsc-numerical-computing-library","title":"10. Setup PETSc (numerical computing library)","text":"<p>Warning</p> <p>PETSc requires Python &lt;= 3.12. Make sure your active environment uses a compatible version.</p> LinuxmacOS <pre><code>./tools/get_petsc.sh\n</code></pre> <p>Fedora/RHEL users</p> <p>If you encounter errors moving libraries, see Troubleshooting: PETSc on Fedora/RHEL.</p> <p>First try the standard script:</p> <pre><code>./tools/get_petsc.sh\n</code></pre> <p>If compilation fails on Apple Silicon (M1/M2/M3/Ultra), run the fix script:</p> <pre><code>./tools/fix_petsc_compile.sh\n</code></pre> <p>See Troubleshooting: PETSc on Apple Silicon for manual steps if needed.</p>"},{"location":"installation.html#11-setup-spider-interior-evolution-model","title":"11. Setup SPIDER (interior evolution model)","text":"<pre><code>./tools/get_spider.sh\n</code></pre>"},{"location":"installation.html#12-install-proteus-framework","title":"12. Install PROTEUS framework","text":"<pre><code>python -m pip install -e \".[develop]\"\n</code></pre>"},{"location":"installation.html#13-enable-pre-commit-hooks","title":"13. Enable pre-commit hooks","text":"<pre><code>pre-commit install -f\n</code></pre>"},{"location":"installation.html#14-done","title":"14. Done!","text":"<p>Any remaining dependencies will be downloaded when the model is first run.</p>"},{"location":"installation.html#optional-modules","title":"Optional modules","text":""},{"location":"installation.html#multi-phase-tidal-heating-model-lovepy","title":"Multi-phase tidal heating model (LovePy)","text":"<p>LovePy is written in Julia. You can use the same environment as AGNI if you wish, but make sure to follow the installation steps below.</p> <pre><code>./tools/get_lovepy.sh\n</code></pre>"},{"location":"installation.html#synthetic-observations-calculator-platon","title":"Synthetic observations calculator (PLATON)","text":"<p>PLATON is a forward modelling and retrieval tool for exoplanet atmospheres. In PROTEUS, this tool is used to generate synthetic transmission and secondary eclipse observations.</p> <pre><code>./tools/get_platon.sh\n</code></pre> <p>Note</p> <p>This script will take some time to run; PLATON will need to download about 10 GB of data from the internet.</p>"},{"location":"installation.html#chemical-kinetics-atmosphere-model-vulcan","title":"Chemical kinetics atmosphere model (VULCAN)","text":"<p>VULCAN is not available as a standard Python package, so it is installed via a dedicated script:</p> <pre><code>./tools/get_vulcan.sh\n</code></pre> <p>For the legacy user install method (deprecated), see the User install page.</p>"},{"location":"kapteyn_cluster_guide.html","title":"Kapteyn Cluster Guide","text":""},{"location":"kapteyn_cluster_guide.html#access-the-kapteyn-cluster-via-vs-code","title":"Access the Kapteyn cluster via VS Code","text":"<p>Follow the instructions at VS Code Instructions Kapteyn Cluster to set up your VS Code environment for the Kapteyn cluster. This allows you to use the Kapteyn cluster as a remote server, enabling you to edit PROTEUS files and run simulations directly from your local machine.</p>"},{"location":"kapteyn_cluster_guide.html#installation","title":"Installation","text":"<ol> <li> <p>If you have not followed the VS Code Instructions above, then now manage your authentication keys to avoid entering your password every time you connect (optional). You can find the instructions on the Kapteyn intranet: How to generate authentication keys for SSH, SFTP, and SCP (Go to Computing &gt; Howto's &gt; How to generate authentication keys for ssh, sftp and scp):</p> <p><pre><code>ssh-keygen -t rsa\n</code></pre> Press Enter to accept the default file location and enter a passphrase if desired. This will create a public/private key pair in <code>~/.ssh/</code>.</p> <p>Then, copy the public key to the Kapteyn cluster: <pre><code>ssh-copy-id -i ~/.ssh/id_rsa.pub &lt;username&gt;@kapteyn.astro.rug.nl\n</code></pre></p> <p>Finally, add the following entry in your <code>~/.ssh/config</code> file, making sure to add your username where appropriate. <pre><code>Host kapteyngateway\n    HostName kapteyn.astro.rug.nl\n    User YOUR_USERNAME_HERE\n    IdentityFile ~/.ssh/id_rsa\n    ForwardAgent yes\n\nHost norma2\n    HostName norma2\n    User YOUR_USERNAME_HERE\n    IdentityFile ~/.ssh/id_rsa\n    ProxyJump kapteyngateway\n    ServerAliveInterval 120\n    ServerAliveCountMax 60\n</code></pre></p> <p>You can now log in without entering your password.</p> </li> <li> <p>Connect to the cluster via SSH. Use <code>norma2</code> whenever possible.</p> <pre><code>ssh norma2\n</code></pre> </li> <li> <p>Create a folder with your username in <code>/dataserver/users/formingworlds/</code>. If you cannot create a folder in there, please contact Tim Lichtenberg to get access rights.</p> <pre><code>mkdir -p /dataserver/users/formingworlds/&lt;username&gt;\ncd /dataserver/users/formingworlds/&lt;username&gt;\n</code></pre> </li> <li> <p>To avoid the cluster terminating PROTEUS jobs, increase the temporary file limit for your user by adding to your shell rc file (e.g., '~/.bashrc'):     <pre><code>echo \"ulimit -Sn 4000000\" &gt;&gt; \"$HOME/.bashrc\"\necho \"ulimit -Hn 5000000\" &gt;&gt; \"$HOME/.bashrc\"\n</code></pre>     Then, reload your shell rc file to make the changes effective:     <pre><code>source \"$HOME/.bashrc\"\n</code></pre></p> </li> <li> <p>You can now follow the usual installation steps here, but, since your home folder is capped    at 9GB, you need to install Julia and miniconda or conda-forge in \"/dataserver/users/formingworlds/\".     ### Julia considerations     If you have already installed Julia in your home folder, you could remove that through <code>rm -rf ~/.julia</code>. <p>If you install Julia through Juliaup this involves: <pre><code>export JULIAUP_HOME=/dataserver/users/formingworlds/&lt;username&gt;/.juliaup\ncurl -fsSL https://install.julialang.org | sh\n</code></pre></p> <p>To also make sure that the Julia ecosystem, such as Julia packages, are also not installed in <code>$HOME</code>, add <code>JULIA_DEPOT_PATH</code> to your <code>~/.shellrc</code>, e.g. <code>~/.bashrc</code>: <pre><code>export JULIA_DEPOT_PATH=/dataserver/users/formingworlds/&lt;username&gt;/.julia\n</code></pre> Setting only this variable will be sufficient if you have not installed Julia through Juliaup. In any case, it is best to have both of these Julia environment variables exported when you log in, so please add this to your <code>~/.shellrc</code>, e.g. <code>~/.bashrc</code>: <pre><code>export JULIAUP_HOME=/dataserver/users/formingworlds/&lt;username&gt;/.juliaup\nexport JULIA_DEPOT_PATH=\"/dataserver/users/formingworlds/&lt;username&gt;/.julia\"\n</code></pre> If you install Julia using <code>tar</code>, use the following steps:</p> <pre><code> export JULIA_DIR=/dataserver/users/formingworlds/&lt;username&gt;/julia-1.11.6\n\n mkdir -p $JULIA_DIR\n\n cd /dataserver/users/formingworlds/&lt;username&gt;\n\n wget https://julialang-s3.julialang.org/bin/linux/x64/1.11/julia-1.11.6-linux-x86_64.tar.gz\n\n tar -xvzf julia-1.11.6-linux-x86_64.tar.gz\n\n echo 'export PATH=/dataserver/users/formingworlds/&lt;username&gt;/julia-1.11.6/bin:$PATH' &gt;&gt; ~/.bashrc\n\n echo 'export JULIA_DEPOT_PATH=/dataserver/users/formingworlds/&lt;username&gt;/.julia' &gt;&gt; ~/.bashrc\n\n source ~/.bashrc\n</code></pre> <pre><code>### Miniconda and conda-forge considerations\nWhen installing miniconda or conda-forge, make sure you do not choose the default path, which is always your home folder. Adjust it to `/dataserver/users/formingworlds/&lt;username&gt;`.\nAlternatively, you can set default paths upfront for miniconda:\n```console\nmkdir -p /dataserver/users/formingworlds/&lt;username&gt;/miniconda3\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O\n    /dataserver/users/formingworlds/&lt;username&gt;/miniconda3/miniconda.sh\nbash /dataserver/users/formingworlds/&lt;username&gt;/miniconda3/miniconda.sh -b -u -p\n    /dataserver/users/formingworlds/&lt;username&gt;/miniconda3\nrm /dataserver/users/formingworlds/&lt;username&gt;/miniconda3/miniconda.sh\n```\nand similarly for conda-forge:\n```console\nmkdir -p /dataserver/users/formingworlds/${USER}/miniforge3\nwget \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\" -O\n    /dataserver/users/formingworlds/${USER}/miniforge3/miniforge.sh\nbash /dataserver/users/formingworlds/${USER}/miniforge3/miniforge.sh -b -p\n    /dataserver/users/formingworlds/${USER}/miniforge3\nrm /dataserver/users/formingworlds/${USER}/miniforge3/miniforge.sh\n```\nFor both Miniconda and conda-forge follow the instructions wrt updating your `~/.shellrc` file.\n\n### Pip cache consideration\nThe pip cache can easily take more than 3 GB when installing PROTEUS and this may exceed your\ndisk quota on your home directory. Therefore, you need to setup your pip cache folder in a different\nplace:\n```console\nmkdir /dataserver/users/formingworlds/${USER}/.pip-cache\nexport PIP_CACHE_DIR=/dataserver/users/formingworlds/${USER}/.pip-cache\n```\n</code></pre>"},{"location":"kapteyn_cluster_guide.html#queuing-manager-condormaster","title":"Queuing Manager: Condormaster","text":"<ul> <li> <p>To use the queuing manager on the Kapteyn cluster, you first need to SSH into Norma1 or Norma2.     <pre><code>ssh norma1\n</code></pre></p> </li> <li> <p>To access Condormaster, run the following command :     <pre><code>ssh condormaster\n</code></pre></p> </li> </ul>"},{"location":"kapteyn_cluster_guide.html#submitting-a-job-on-condormaster","title":"Submitting a Job on Condormaster","text":"<ul> <li> <p>To run a job using Condormaster, you first need to write a submit script. Begin by navigating to your home directory and creating a new submit script using :     <pre><code>nano name_of_your_script.submit\n</code></pre></p> </li> <li> <p>You can copy and paste the example submit script below (to start a single PROTEUS simulation) and modify it according to your needs.</p> </li> </ul> <pre><code>    getenv = True\n    universe = vanilla\n    executable = /dataserver/users/formingworlds/${USER}/miniconda3/bin/conda\n    arguments = run --name proteus --no-capture-output proteus start --config /dataserver/users/formingworlds/${USER}/PROTEUS/input/demos/escape.toml\n    log = condor_outputs/log/logfile.$(PROCESS)\n    output = condor_outputs/output/outfile.$(PROCESS)\n    error = condor_outputs/output/errfile.$(PROCESS)\n    notify_user = &lt;your-email&gt;@astro.rug.nl\n    Requirements = (Cluster == \"normas\")\n    queue 1\n</code></pre> <p>To exit nano, press <code>Ctrl+X</code>, then press <code>Enter</code> when prompted to save the file.</p>"},{"location":"kapteyn_cluster_guide.html#updating-the-submit-script","title":"Updating the Submit Script","text":"<p>Modify the following variables according to your needs :</p> <ul> <li><code>executable</code>: Specify the absolute path to the Python environment (pyenv or  conda) you use to run PROTEUS. If you want to run another (python) script, you can modify the <code>executable</code> line with the absolute path to your script :</li> </ul> <p><code>executable = /dataserver/users/formingworlds/lania/mscthesis/results/testscript.py</code></p> <ul> <li><code>arguments</code>: Update the path to the config file for your PROTEUS simulation. If using <code>tools/grid_proteus.py</code>, modify the entire command accordingly. If you want to run another (python) script, you can modify the <code>arguments</code> line with the absolute path to your input and output directory :</li> </ul> <p><code>arguments = -input [absolute path to input file] -outputdirectory [absolute path to output directory]</code></p> <ul> <li> <p><code>notify_user</code>: Enter your email address to receive job completion notifications.</p> </li> <li> <p><code>output</code> : The outfile will contain the outputs/print statements of your job.</p> </li> <li> <p><code>error</code> : The errfile file will contain the handled exceptions or runtime errors occuring while your job was running.</p> </li> </ul> <p>For further details, refer to the documentation on the Kapteyn intranet: How to use Condor? (Go to Computing &gt; Howto's &gt; linux &gt; How to use Condor?) This documentation is updated regularly, so be sure to check for the latest information. Also for more details about condor, the HTCondor documentation can be found here HT Condor manual.</p>"},{"location":"kapteyn_cluster_guide.html#submitting-and-monitoring-jobs","title":"Submitting and Monitoring Jobs","text":"<ul> <li> <p>To submit your script, run: <code>condor_submit name_of_your_script.submit</code> <pre><code>condor_submit name_of_your_script.submit\n</code></pre></p> </li> <li> <p>To check the status of your job, use:     <pre><code>condor_q\n</code></pre> or     <pre><code>condor_q -better-analyze\n</code></pre> The second command provides a more detailed job status analysis.</p> </li> <li> <p>Another useful command is     <pre><code>condor_status\n</code></pre> This displays the jobs currently running on Condormaster, including both your jobs and those of other users.</p> </li> </ul>"},{"location":"kapteyn_cluster_guide.html#exiting-condormaster","title":"Exiting Condormaster","text":"<ul> <li>To exit Condormaster and return to Norma1/Norma2, run:     <pre><code>exit\n</code></pre></li> </ul>"},{"location":"kapteyn_cluster_guide.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"kapteyn_cluster_guide.html#netcdf-error","title":"NetCDF Error","text":"<p>SOCRATES is using the NetCDF version installed by Python in your PROTEUS environment instead of the NetCDF version installed on the Kapteyn cluster system.</p> <p>To resolve this issue:</p> <ol> <li>Deactivate all conda environments.</li> <li>Go to the PROTEUS folder : <code>cd PROTEUS/</code></li> <li>Delete the <code>socrates/</code> directory using <code>rm -r socrates/</code></li> <li>Run the <code>./tools/get_socrates.sh</code> command to download SOCRATES again, ensuring this is done OUTSIDE of any conda environment.</li> <li>Execute the <code>cat socrates/set_rad_env</code> command to verify that SOCRATES is pointing to the correct NetCDF version (i.e. the NetCDF version installed on the Kapteyn cluster system).</li> <li>Finally, run a PROTEUS simulation using the <code>default.toml</code> configuration file to confirm it is working correctly.</li> </ol>"},{"location":"kapteyn_cluster_guide.html#error-reporting","title":"Error reporting","text":"<ul> <li>If you encounter an error that is not listed here, please create a new issue on the PROTEUS GitHub webpage (green button 'New issue' on the top right, choose 'Bug').</li> <li>Include details about what you were trying to do and how the error occurred. Providing a screenshot or copying/pasting the error message and log file can help others understand the issue better.</li> <li>Once the issue has been resolved, ensure that this troubleshooting section is updated to include the solution for future reference. You can check here how to edit the documentation.</li> </ul>"},{"location":"local_machine_guide.html","title":"Local Machine Guide","text":"<p>These steps should be performed before installing PROTEUS on your computer. They do not apply when running PROTEUS on a server or HPC cluster. For instructions on configuring PROTEUS on a remote machine, see the cluster guide pages.</p> <p>Once you have followed these steps, go back to the main installation guide page.</p>"},{"location":"local_machine_guide.html#macos","title":"macOS","text":"<ol> <li> <p>Open the terminal and install the Xcode developer tools:</p> <pre><code>xcode-select --install\n</code></pre> </li> <li> <p>Install the required libraries via the most appropriate method for you.</p> <p>Homebrew (recommended)</p> <pre><code>brew install netcdf netcdf-fortran wget gcc open-mpi\n</code></pre> <p>MacPorts</p> <pre><code>sudo port install netcdf-fortran +gcc8 wget gcc13 openmpi\n</code></pre> <p>Apple Silicon (M1/M2/M3/Ultra)</p> <p>The <code>gcc</code> and <code>open-mpi</code> packages are required for compiling PETSc on Apple Silicon Macs. If you skip these, you may encounter compilation errors during the PETSc installation step. See the Troubleshooting page for details.</p> </li> <li> <p>macOS Catalina (10.15) and later uses <code>zsh</code> as the default shell. Replace <code>.bashrc</code> with <code>.zshrc</code> throughout the installation instructions if you are using the default shell.</p> </li> </ol>"},{"location":"local_machine_guide.html#debian-ubuntu-linux","title":"Debian / Ubuntu Linux","text":"<p>Install <code>gfortran</code> and the NetCDF libraries via your package manager:</p> <pre><code>sudo apt install libnetcdff-dev gfortran\n</code></pre>"},{"location":"local_machine_guide.html#fedora-redhat-linux","title":"Fedora / RedHat Linux","text":"<p>Install the required libraries via your package manager:</p> <pre><code>sudo dnf install gcc gcc-gfortran gcc-c++ netcdf netcdf-fortran netcdf-fortran-devel \\\n    lapack lapack-devel lapack-static sundials-mpich openmpi openmpi-devel f2c f2c-libs\n</code></pre>"},{"location":"local_machine_guide.html#microsoft-windows","title":"Microsoft Windows","text":"<p>PROTEUS is not natively supported on Windows. To run PROTEUS on a Windows machine, use WSL2 (Windows Subsystem for Linux) and follow the Debian / Ubuntu Linux instructions above.</p> <pre><code>wsl --install -d Ubuntu\n</code></pre> <p>After installing WSL2 and launching an Ubuntu terminal, proceed with the main installation guide as on Linux.</p> <p>For remote development on a cluster instead, see the VS Code Instructions for Kapteyn Cluster.</p>"},{"location":"model.html","title":"PROTEUS Model Framework","text":""},{"location":"model.html#overview","title":"Overview","text":"<p>PROTEUS is a modular framework for simulating the time evolution of small (exo)planets. It is designed to be flexible, reflecting the broad diversity of planetary conditions already discovered, with the view of being updated to incorporate additional physics as the need arises. This approach stands in contrast to common monolithic models in the literature. PROTEUS is free and open-source, which permits external scrutiny of its workings. It is directly based upon the model of Lichtenberg et al. (2021) although the code has evolved substantially from that state.</p>"},{"location":"model.html#design-philosophy","title":"Design philosophy","text":"<p>George Box famously put that \"all models are wrong, but some are useful\". PROTEUS therefore leverages a modular and hierarchical modelling approach. Multiple independent models can fill the role of a given module, and each model can be used stand-alone. Hierarchical modelling allows inter-comparison of simple and complex models, taking advantage of the easy comprehension of the simple in order to diagnose and validate the qualitative behaviour of the complex. Simplified dummy modules are not designed for quantitatively meaningful calculations, but only to qualitatively capture end-member behaviours.</p>"},{"location":"model.html#system-architecture","title":"System architecture","text":"<p>Although PROTEUS aims to treat the problem of planetary evolution, it must necessarily also handle external processes which act upon the planet (e.g. tidal heating). The framework therefore models the combined system of a planet, its orbital mechanics, and the evolution of its host star. The planet itself is conceptually sub-divided into a vaporised atmosphere component above an interior component containing a silicate mantle and metallic core. PROTEUS facilitates communication between individual software modules which each implement a model for a specific part of the overall system. Conceptually, PROTEUS modules (e.g. the interior) are 'slots' which are filled by specific implementations: the 'models' (e.g. Aragog).</p> <p> Schematic of PROTEUS components and corresponding modules. </p>"},{"location":"model.html#module-overview","title":"Module overview","text":"Module Implementations Role Interior Aragog, SPIDER, dummy Mantle/core thermal evolution Atmosphere (climate) AGNI, JANUS, dummy Radiative-convective profile Atmosphere (chemistry) VULCAN, dummy Chemical kinetics Star MORS, dummy Stellar evolution Escape ZEPHYRUS, dummy Atmospheric escape Outgassing CALLIOPE, dummy Volatile exchange between interior and atmosphere Orbit Obliqua, dummy Orbital evolution and tidal heating"},{"location":"model.html#time-evolution-vs-equilibrium","title":"Time evolution vs equilibrium","text":"<p>Only the interior and star modules have an explicit notion of time-evolution. All other modules are applied at equilibrium, such that the quantities calculated by these modules are effectively updated instantaneously at each time-step. This assumes that the physical processes handled by these equilibrium modules reach steady-state on time-scales shorter than the physics considered by interior and stellar evolution modules.</p> <p>For further information on the model, see the Bibliography.</p>"},{"location":"snellius_cluster_guide.html","title":"Snellius Cluster Guide","text":""},{"location":"snellius_cluster_guide.html#installation","title":"Installation","text":"<ol> <li> <p>Connect via SSH. See instructions from SURF here.</p> </li> <li> <p>Set up your working environment. You have to load some modules and set up the environment variables prior to the installation. To facilitate this, we suggest that you copy the following lines into your <code>~/.bashrc</code> file.</p> <p><pre><code>module load 2024\nmodule load MPICH/4.2.2-GCC-13.3.0\nexport LD_LIBRARY_PATH=\"\"\nmodule load netCDF-Fortran/4.6.1-gompi-2024a\nexport RAD_DIR=\"$HOME/SOCRATES/\"\nexport FWL_DATA=\"$HOME/FWL_DATA/\"\n</code></pre> You must then logout.</p> </li> <li> <p>When you login again, run <code>module list</code> and confirm that you have loaded the following modules.</p> <pre><code>Currently Loaded Modules:\n1) 2024                           11) libpciaccess/0.18.1-GCCcore-13.3.0  21) Szip/2.1.1-GCCcore-13.3.0\n2) GCCcore/13.3.0                 12) hwloc/2.10.0-GCCcore-13.3.0         22) HDF5/1.14.5-gompi-2024a\n3) zlib/1.3.1-GCCcore-13.3.0      13) OpenSSL/3                           23) cURL/8.7.1-GCCcore-13.3.0\n4) binutils/2.42-GCCcore-13.3.0   14) libevent/2.1.12-GCCcore-13.3.0      24) gzip/1.13-GCCcore-13.3.0\n5) GCC/13.3.0                     15) libfabric/1.21.0-GCCcore-13.3.0     25) lz4/1.9.4-GCCcore-13.3.0\n6) numactl/2.0.18-GCCcore-13.3.0  16) PMIx/5.0.2-GCCcore-13.3.0           26) zstd/1.5.6-GCCcore-13.3.0\n7) UCX/1.16.0-GCCcore-13.3.0      17) PRRTE/3.0.5-GCCcore-13.3.0          27) bzip2/1.0.8-GCCcore-13.3.0\n8) MPICH/4.2.2-GCC-13.3.0         18) UCC/1.3.0-GCCcore-13.3.0            28) netCDF/4.9.2-gompi-2024a\n9) XZ/5.4.5-GCCcore-13.3.0        19) OpenMPI/5.0.3-GCC-13.3.0            29) netCDF-Fortran/4.6.1-gompi-2024a\n10) libxml2/2.12.7-GCCcore-13.3.0  20) gompi/2024a\n</code></pre> </li> <li> <p>You can now follow the usual installation steps here.</p> </li> </ol> <p>Note that when logging into <code>snellius.surf.nl</code>, you will not always connect to the same computer. For example, sometimes your hostname might be <code>int5</code> while other times it could be <code>int6</code>. This means that <code>tmux</code> sessions might suddenly appear to vanish, but in reality they are running on a different node. You can access a particular session (e.g. <code>int5</code>) by running <code>ssh int5</code> once logged into Snellius.</p>"},{"location":"test_building.html","title":"Building Robust Tests","text":""},{"location":"test_building.html#what-this-document-is-for","title":"What This Document Is For","text":"<p>New to testing? This guide helps you write tests for PROTEUS code. Tests are small programs that check if your code works correctly. When you change code, tests catch bugs before they reach production.</p> <p>Key concept: For most code changes, you only need unit tests (fast, isolated tests). Integration tests are for advanced scenarios involving multiple physics modules working together.</p> <p>For test markers and CI pipelines, see Test Categorization. For coverage analysis and troubleshooting, see Test Infrastructure.</p>"},{"location":"test_building.html#quick-start-writing-your-first-test","title":"Quick Start: Writing Your First Test","text":"<ol> <li>Create test file: For <code>src/proteus/utils/helper.py</code>, create <code>tests/utils/test_helper.py</code></li> <li>Add a test function: Start with <code>test_</code> prefix, add <code>@pytest.mark.unit</code> marker</li> <li>Run it: <code>pytest tests/utils/test_helper.py -v</code></li> <li>Check coverage: <code>pytest --cov=src tests/utils/</code></li> </ol> <pre><code>import pytest\nfrom proteus.utils.helper import my_function\n\n@pytest.mark.unit\ndef test_my_function_basic():\n    \"\"\"Test that my_function returns expected value.\"\"\"\n    result = my_function(input_value=10)\n    assert result == pytest.approx(expected_value, rel=1e-5)\n</code></pre>"},{"location":"test_building.html#developer-workflow","title":"Developer Workflow","text":"<ol> <li>Open source: The file under test (e.g., <code>src/proteus/utils/helper.py</code>)</li> <li>Open destination: The test file (e.g., <code>tests/utils/test_helper.py</code>)</li> <li>Open fixtures: <code>tests/conftest.py</code> for available fixtures</li> <li>Write tests: Use the prompts below if using AI assistance</li> <li>Run and verify: <code>pytest -m unit</code> then <code>ruff format tests/</code></li> </ol>"},{"location":"test_building.html#master-prompt-unit-tests","title":"Master Prompt (Unit Tests)","text":"<p>Copy into the chat when generating unit tests:</p> <p>Act as a Senior Scientific Software Engineer for PROTEUS.</p> <p>I need robust unit tests for the open file. Follow these strict guidelines:</p> <ol> <li>Architecture: Mirror the source. If testing <code>class Convection</code>, create <code>class TestConvection</code>. File: <code>tests/&lt;module&gt;/test_&lt;filename&gt;.py</code> for <code>src/proteus/&lt;module&gt;/&lt;filename&gt;.py</code>.</li> <li>Mocking: This is a unit test. Aggressively mock heavy physics (SOCRATES, AGNI) and I/O with <code>unittest.mock</code>. Tests must run in &lt;100 ms.</li> <li>Precision: Use <code>pytest.approx(expected, rel=1e-5)</code> for all float comparisons. Never use <code>==</code> for floats.</li> <li>Physics: Use physically valid inputs (e.g. T &gt; 0 K, P &gt; 0) unless testing error handling.</li> <li>Coverage: Aim for high coverage; include edge cases (None, empty arrays, invalid values where relevant).</li> <li>Style: Use <code>@pytest.mark.parametrize</code> for data-driven tests. Add a brief docstring per test describing the scenario. Use <code>@pytest.mark.unit</code>.</li> <li>Format: Run <code>ruff format</code> on test files before committing.</li> </ol> <p>Generate the tests now.</p>"},{"location":"test_building.html#integration-prompt-standard-configuration","title":"Integration Prompt (Standard Configuration)","text":"<p>Use when adding or extending integration tests (e.g. ARAGOG+AGNI+CALLIOPE+ZEPHYRUS+MORS):</p> <p>Act as a Senior Scientific Software Engineer for PROTEUS.</p> <p>I need an integration test for the Standard Configuration (e.g. test_std_config.py or multi-module coupling).</p> <ol> <li>Scope: Test coupling of the relevant modules (ARAGOG, AGNI, CALLIOPE, ZEPHYRUS, MORS as needed).</li> <li>Mocking: Do not mock internal physics between these modules. Mock only external I/O (e.g. network downloads) with <code>unittest.mock</code>.</li> <li>Config: Use fixtures from <code>tests/conftest.py</code> and <code>tests/integration/conftest.py</code> (e.g. <code>intermediate_params</code>, config paths).</li> <li>Verification: Stable evolution (multiple timesteps, no crash/NaN); energy and mass conservation with stated tolerances; feedback checks (e.g. T_surf \u2194 outgassing \u2194 atmos_mass).</li> <li>Marker: Use <code>@pytest.mark.integration</code> (and <code>@pytest.mark.slow</code> if long-running).</li> </ol> <p>Generate the integration test skeleton.</p>"},{"location":"test_building.html#see-also","title":"See Also","text":"<ul> <li>Test Categorization \u2014 Markers, CI pipeline, fixtures</li> <li>Test Infrastructure \u2014 Layout, coverage, reusable quality gate</li> <li>Docker CI Architecture \u2014 Docker image, CI pipelines</li> <li>.github/copilot-instructions.md \u2014 Test commands and coverage thresholds</li> </ul>"},{"location":"test_categorization.html","title":"Test Categorization and CI/CD","text":""},{"location":"test_categorization.html#what-this-document-is-for","title":"What This Document Is For","text":"<p>New to PROTEUS testing? This document explains how we organize tests into categories and how CI (Continuous Integration) automatically runs them when you submit code.</p> <p>Key concept: Tests are labeled with markers that tell pytest what kind of test they are. Different markers run at different times\u2014fast tests run on every pull request, slow tests run overnight.</p> <p>For writing tests, see Test Building. For coverage analysis and troubleshooting, see Test Infrastructure.</p>"},{"location":"test_categorization.html#test-categories-markers","title":"Test Categories (Markers)","text":"<p>Add one of these markers above each test function:</p> Marker What It Tests Speed When It Runs <code>@pytest.mark.unit</code> Python logic with mocked physics &lt;100 ms Every PR <code>@pytest.mark.smoke</code> Real binaries, 1 timestep &lt;30 s Every PR <code>@pytest.mark.integration</code> Multiple modules working together Minutes Nightly only <code>@pytest.mark.slow</code> Full physics simulations Hours Nightly only <code>@pytest.mark.skip</code> Temporarily disabled \u2014 Never"},{"location":"test_categorization.html#which-marker-should-i-use","title":"Which marker should I use?","text":"<ul> <li>Most tests \u2192 <code>unit</code>: Testing a single function? Mock external dependencies, use <code>unit</code>.</li> <li>Testing real binaries \u2192 <code>smoke</code>: Need SOCRATES/AGNI/SPIDER actually running? Use <code>smoke</code>. Module-level smoke tests (e.g. in <code>tests/atmos_clim/</code>) validate a single binary with 1 timestep. Integration-level smoke tests (in <code>tests/integration/</code>) validate the coupling framework end-to-end with dummy modules.</li> <li>Testing module coupling \u2192 <code>integration</code>: ARAGOG + AGNI working together? Use <code>integration</code>.</li> <li>Full science runs \u2192 <code>slow</code>: Multi-hour simulations? Use <code>slow</code>.</li> </ul>"},{"location":"test_categorization.html#cicd-pipeline","title":"CI/CD Pipeline","text":""},{"location":"test_categorization.html#what-happens-when-you-open-a-pr","title":"What Happens When You Open a PR","text":"<ol> <li>Structure check: Validates <code>tests/</code> mirrors <code>src/proteus/</code></li> <li>Unit tests (Linux): Runs <code>pytest -m \"unit and not skip\"</code> with coverage</li> <li>Diff-cover: Checks 80% coverage on your changed lines</li> <li>Smoke tests (Linux): Runs <code>pytest -m \"smoke and not skip\"</code></li> <li>Unit tests (macOS): Runs unit tests on macOS (no compiled binaries)</li> <li>Lint: Checks code style with ruff</li> <li>Summary: Aggregates results from all platforms into a unified report</li> </ol> <p>Runtime: ~5-10 minutes</p>"},{"location":"test_categorization.html#what-happens-nightly","title":"What Happens Nightly","text":"<p>The nightly workflow (<code>ci-nightly.yml</code>) is primarily triggered by <code>docker-build.yml</code> after the 2am UTC image rebuild. A 3am UTC cron acts as a fallback if the docker build didn't run. A deduplication check prevents running twice.</p> <ul> <li>Runs ALL tests (unit \u2192 smoke \u2192 integration \u2192 slow)</li> <li>Updates coverage thresholds (ratcheting)</li> <li>Uploads aggregate coverage (unit + smoke + integration) to Codecov</li> <li>Sets <code>PROTEUS_CI_NIGHTLY=1</code> to enable additional smoke tests</li> </ul>"},{"location":"test_categorization.html#coverage-rules","title":"Coverage Rules","text":"Rule Value What It Means Grace period 0.3% Small coverage drops allowed (warning posted) Diff-cover 80% Your changed lines need 80% coverage Staleness 48h PR fails if nightly data is too old"},{"location":"test_categorization.html#test-layout","title":"Test Layout","text":"<p>Tests mirror <code>src/proteus/</code>. Validation: <code>bash tools/validate_test_structure.sh</code>. Special dirs <code>data</code>, <code>helpers</code>, <code>integration</code> are handled in validation.</p> <pre><code>tests/\n\u251c\u2500\u2500 integration/     # test_smoke_*.py, test_integration_*.py, test_aragog_*, test_std_config, etc.\n\u251c\u2500\u2500 config/, utils/, plot/, star/, orbit/, interior/, escape/, outgas/, observe/, atmos_clim/, atmos_chem/\n\u251c\u2500\u2500 grid/, inference/, data/\n\u251c\u2500\u2500 test_cli.py, test_init.py\n\u2514\u2500\u2500 conftest.py      # Shared fixtures (see Test Infrastructure)\n</code></pre>"},{"location":"test_categorization.html#fixtures-testsconftestpy","title":"Fixtures (<code>tests/conftest.py</code>)","text":"<ul> <li>Parameter classes: <code>EarthLikeParams</code>, <code>UltraHotSuperEarthParams</code>, <code>IntermediateSuperEarthParams</code> (session-scoped).</li> <li>Config paths: <code>config_earth</code>, <code>config_minimal</code>, <code>config_dummy</code>, <code>proteus_root</code>.</li> <li>Fixtures: <code>earth_params</code>, <code>ultra_hot_params</code>, <code>intermediate_params</code> (instances of the above).</li> </ul> <p>Integration-specific fixtures (e.g. multi-timestep runs, conservation checks) are in <code>tests/integration/conftest.py</code>. See Test Infrastructure for details.</p>"},{"location":"test_categorization.html#running-tests-locally","title":"Running Tests Locally","text":"<pre><code>pytest -m \"unit and not skip\"           # Unit only (matches PR)\npytest -m \"smoke and not skip\"         # Smoke only\npytest -m \"(unit or smoke) and not skip\"  # What PR runs\npytest -m integration                   # Integration\npytest -m slow                         # Slow\npytest -m \"not slow\"                   # All except slow\npytest --cov=src --cov-report=html     # With coverage\n</code></pre> <p>For fast gate check: <code>pytest -m \"unit and not skip\" --cov=src --cov-fail-under=&lt;value&gt;</code> (value from <code>pyproject.toml</code>).</p>"},{"location":"test_categorization.html#adding-new-tests","title":"Adding New Tests","text":"<ol> <li>Choose marker: <code>unit</code> / <code>smoke</code> / <code>integration</code> / <code>slow</code>.</li> <li>Create <code>tests/&lt;module&gt;/test_&lt;filename&gt;.py</code> if needed (mirror source).</li> <li>Use <code>@pytest.mark.&lt;marker&gt;</code> and docstrings; use <code>pytest.approx</code> for floats.</li> <li>Run <code>bash tools/validate_test_structure.sh</code>; run the relevant marker group; ensure coverage meets the fast gate for unit changes.</li> </ol>"},{"location":"test_categorization.html#coverage-requirements","title":"Coverage Requirements","text":""},{"location":"test_categorization.html#coverage-gates","title":"Coverage Gates","text":"Gate Tests Included When Checked Threshold Source Fast gate unit + smoke Every PR <code>[tool.proteus.coverage_fast] fail_under</code> Estimated total union of PR (unit+smoke) + nightly Every PR <code>[tool.coverage.report] fail_under</code> Full gate unit + smoke + integration + slow Nightly <code>[tool.coverage.report] fail_under</code> Diff-cover changed lines only Every PR Hard-coded 80%"},{"location":"test_categorization.html#what-each-test-tier-contributes","title":"What Each Test Tier Contributes","text":"<ul> <li><code>unit</code> \u2014 Bulk of Python logic coverage (functions, branches, error paths). Fastest feedback loop.</li> <li><code>smoke</code> \u2014 Covers binary wrapper code and real I/O paths that unit tests mock away.</li> <li><code>integration</code> \u2014 Covers cross-module coupling paths (e.g., ARAGOG + JANUS handoff).</li> <li><code>slow</code> \u2014 Full scientific validation. Contributes to coverage but primarily validates physics, not code paths.</li> </ul> <p>All thresholds auto-increase (\"ratchet\") and never decrease. Check coverage locally with <code>pytest --cov=src --cov-report=html</code>.</p> <p>For details on how coverage is collected across workflows and how the estimated total is computed, see Coverage Collection &amp; Reporting.</p>"},{"location":"test_categorization.html#references","title":"References","text":"<ul> <li>Test Infrastructure \u2014 Coverage workflows, reusable quality gate, troubleshooting</li> <li>Test Building \u2014 Prompts for unit/integration tests</li> <li>Docker CI Architecture \u2014 Docker image, CI pipelines</li> <li>.github/copilot-instructions.md \u2014 Commands and thresholds</li> </ul>"},{"location":"test_infrastructure.html","title":"Testing Infrastructure","text":""},{"location":"test_infrastructure.html#what-this-document-is-for","title":"What This Document Is For","text":"<p>New to PROTEUS? This document explains how our testing system works: how to run tests, check code coverage, and troubleshoot common issues.</p> <p>Key concepts:</p> <ul> <li>Coverage measures what percentage of your code is tested. Higher is better.</li> <li>CI (Continuous Integration) automatically runs tests when you push code.</li> <li>Thresholds are minimum coverage percentages that must be met.</li> </ul> <p>For test markers and CI pipelines, see Test Categorization. For writing tests, see Test Building.</p>"},{"location":"test_infrastructure.html#table-of-contents","title":"Table of Contents","text":"<ol> <li>Quick Start</li> <li>CI/CD Status</li> <li>Developer Workflow</li> <li>Best Practices</li> <li>Coverage Analysis</li> <li>Coverage Collection &amp; Reporting</li> <li>Pre-commit Checklist</li> <li>Troubleshooting</li> <li>Reusable Quality Gate</li> <li>References</li> </ol>"},{"location":"test_infrastructure.html#quick-start","title":"Quick Start","text":"<p>First time? Install with <code>pip install -e \".[develop]\"</code>, then run tests:</p> <pre><code>pytest -m \"unit and not skip\"           # Fast unit tests (~2 min)\npytest -m \"smoke and not skip\"          # Smoke tests with real binaries\npytest --cov=src --cov-report=html      # Generate coverage report\nopen htmlcov/index.html                 # View coverage in browser\n</code></pre> <p>Before committing: 1. Run <code>pytest -m \"unit and not skip\"</code> \u2014 must pass 2. Run <code>ruff check src/ tests/ &amp;&amp; ruff format src/ tests/</code> \u2014 must pass 3. Run <code>bash tools/validate_test_structure.sh</code> \u2014 must pass</p>"},{"location":"test_infrastructure.html#cicd-status","title":"CI/CD Status","text":""},{"location":"test_infrastructure.html#how-ci-works","title":"How CI Works","text":"<p>When you open a pull request, CI automatically:</p> <ol> <li>Validates test file structure</li> <li>Runs unit tests and checks coverage (must meet threshold)</li> <li>Runs smoke tests</li> <li>Checks code style with ruff</li> </ol> <p>Current coverage thresholds (from <code>pyproject.toml</code>):</p> <ul> <li>Fast gate: unit + smoke, checked on PRs</li> <li>Full gate: all tests, checked nightly</li> </ul>"},{"location":"test_infrastructure.html#workflows","title":"Workflows","text":"Workflow Runs When What It Does <code>ci-pr-checks.yml</code> Every PR Unit + smoke tests (Linux), unit tests (macOS), lint, ~5-10 min <code>docker-build.yml</code> Daily 2am UTC / dependency changes Rebuilds Docker image, then triggers nightly <code>ci-nightly.yml</code> Triggered by docker-build (fallback: 3am cron) All tests including slow, updates thresholds, uploads coverage to Codecov <p>Key features:</p> <ul> <li>Grace period: PRs can merge with \u22640.3% coverage drop (warning posted)</li> <li>Diff-cover: 80% coverage required on changed lines</li> <li>Auto-ratcheting: Thresholds only increase, never decrease</li> </ul>"},{"location":"test_infrastructure.html#developer-workflow","title":"Developer Workflow","text":"<ol> <li>Write or modify code</li> <li>Write or update tests (<code>tests/&lt;module&gt;/test_&lt;filename&gt;.py</code> mirrors <code>src/proteus/&lt;module&gt;/&lt;filename&gt;.py</code>)</li> <li>Run tests: <code>pytest tests/&lt;module&gt;/</code> or <code>pytest -m unit</code></li> <li>Check coverage: <code>pytest --cov=src</code> (CI uses <code>--cov=src</code> for unit)</li> <li>Validate structure: <code>bash tools/validate_test_structure.sh</code></li> <li>Lint: <code>ruff check src/ tests/</code> and <code>ruff format src/ tests/</code></li> <li>Commit and push; CI runs automatically</li> </ol> <p>Adding new code: Create <code>src/proteus/&lt;module&gt;/&lt;file&gt;.py</code> and <code>tests/&lt;module&gt;/test_&lt;filename&gt;.py</code>. Use fixtures from <code>tests/conftest.py</code> and <code>tests/integration/conftest.py</code>. See Test Building for prompts and Test Categorization for fixtures list.</p> <p>Basic test structure: Use <code>@pytest.mark.unit</code> (or other marker), docstrings, <code>pytest.approx()</code> for floats, and <code>unittest.mock</code> for external/heavy code in unit tests. Example:</p> <pre><code>@pytest.mark.unit\ndef test_function_basic():\n    \"\"\"Test basic functionality.\"\"\"\n    result = function_to_test(input_value)\n    assert result == pytest.approx(expected, rel=1e-5)\n</code></pre>"},{"location":"test_infrastructure.html#best-practices","title":"Best Practices","text":"Practice Guideline Structure Mirror <code>src/proteus/</code> in <code>tests/</code>; one test file per source file Markers Use <code>unit</code> / <code>smoke</code> / <code>integration</code> / <code>slow</code> consistently Floats Use <code>pytest.approx(val, rel=1e-5)</code> or <code>np.testing.assert_allclose</code>; never <code>==</code> Mocking Unit tests mock I/O and heavy physics; integration tests use real modules Docstrings Explain physical scenario being tested Determinism Set seeds (<code>np.random.seed(42)</code>); use <code>tmp_path</code> for temp files Coverage Focus on critical paths; use <code>bash tools/coverage_analysis.sh</code> <p>See Test Categorization for marker details and Test Building for prompts.</p>"},{"location":"test_infrastructure.html#coverage-analysis-workflow","title":"Coverage Analysis Workflow","text":"<p>Generate reports:</p> <pre><code>pytest --cov=src --cov-report=term-missing   # Terminal report with missing lines\npytest --cov=src --cov-report=html           # HTML report\nopen htmlcov/index.html                      # View in browser\nbash tools/coverage_analysis.sh              # Coverage by module\ncoverage report --show-missing --skip-covered  # Only uncovered files\n</code></pre> <p>Thresholds (in <code>pyproject.toml</code>): - <code>[tool.proteus.coverage_fast] fail_under</code> \u2014 Fast gate (PRs) - <code>[tool.coverage.report] fail_under</code> \u2014 Full gate (nightly)</p> <p>Thresholds auto-ratchet upward; never decrease manually.</p>"},{"location":"test_infrastructure.html#coverage-collection-reporting","title":"Coverage Collection &amp; Reporting","text":""},{"location":"test_infrastructure.html#two-workflow-architecture","title":"Two-Workflow Architecture","text":"<p>Coverage data flows through two CI workflows that serve different purposes:</p> Workflow Tests Run Codecov Flag Artifact Produced <code>ci-pr-checks.yml</code> unit + smoke <code>unit-tests</code> <code>coverage-unit.json</code> (this PR only) <code>ci-nightly.yml</code> unit + smoke + integration (+ slow) <code>nightly</code> <code>coverage-integration-only.json</code> (combined) <p>The PR workflow runs fast tests on every pull request. The nightly workflow runs the full suite and saves its combined coverage as an artifact that subsequent PR runs download.</p>"},{"location":"test_infrastructure.html#estimated-total-coverage-union-of-lines","title":"Estimated Total Coverage (Union-of-Lines)","text":"<p>On each PR, the workflow estimates what total coverage would be if integration tests were also run, without actually running them. It does this by combining the PR's own coverage with the latest nightly artifact:</p> <pre><code> ci-pr-checks.yml                        ci-nightly.yml (last successful)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502  Run unit+smoke  \u2502                     \u2502  coverage-        \u2502\n \u2502  on PR code      \u2502                     \u2502  integration-     \u2502\n \u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2502                     \u2502  only.json        \u2502\n \u2502  coverage-       \u2502                     \u2502  (unit+smoke+     \u2502\n \u2502  unit.json       \u2502                     \u2502   integration)    \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                                        \u2502\n          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502  Union-of-Lines \u2502\n                \u2502  Algorithm      \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502  Estimated      \u2502\n                \u2502  Total Coverage \u2502\n                \u2502  (compared to   \u2502\n                \u2502  full threshold)\u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Algorithm (4 steps):</p> <ol> <li>Parse both JSON files (<code>coverage-unit.json</code> from this PR, <code>coverage-integration-only.json</code> from nightly)</li> <li>Normalize file paths (strip container prefixes like <code>/opt/proteus/</code>) so lines match across environments</li> <li>Compute union of covered lines and union of executable lines across both datasets</li> <li>Compare <code>100 * len(covered_union) / len(executable_union)</code> against the full threshold from <code>pyproject.toml</code></li> </ol> <p>Stale nightly lines</p> <p>The nightly artifact (<code>coverage-integration-only.json</code>) contains combined unit + smoke + integration coverage, not just integration. If unit/smoke coverage drops on a PR, stale lines from the nightly artifact can mask the regression until the next nightly run updates the baseline. A 48-hour staleness check mitigates this but does not eliminate it.</p>"},{"location":"test_infrastructure.html#codecov-integration","title":"Codecov Integration","text":"<p>Two upload flags partition coverage data on codecov.io:</p> Flag Uploaded By Contains <code>unit-tests</code> <code>ci-pr-checks.yml</code> Unit + smoke coverage from this PR <code>nightly</code> <code>ci-nightly.yml</code> Unit + smoke + integration coverage from nightly <p>Configuration in <code>codecov.yml</code>:</p> <ul> <li>Project target: <code>auto</code> \u2014 Codecov tracks the project coverage trend automatically</li> <li>Patch target: <code>80%</code> \u2014 new/changed lines must have 80% coverage</li> <li><code>carryforward: true</code> on both flags \u2014 if a flag isn't uploaded in a commit (e.g., nightly doesn't run on PRs), Codecov carries forward the last known value instead of reporting a drop</li> </ul>"},{"location":"test_infrastructure.html#readme-coverage-badge","title":"README Coverage Badge","text":"<p>The Codecov badge in <code>README.md</code> must include <code>/branch/main/</code> in its URL to display the correct coverage value. Without this path segment, Codecov returns \"unknown\".</p> <p>Correct format: <code>https://codecov.io/gh/FormingWorlds/PROTEUS/branch/main/graph/badge.svg</code></p>"},{"location":"test_infrastructure.html#badge-validation-tests","title":"Badge Validation Tests","text":"<p>The file <code>tests/test_readme_badges.py</code> contains unit tests that guard against badge URL regressions:</p> <ul> <li>All badge image URLs use HTTPS and point to allowed domains</li> <li>All expected badges (Unit Tests, Integration Tests, docs, License, Codecov, DOI) are present</li> <li>The Codecov badge URL includes <code>/branch/main/</code></li> <li>Workflow files referenced by badge URLs exist on disk</li> </ul> <p>These tests run on every PR as part of the unit test suite.</p>"},{"location":"test_infrastructure.html#pre-commit-checklist","title":"Pre-commit Checklist","text":"<p>Run these before every commit:</p> <pre><code># 1. Tests pass\npytest -m \"unit and not skip\"\n\n# 2. Lint passes\nruff check src/ tests/ &amp;&amp; ruff format src/ tests/\n\n# 3. Structure valid\nbash tools/validate_test_structure.sh\n</code></pre> <p>For new code:</p> <ul> <li>[ ] Added tests with correct markers (see Test Categorization)</li> <li>[ ] Coverage meets fast gate threshold</li> <li>[ ] Docstrings explain physical scenarios</li> </ul>"},{"location":"test_infrastructure.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"test_infrastructure.html#common-issues","title":"Common Issues","text":"Issue Cause Solution <code>pytest: unrecognized arguments: --cov</code> pytest-cov not installed <code>pip install -e \".[develop]\"</code> Coverage below threshold Coverage dropped Add tests; see <code>coverage report --show-missing</code>; do not lower threshold Tests not found Discovery / naming <code>pytest --collect-only</code>; ensure <code>test_*.py</code> and <code>test_*</code> functions Import errors Package not installed <code>pip install -e \".[develop]\"</code>; check <code>src/</code> layout CI fails, local passes Environment / deps Match Python version and pyproject.toml; check CI logs Ruff fails Style violations <code>ruff check --fix src/ tests/</code> and <code>ruff format src/ tests/</code>"},{"location":"test_infrastructure.html#debugging-tests","title":"Debugging Tests","text":"<pre><code>pytest -v --showlocals -x tests/module/test_file.py::test_function\npytest --pdb   # Debugger on failure\n</code></pre>"},{"location":"test_infrastructure.html#stale-nightly-baseline","title":"Stale Nightly Baseline","text":"<p>PR checks compare coverage against the last successful nightly run. If the nightly workflow fails (e.g. data download timeout, CI infrastructure issues, or transient test failures), the baseline becomes stale (&gt;48 hours old) and PRs will fail validation. To fix this, trigger the nightly workflow manually and wait for it to complete.</p>"},{"location":"test_infrastructure.html#docker-ci","title":"Docker CI","text":"<ul> <li>Build fails: <code>docker build -t proteus-test .</code> locally; check Dockerfile and deps.</li> <li>Image pull fails: Verify <code>ghcr.io/formingworlds/proteus:latest</code> is public.</li> <li>Tests fail in container: <code>docker run -it ghcr.io/formingworlds/proteus:latest bash</code> and run <code>pytest -m unit -v</code> inside.</li> </ul>"},{"location":"test_infrastructure.html#reusable-quality-gate-for-ecosystem-modules","title":"Reusable Quality Gate for Ecosystem Modules","text":"<p>PROTEUS provides a reusable workflow for ecosystem modules (CALLIOPE, JANUS, MORS, etc.) to adopt consistent testing standards.</p> <p>Location: <code>.github/workflows/proteus_test_quality_gate.yml</code></p> <p>Purpose: Standardized test quality gate that ecosystem modules can call from their own CI workflows.</p> <p>Inputs:</p> Input Default Description <code>python-version</code> <code>3.12</code> Python version for testing <code>coverage-threshold</code> <code>30</code> Minimum coverage percentage <code>grace-period</code> <code>0.3</code> Allowed coverage drop (percentage points) <code>working-directory</code> <code>.</code> Project subdirectory <code>pytest-args</code> <code>''</code> Additional pytest arguments <p>Why use it:</p> <ul> <li>Consistency: Same testing standards across all ecosystem modules</li> <li>Maintenance: Updates to quality gate propagate to all modules</li> <li>Best practices: Includes coverage reporting, artifact upload, proper caching</li> </ul>"},{"location":"test_infrastructure.html#implementation-guide-for-ecosystem-modules","title":"Implementation Guide for Ecosystem Modules","text":"<p>Step 1: Ensure your module has the required structure</p> <pre><code>your-module/\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 your_module/\n\u2502       \u2514\u2500\u2500 __init__.py\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 test_*.py\n\u251c\u2500\u2500 pyproject.toml          # Must have [project.optional-dependencies] develop = [...]\n\u2514\u2500\u2500 .github/\n    \u2514\u2500\u2500 workflows/\n        \u2514\u2500\u2500 tests.yml       # Create this file\n</code></pre> <p>Step 2: Add <code>[develop]</code> dependencies to <code>pyproject.toml</code></p> <pre><code>[project.optional-dependencies]\ndevelop = [\n    \"pytest&gt;=7.0\",\n    \"pytest-cov&gt;=4.0\",\n    \"ruff&gt;=0.1.0\",\n]\n</code></pre> <p>Step 3: Create <code>.github/workflows/tests.yml</code></p> <p>Basic example: <pre><code>name: Tests\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\njobs:\n  test:\n    uses: FormingWorlds/PROTEUS/.github/workflows/proteus_test_quality_gate.yml@main\n    with:\n      coverage-threshold: 30\n</code></pre></p> <p>Step 4: Configure for your module's needs</p> <p>Example configurations for different scenarios:</p> <pre><code># CALLIOPE - Higher coverage, specific markers\njobs:\n  test:\n    uses: FormingWorlds/PROTEUS/.github/workflows/proteus_test_quality_gate.yml@main\n    with:\n      coverage-threshold: 50\n      pytest-args: '-m \"unit and not skip\" -v'\n\n# JANUS - Lower initial threshold, exclude slow tests\njobs:\n  test:\n    uses: FormingWorlds/PROTEUS/.github/workflows/proteus_test_quality_gate.yml@main\n    with:\n      coverage-threshold: 25\n      pytest-args: '-m \"not slow\"'\n\n# MORS - Monorepo with subdirectory\njobs:\n  test:\n    uses: FormingWorlds/PROTEUS/.github/workflows/proteus_test_quality_gate.yml@main\n    with:\n      working-directory: 'python'\n      coverage-threshold: 40\n</code></pre>"},{"location":"test_infrastructure.html#codecov-integration_1","title":"Codecov Integration","text":"<p>To enable Codecov uploads, add the <code>CODECOV_TOKEN</code> secret to your repository:</p> <ol> <li>Go to codecov.io and connect your repository</li> <li>Copy the upload token</li> <li>In GitHub: Settings \u2192 Secrets \u2192 Actions \u2192 New repository secret</li> <li>Name: <code>CODECOV_TOKEN</code>, Value: your token</li> </ol> <p>The workflow automatically uploads coverage if the secret exists.</p>"},{"location":"test_infrastructure.html#transitioning-from-custom-workflows","title":"Transitioning from Custom Workflows","text":"<p>If your module has an existing test workflow:</p> <ol> <li>Keep your workflow temporarily: Rename to <code>tests-old.yml</code></li> <li>Create new workflow: Add <code>tests.yml</code> using the quality gate</li> <li>Compare results: Ensure both pass on a few PRs</li> <li>Remove old workflow: Delete <code>tests-old.yml</code> once confident</li> </ol>"},{"location":"test_infrastructure.html#troubleshooting_1","title":"Troubleshooting","text":"Issue Solution <code>pip install</code> fails Ensure <code>pyproject.toml</code> has valid <code>[project.optional-dependencies]</code> Coverage too low Lower <code>coverage-threshold</code> initially, ratchet up over time Tests not found Check <code>tests/</code> directory exists and contains <code>test_*.py</code> files Codecov upload fails Add <code>CODECOV_TOKEN</code> secret or ignore (non-fatal)"},{"location":"test_infrastructure.html#references","title":"References","text":""},{"location":"test_infrastructure.html#proteus-documentation","title":"PROTEUS Documentation","text":"<ul> <li>Test Categorization \u2014 Markers, CI pipeline, fixtures</li> <li>Test Building \u2014 Prompts for unit/integration tests</li> <li>Docker CI Architecture \u2014 Docker image, CI pipelines</li> <li>AI-Assisted Development \u2014 Using AI for tests and code review</li> <li>tests/conftest.py \u2014 Shared fixtures</li> <li>.github/copilot-instructions.md \u2014 Commands and thresholds</li> </ul>"},{"location":"test_infrastructure.html#external-resources","title":"External Resources","text":"<ul> <li>pytest Documentation</li> <li>coverage.py Documentation</li> <li>ruff Documentation</li> </ul>"},{"location":"troubleshooting.html","title":"Troubleshooting","text":"<p>This section includes troubleshooting advice for common errors, organized by platform. If you encounter errors that you cannot solve via the standard step-by-step guide or the advice below, contact the developers.</p>"},{"location":"troubleshooting.html#quick-error-index","title":"Quick error index","text":"Error / symptom Section <code>Permission denied (publickey)</code> SSH keys <code>Out-of-date modules detected</code> Module updates Slow Zenodo downloads Data downloads <code>libudev.so.1</code> not found libudev <code>OpenSSL_jll</code> / Julia error Julia compatibility PETSc fails on Apple Silicon PETSc Apple Silicon <code>x86-pad-for-align</code> / SOCRATES SOCRATES compilation <code>libcrypto.3.dylib</code> not found netCDF error PETSc error moving libraries Fedora/RHEL PETSc"},{"location":"troubleshooting.html#general-all-platforms","title":"General (all platforms)","text":""},{"location":"troubleshooting.html#cannot-clone-module-or-permission-denied-publickey","title":"Cannot clone module, or Permission denied (publickey)","text":"<p>Have you added your SSH key to GitHub? See these pages for guidance:</p> <ul> <li>Checking for existing SSH keys</li> <li>Generating a new SSH key</li> <li>Adding a new SSH key to your Github account</li> <li>Testing your SSH connection</li> </ul>"},{"location":"troubleshooting.html#out-of-date-modules-detected","title":"Out-of-date modules detected","text":"<p>You will see an error that looks like this:</p> <pre><code>[ INFO  ] Validating module versions\n[ ERROR ] (MODULENAME) module is out of date: (INSTALLED VER) &lt; (REQUIRED VER)\nUncaught exception\n    ...\nOSError: Out-of-date modules detected. Refer to the Troubleshooting guide on the wiki\n</code></pre> <p>This means one of the required PROTEUS modules is outdated on your machine. You need to update the named module before running PROTEUS again.</p> <p>You can check which modules are out of date by running:</p> <pre><code>proteus doctor\n</code></pre> <p>If you have not manually installed this module, go to the PROTEUS folder and run:</p> <pre><code>python -m pip install -e .\n</code></pre> <p>If you have a developer installation of the module, go to the module's own folder and run:</p> <pre><code>git checkout main\ngit pull\npython -m pip install -U -e .\n</code></pre>"},{"location":"troubleshooting.html#data-download-errors-or-slow-zenodo-downloads","title":"Data download errors or slow Zenodo downloads","text":"<p>PROTEUS automatically downloads input data files from Zenodo. The code works without an API token using public access, but you can optionally configure a token for enhanced rate limits.</p> Optional: Configure a Zenodo API token <p>If you encounter rate limiting errors or slow downloads, you can set up a Zenodo API token:</p> <ol> <li>Get a personal access token from Zenodo</li> <li>Configure it:     <pre><code>pystow set zenodo api_token &lt;your-token&gt;\n</code></pre></li> <li>Verify it's configured:     <pre><code>pystow get zenodo api_token\n</code></pre></li> </ol> <p>The token is optional. PROTEUS uses public access by default and falls back gracefully if the token is unavailable.</p>"},{"location":"troubleshooting.html#petsc-complains-about-being-in-the-wrong-directory","title":"PETSc complains about being in the wrong directory","text":"<p>First, check that you are in the correct directory when running <code>make</code> or <code>./configure</code>. If you are, this could be caused by the environment variable <code>PETSC_DIR</code> remaining set after a previous PETSc installation. Run <code>unset PETSC_DIR</code> and try again.</p>"},{"location":"troubleshooting.html#libudevso1-not-found","title":"<code>libudev.so.1</code> not found","text":"<p>This happens when compiling SPIDER within a Python environment that is incompatible with PETSc. Try compiling SPIDER (and/or PETSc) in your system's base Python environment, and ensure that you are not using Python &gt;= 3.13, as SPIDER is only supported for Python versions &lt;= 3.12 currently.</p>"},{"location":"troubleshooting.html#julia-compatibility-error","title":"Julia compatibility error","text":"<p>There are incompatibilities between Python and some versions of Julia. Julia version 1.12+ is not yet supported because it requires a version of the OpenSSL library that is incompatible with Python.</p> <p>You must use Python 3.12 and Julia 1.11 to avoid these problems:</p> <pre><code>juliaup add 1.11\njuliaup default 1.11\n</code></pre>"},{"location":"troubleshooting.html#macos","title":"macOS","text":""},{"location":"troubleshooting.html#petsc-compilation-fails-on-apple-silicon","title":"PETSc compilation fails on Apple Silicon","text":"<p>Applies to: macOS on Apple Silicon (M1, M2, M3, Ultra)</p> <p>If <code>get_petsc.sh</code> fails during configuration or compilation on Apple Silicon Macs, this is because the default PETSc configuration can conflict with the Apple Silicon toolchain.</p> <p>Prerequisites</p> <p>Make sure you have installed GCC and OpenMPI via Homebrew:</p> <pre><code>brew install gcc open-mpi\n</code></pre> <p>Automated fix</p> <p>Run the provided fix script from the PROTEUS root directory:</p> <pre><code>./tools/fix_petsc_compile.sh\n</code></pre> <p>This script sets the correct environment variables and reconfigures PETSc with Apple Silicon-compatible flags.</p> Manual fix (if the script does not work) <ol> <li> <p>Set environment variables in your shell config (<code>~/.zshrc</code> or <code>~/.bashrc</code>):</p> <pre><code>export SDKROOT=$(xcrun --show-sdk-path)\nexport CC=mpicc\nexport CXX=mpicxx\nexport FC=mpifort\nexport F77=mpifort\n</code></pre> </li> <li> <p>Navigate to <code>petsc/</code> and run the configure command manually:</p> <pre><code>cd petsc\n./configure \\\n   PETSC_ARCH=arch-darwin-c-opt \\\n   CC=mpicc \\\n   CXX=mpicxx \\\n   FC=mpifort \\\n   F77=mpifort \\\n   LDFLAGS=\"-L/opt/homebrew/lib -Wl,-w\" \\\n   --with-debugging=0 \\\n   --with-cxx-dialect=14 \\\n   --download-sundials2=1\n</code></pre> </li> <li> <p>Build and test:</p> <pre><code>make PETSC_DIR=$(pwd) PETSC_ARCH=arch-darwin-c-opt all\nmake PETSC_DIR=$(pwd) PETSC_ARCH=arch-darwin-c-opt check\n</code></pre> </li> </ol> <p>This has been tested on M1 Ultra (macOS Sequoia 15.6.1) and M2 (macOS 26.2).</p>"},{"location":"troubleshooting.html#petsc-tests-error-network-configuration","title":"PETSc tests error (network configuration)","text":"<p>Error when running the PETSc tests:</p> <pre><code>Fatal error in PMPI_Init_thread: Other MPI error, error stack:\nMPIR_Init_thread(467)...............:\nMPID_Init(177).....................: channel initialization failed\nMPIDI_CH3_Init(70).................:\nMPID_nem_init(319).................:\nMPID_nem_tcp_init(171).............:\nMPID_nem_tcp_get_business_card(418):\nMPID_nem_tcp_init(377).............: gethostbyname failed, localhost (errno 3)\n</code></pre> <p>This is a network configuration issue. To fix it, add the following to <code>/etc/hosts</code>, where <code>computername</code> is your hostname:</p> <pre><code>127.0.0.1   computername.local\n127.0.0.1   computername\n</code></pre> <p>Then enable Remote Login in your Sharing settings and add your user to the allowed access list.</p>"},{"location":"troubleshooting.html#errors-during-petsc-configuration-or-compilation-x86-header","title":"Errors during PETSc configuration or compilation (x86 header)","text":"<p>One of the following errors during PETSc configuration or compilation:</p> <pre><code>This header is only meant to be used on x86 and x64 architecture\n#error \"This header is only meant to be used on x86 and x64 architecture\n</code></pre> <p>Install Python via Homebrew and use pip to manage packages, rather than Conda.</p>"},{"location":"troubleshooting.html#errors-during-the-socrates-compilation","title":"Errors during the SOCRATES compilation","text":"<p>One of the following errors happen during the SOCRATES compilation:</p> <ol> <li><code>clang (LLVM option parsing): Unknown command line argument '-x86-pad-for-align=false'.  Try: 'clang (LLVM option parsing) --help'</code></li> <li><code>clang (LLVM option parsing): Did you mean '--x86-slh-loads=false'?</code></li> </ol> <p>There is an issue with your compiler, either the standard Apple <code>clang</code> or <code>gcc</code> installed by <code>brew</code>. Follow the steps provided at the StackOverflow thread:</p> <pre><code>sudo rm -rf /Library/Developer/CommandLineTools\nsudo xcode-select --install\n</code></pre>"},{"location":"troubleshooting.html#fastchem-compilation-with-apple-silicon","title":"FastChem compilation with Apple Silicon","text":"<p>With Apple Silicon hardware (M1/M2/M3), the option <code>-march=native</code> sometimes causes issues.</p> <p>Make sure to use the GNU version of <code>g++</code>, not the Apple one. The Apple version located at <code>/usr/bin/gcc</code> is actually a wrapper around <code>clang</code>.</p> <p>The Homebrew version located at <code>/opt/homebrew/bin/</code> works well. Find out which <code>gcc</code> version Homebrew installed (<code>ls /opt/homebrew/bin/gcc-*</code>), and edit the file <code>make.globaloptions</code> in the FastChem directory to use, e.g. <code>g++-12</code> or <code>g++-13</code> instead of <code>g++</code>.</p>"},{"location":"troubleshooting.html#python-netcdf-library-error","title":"Python netCDF library error","text":"<p>Error: <code>Library not loaded: '@rpath/libcrypto.3.dylib'</code></p> <p>Create a symlink in the local Python installation. See this guide for details.</p> <pre><code>brew install openssl\n</code></pre> <p>Follow the instructions at the end of the <code>openssl</code> installation (replace <code>USERNAME</code> with your own system username):</p> <pre><code>echo 'export PATH=\"/usr/local/opt/openssl@3/bin:$PATH\"' &gt;&gt; /Users/USERNAME/.bash_profile\necho 'export LDFLAGS=\"-L/usr/local/opt/openssl@3/lib\"' &gt;&gt;/Users/USERNAME/.bash_profile\necho 'export CPPFLAGS=\"-I/usr/local/opt/openssl@3/include\"' &gt;&gt;/Users/USERNAME/.bash_profile\nln -s /usr/local/opt/openssl/lib/libcrypto.3.dylib /Users/USERNAME/opt/anaconda3/envs/proteus/lib/python3.10/site-packages/netCDF4/../../../\nln -s /usr/local/opt/openssl/lib/libssl.3.dylib /Users/$USER/opt/anaconda3/envs/proteus/lib/python3.10/site-packages/netCDF4/../../../\n</code></pre>"},{"location":"troubleshooting.html#modulenotfounderror-no-module-named-yaml","title":"<code>ModuleNotFoundError: No module named 'yaml'</code>","text":"<p>Install <code>yaml</code> via pip rather than via conda:</p> <pre><code>python -m pip install pyyaml\n</code></pre>"},{"location":"troubleshooting.html#missing-ifort-compilers","title":"Missing <code>ifort</code> compilers","text":"<p>If the SOCRATES installation steps complain about missing <code>ifort</code> compilers:</p> <p>Install Intel compilers from the Intel oneAPI Toolkit page. First install Intel oneAPI Base Toolkit, then Intel oneAPI HPC Toolkit. Follow the instructions provided after the installation to set the locations of <code>ifort</code> in your environment.</p>"},{"location":"troubleshooting.html#modulenotfounderror-no-module-named-_tkinter","title":"<code>ModuleNotFoundError: No module named '_tkinter'</code>","text":"<p>Install the <code>tkinter</code> package using <code>brew</code>:</p> <pre><code>brew install python-tk\n</code></pre>"},{"location":"troubleshooting.html#tkagg-backend-x-forwarding-error","title":"TkAgg backend / X-forwarding error","text":"<p>Error: <code>ImportError: Cannot load backend 'TkAgg' which requires the 'tk' interactive framework, as 'headless' is currently running</code></p> <p>If you are connecting to a computer over SSH, make sure to enable X-forwarding. This can be done in your SSH configuration file or as a command line parameter. Install XQuartz on your Mac.</p>"},{"location":"troubleshooting.html#yaml-library-error","title":"YAML library error","text":"<p>Error: <code>ld: unsupported tapi file type '!tapi-tbd' in YAML file '/Library/Developer/CommandLineTools/SDKs/MacOSX13.sdk/usr/lib/libSystem.tbd' for architecture arm64</code></p> <p>There is an issue with your <code>ld</code>, potentially caused by an existing installation of <code>anaconda</code>. Delete all traces of <code>anaconda</code> by following the steps at the Anaconda uninstall guide, then install Python via <code>brew</code>.</p>"},{"location":"troubleshooting.html#gfortran-cannot-find-system-libraries","title":"gfortran cannot find system libraries","text":"<p>Error: <code>ld library not found for -lsystem</code></p> <p>This usually occurs when your operating system has been updated, but Xcode is not on the latest version. Try updating or reinstalling Xcode.</p> <p>Alternatively, try adding the following flag to the gfortran compile and link steps:</p> <pre><code>-L/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/lib/\n</code></pre>"},{"location":"troubleshooting.html#linux","title":"Linux","text":""},{"location":"troubleshooting.html#cannot-compile-petsc-on-compute-cluster","title":"Cannot compile PETSc on compute cluster","text":"<p>There are various reasons that PETSc might have problems compiling. On a cluster, this could be caused by a mismatch between your MPICH version and SLURM. This is a known problem on Snellius.</p> <p>Try loading the OpenMPI module:</p> <pre><code>module load mpi/openmpi-x86_64\n</code></pre>"},{"location":"troubleshooting.html#cannot-compile-petsc-error-moving-libraries-fedorarhel","title":"Cannot compile PETSc, error moving libraries (Fedora/RHEL)","text":"<p>If you are using Fedora or RedHat Linux distributions, you might encounter an error:</p> <pre><code>Error moving\n  /home/$USER/PROTEUS/petsc/arch-linux-c-opt/externalpackages/f2cblaslapack-3.8.0.q2\n  libraries\n</code></pre> <p>To resolve, follow these steps:</p> <ol> <li>Install libraries:     <pre><code>sudo dnf install lapack lapack-devel lapack-static sundials-mpich \\\n    openmpi openmpi-devel f2c f2c-libs\n</code></pre></li> <li>Enable module utility: <code>source /etc/profile.d/modules.sh</code></li> <li>Load OpenMPI module: <code>module load mpi/openmpi-x86_64</code></li> <li>Add the commands from steps 2 and 3 to your <code>~/.bashrc</code> file.</li> <li>Run the <code>get_petsc.sh</code> script again.</li> </ol>"},{"location":"troubleshooting.html#netcdffso-not-found","title":"<code>netcdff.so</code> not found","text":"<p>This happens when SOCRATES cannot find the NetCDF-Fortran library. Make sure that it's installed. You can also try adding the library location to your <code>LD_LIBRARY_PATH</code> variable. Find the location of the NetCDF-Fortran library using:</p> <pre><code>nf-config --flibs\n</code></pre>"},{"location":"usage.html","title":"Usage","text":"<p>This page describes how to use PROTEUS. The framework can be run standalone, as a grid of simulations, or as a forward model within a retrieval framework. In all cases you will need to configure the model via a 'configuration file', which you can read about in a dedicated page here. If you encounter any problems, please visit the troubleshooting page.</p> <p>Quick start</p> <p><pre><code>proteus start -c input/all_options.toml\n</code></pre> Results appear in <code>output/all_options/</code>. See Output and results for details.</p> <p>We start by describing how to run a single instance of PROTEUS...</p>"},{"location":"usage.html#running-proteus-from-the-terminal","title":"Running PROTEUS from the terminal","text":"<p>PROTEUS has a command-line interface (CLI) that can be accessed by running <code>proteus</code> on the command line. Try <code>proteus --help</code> to see the available commands!</p> <p>You can directly run PROTEUS using the command:</p> <p><pre><code>proteus start -c [cfgfile]\n</code></pre> Where <code>[cfgfile]</code> is the path to the required configuration file. Pass the flag <code>--resume</code> in to resume the simulation from the disk.</p> <p>A good first test is to run the <code>all_options.toml</code> config, which is located in the <code>input</code> folder:</p> <p><pre><code>proteus start -c input/all_options.toml\n</code></pre> This will run a simulation and write the results to the <code>output/</code> folder inside your PROTEUS directory.</p> <p>See the config guide for information on how to edit the configurations files, and an explanation of their structure.</p> <p>PROTEUS will automatically check if any lookup-tables or data need to be downloaded for it to run. To disable this functionality, pass the <code>--offline</code> flag to the <code>proteus start</code> command shown above.</p>"},{"location":"usage.html#output-and-results","title":"Output and results","text":"<p>Simulations with PROTEUS create several types of output files. For the <code>all_options</code> example, the results are located at <code>output/all_options/</code>. The tree below outlines the purposes of some files and subfolders contained within the simulation's output folder.</p> <pre><code>all_options/\n \u251c\u2500runtime_helpfile.csv         &lt;---- table containing the main simulation results\n \u251c\u2500proteus_00.log               &lt;---- the log file from the simulation\n \u251c\u2500init_coupler.toml            &lt;---- a completed copy of the configuration file\n \u251c\u2500status                       &lt;---- status of the simulation\n \u251c\u2500data/\n \u2502 \u251c\u2500files ending in _atm.nc    &lt;---- atmosphere data\n \u2502 \u251c\u2500files ending in .json      &lt;---- interior data\n \u2502 \u2514\u2500data.tar                   &lt;---- atmosphere &amp; interior data archive\n \u251c\u2500observe/\n \u2502 \u2514\u2500files ending in .csv       &lt;---- synthetic/simulated observations of the planet\n \u251c\u2500offchem/\n \u2502 \u2514\u2500vulcan.csv                 &lt;---- atmospheric mixing ratios calculated with VULCAN\n \u251c\u2500plots\n \u2502 \u251c\u2500plot_chem_atmosphere.png   &lt;---- plot of atmospheric mixing ratios\n \u2502 \u251c\u2500plot_escape.png            &lt;---- plot of volatile inventories over time\n \u2502 \u251c\u2500plot_global_log.png        &lt;---- plot containing an overview of the simulation\n \u2502 \u2514\u2500other files                &lt;---- any other plots\n</code></pre>"},{"location":"usage.html#running-proteus-on-remote-machines-servers","title":"Running PROTEUS on remote machines / servers","text":"<p>Using PROTEUS on a remote machine (e.g. Habrok, the Kapteyn cluster, etc.) is best done through tmux. Tmux allows you to leave programs running in the 'background' for long periods of time. You can find detailed documentation here.</p> <ul> <li>For example, you can start a new tmux session with the command:     <pre><code>tmux new -s &lt;session_name&gt;\n</code></pre></li> <li>Inside the tmux session, start your simulation:     <pre><code>proteus start -c input/all_options.toml\n</code></pre></li> <li>To detach from the session, press <code>Ctrl + b</code>, then <code>d</code>. You can reattach to the session later with:     <pre><code>tmux attach -t &lt;session_name&gt;\n</code></pre></li> <li>To list all tmux sessions, use:     <pre><code>tmux ls\n</code></pre></li> <li>To kill a tmux session, use:     <pre><code>tmux kill-session -t &lt;session_name&gt;\n</code></pre></li> <li>The above started simulation will store the output data in the PROTEUS <code>output/</code> folder. You can check the progress of the simulation by looking at the log files in this folder. The log files are named according to the simulation name and contain information about the simulation's progress and any errors that may have occurred.</li> <li>If you want to check if you are using CPUs on the cluster, use the command:     <pre><code>htop -u $USER\n</code></pre></li> <li>Press <code>Ctrl + c</code> to exit the <code>htop</code> command.</li> </ul>"},{"location":"usage.html#running-grids-of-simulations-ensembles","title":"Running grids of simulations (ensembles)","text":"<p>It is often useful to run grids of forward models, where each point in a grid represents a different set of parameters. This can also be done using the command line interface. For example:</p> <pre><code>proteus grid -c input/ensembles/example.grid.toml\n</code></pre> <p>Configure a grid of your choosing by creating a TOML file which specifies the grid's axes and determines how it should be run. An example configuration file for a PROTEUS grid is available at <code>input/ensembles/example.grid.toml</code>, which uses the dummy configuration file as a \"reference\" and then modifies it for every combination of the parameters in the <code>.grid.toml</code> file.</p> <p>Grids can be dispatched with or without using a workload manager. In PROTEUS, we use the Slurm workload manager, which can allow running large ensembles of models on high-performance compute clusters. The subsections below detail cases with/without Slurm.</p> Without Slurm With Slurm Config setting <code>use_slurm = false</code> <code>use_slurm = true</code> Process management PROTEUS manages subprocesses Slurm manages jobs PROTEUS must stay open? Yes No Where to run Servers, multicore desktops HPC clusters (Habrok, Snellius)"},{"location":"usage.html#without-slurm","title":"Without Slurm","text":"<p>Firstly, set <code>use_slurm = false</code>. In this case, the GridPROTEUS routine will manage the individual subprocesses which compose the grid. The variable <code>max_jobs</code> specifies the maximum number of CPU cores which should be utilised by the grid at any one time. This is limited by the number of CPU cores available on your machine. This method works without Slurm, and can be applied on servers or on multicore personal computers.</p> <p>In this case, you will need to make sure that PROTEUS stays open in order to manage its subprocesses.</p>"},{"location":"usage.html#with-slurm","title":"With Slurm","text":"<p>Alternatively, you can access high performance compute nodes through the Slurm workload manager (e.g. on Habrok and Snellius). This is a two-step process. To do this, set <code>use_slurm = true</code> in your grid's configuration file. Then set <code>max_mem</code> and <code>max_days</code> to specify how much memory should be allocated to each job (each simulation). These values are nominally 3 GB and 2 days. Ensure that these values are within the limits of the server you are working on.</p> <p>With these options enabled, running PROTEUS will produce a script called <code>slurm_dispatch.sh</code> in the specified output folder, as well as write the required configuration files to a subfolder called <code>cfgs/</code>.</p> <p>To dispatch your grid via Slurm, you must then run the command <code>sbatch &lt;path&gt;</code> where <code>&lt;path&gt;</code> is the path to the dispatch script created by the <code>proteus grid</code> command. You will be prompted to do this in the terminal.</p> <p>Monitor your running jobs with <code>squeue -u $USER</code>. To cancel all of your running jobs, use <code>scancel -u $USER</code>. The original PROTEUS process does not need to stay open when using Slurm to manage the subprocesses.</p>"},{"location":"usage.html#viewing-grid-status","title":"Viewing grid status","text":"<p>Use the CLI to view the status of a grid, such as to check cases which are finished. For example, the command below will summarise the top-level statuses of the demo grid.</p> <pre><code>proteus grid-summarise -o output/grid_demo/\n</code></pre> <p>Add <code>-s</code> find out which cases have a particular status. For example, the command below will list all completed cases.</p> <pre><code>proteus grid-summarise -o output/grid_demo/ -s completed\n</code></pre>"},{"location":"usage.html#packaging-grid-results","title":"Packaging grid results","text":"<p>Use the CLI to package the results of a grid into a zip file; e.g. for sharing or backing-up. The command below will create <code>pack.zip</code> in the <code>grid_demo/</code> folder. This does not store all the data for each case - only the most important files. For example the 1D resolved interior and atmospheric data are lost. Files containing global parameters and auto-generated plots are preserved.</p> <pre><code>proteus grid-pack -o output/grid_demo/\n</code></pre>"},{"location":"usage.html#retrieval-scheme-bayesian-optimisation","title":"Retrieval scheme (Bayesian optimisation)","text":"<p>Retrieval methods efficiently sample a given parameter space in order to find the point at which a forward model best matches some observations. These methods has seen success in recent years, and are often more efficient than naive grid-search methods. However, retrieval schemes usually require that a forward model is fast and inexpensive to run. Bayesian Optimisation is one approach to parameter retrievals; you can read more about it in this article.</p> <p>We have included a retrieval scheme within PROTEUS ref. To use our Bayesian optimisation scheme, please see the instructions on its dedicated page here.</p>"},{"location":"usage.html#postprocessing-of-results-with-offline-chemistry","title":"Postprocessing of results with 'offline' chemistry","text":"<p>PROTEUS includes an \"offline\" chemistry functionality, which uses results of a simulation as an input to the VULCAN chemical kinetics model, capturing the additional physics.</p> <p>Access the offline chemistry via the command line interface:</p> <p><pre><code>proteus offchem -c [cfgfile]\n</code></pre> This will run VULCAN as a subprocess. This command should not be used in batch processing.</p> <p>PROTEUS will perform this step automatically when the configuration variable <code>atmos_chem.when</code> is set to <code>\"offline\"</code>.</p>"},{"location":"usage.html#postprocessing-of-results-with-synthetic-observations","title":"Postprocessing of results with synthetic observations","text":"<p>Similarly to the offline chemistry, PROTEUS results can be postprocessed to generate synthetic observations. Transmission and emission spectra are generated based on the modelled temperature-pressure profile, as well as atmospheric composition. The composition can be set by the output of the offline chemistry calculation (see config file).</p> <p>Access the synthetic observation functionality via the command line interface:</p> <pre><code>proteus observe -c [cfgfile]\n</code></pre> <p>PROTEUS will perform this step automatically if enabled in the configuration file.</p>"},{"location":"usage.html#postprocessing-of-results-with-agni-for-multiprofile-analysis","title":"Postprocessing of results with AGNI for multiprofile analysis","text":"<p>PROTEUS includes a functionality to postprocess the planet's atmosphere for a number of zenith angles. This allows the user to obtain localized thermal profiles based on the angle of irradation on the atmosphere,  this is particularly useful for first-hand results on tidally locked planets.</p> <p>Access the atmospheric postprocessing functionality via the command line interface, while in <code>PROTEUS</code>:</p> <p><pre><code>julia tools/multiprofile_postprocess.jl output/[outputdir] 0,36,45,89\n</code></pre> This example finds results for 4 zenith angles, namely [0,36,45,89], but the script works for any number of zenith angles and creates a new <code>..._atm_z{angle}.nc</code> file in the <code>data</code> directory, within the output directory, for each angle.</p>"},{"location":"usage.html#archiving-output-files","title":"Archiving output files","text":"<p>Running PROTEUS can generate a large number of files, which is problematic when also running large grids of simulations. To counter this, the <code>params.out.archive_mod</code> configuration option can be used to tell PROTEUS when to archive its output files. This will gather the output files of each run into <code>.tar</code> files.</p> <p>Archiving the output files makes them inaccessible for analysis or plotting. Extract the archives from a run using the proteus command line interface: <pre><code>proteus extract-archives -c [cfgfile]\n</code></pre></p> <p>This is reversible. To pack the data files into <code>.tar</code> archives again: <pre><code>proteus create-archives -c [cfgfile]\n</code></pre></p>"},{"location":"usage.html#version-checking","title":"Version checking","text":"<p>The <code>proteus doctor</code> command helps diagnose potential issues with your PROTEUS installation. It tells you about outdated or missing packages, and whether all environment variables have been set.</p> <pre><code>$ proteus doctor\nPackages\naragog: ok\nfwl-calliope: ok\nfwl-janus: ok\nfwl-proteus: ok\nfwl-mors: ok\nfwl-zephyrus: ok\nfwl-zalmoxis: ok\nAGNI: Update available 1.7.1 -&gt; Ledoux, oceans, water, clouds, and blackbody stars\n\nEnvironment variables\nFWL_DATA: ok\nRAD_DIR: ok\n</code></pre>"},{"location":"user_install.html","title":"User Install (Deprecated)","text":"<p>Deprecated</p> <p>This installation method is deprecated. We recommend following the main Installation guide instead, which provides a full developer setup with editable submodule installations.</p>"},{"location":"user_install.html#requirements","title":"Requirements","text":"<ol> <li>conda (miniconda or miniforge)</li> <li>git (install via conda if needed: <code>conda install git</code>)</li> <li>Julia installation (see Installation guide)</li> <li>~20 GB of disk space</li> <li>~30 minutes of your time (mostly waiting)</li> </ol> <p>miniforge and HDF5/NetCDF conflicts</p> <p>There is currently a known conflict between Julia and Conda versions of HDF5 and/or NetCDF libraries when using miniforge. If you encounter issues, try using miniconda instead.</p>"},{"location":"user_install.html#steps","title":"Steps","text":"<ol> <li><code>git clone https://github.com/FormingWorlds/PROTEUS.git</code></li> <li><code>cd PROTEUS</code></li> <li><code>conda env create -f environment.yml</code></li> <li><code>conda activate proteus</code></li> <li><code>pip install -e .</code></li> <li><code>proteus install-all --export-env</code></li> </ol> <p>macOS: PETSc compilation on Apple Silicon</p> <p>If <code>proteus install-all</code> fails during the PETSc step on Apple Silicon (M1/M2/M3/Ultra), run the automated fix script:</p> <pre><code>./tools/fix_petsc_compile.sh\n</code></pre> <p>See the Troubleshooting page for details.</p>"},{"location":"user_install.html#before-running-proteus","title":"Before running PROTEUS","text":"<p>If you want to start running PROTEUS right away (from the same shell that you used to install PROTEUS), reload your environment variables:</p> bashzsh <pre><code>source ~/.bashrc\nconda activate proteus\n</code></pre> <pre><code>source ~/.zshrc\nconda activate proteus\n</code></pre> <p>If you chose <code>proteus install-all</code> without <code>--export-env</code>, you will need to set your <code>PATH</code>, <code>FWL_DATA</code>, Julia, and <code>RAD_DIR</code> environment variables manually.</p> <p>When you log into the machine where you installed PROTEUS through <code>proteus install-all --export-env</code>, the environment variables <code>FWL_DATA</code> and <code>RAD_DIR</code> will already be set (because your shell config file was updated during installation). You still need to run <code>conda activate proteus</code>.</p>"},{"location":"user_install.html#updating-proteus","title":"Updating PROTEUS","text":"<ol> <li><code>conda activate proteus</code> (if not already activated)</li> <li><code>proteus update-all</code></li> </ol>"},{"location":"paper/paper.html","title":"Summary","text":"<p>PROTEUS is a modular numerical framework designed to tackle the interdisciplinary challenge of understanding the coupled evolution of the atmospheres and interiors of rocky planets and exoplanets over geologic timescales. It iteratively couples the numerical solutions of interoperable physical and chemical modules, each of which are designed to describe a specific component of the planet and its environment. Processes considered are, for example, atmospheric radiative transfer, stellar evolution, volatile in- and outgassing, and mantle convection. By employing an evolutionary framework, PROTEUS is able to resolve how interior-atmosphere history leads to hysteresis in planetary composition, climate, and structure that steady-state approaches cannot disambiguate. Its modularity allows robust physical and numerical tests against known semi-analytic solutions and empirical evidence on the level of both individual processes and the interconnected planet system. The current primary use case of PROTEUS is the simulation of the coupled geophysical and climatic evolution of individual and ensembles of rocky (exo-)planets from their primordial magma ocean phase to either global energy balance equilibrium, mantle solidification, or complete atmospheric escape. Simulation results can be aimed at advancing our theoretical understanding of planetary evolution or be compared against current and future astronomical observations. Through its modular implementation, PROTEUS offers multiple avenues to extend its functionality and use-cases in the future, for example toward more volatile-rich planets, solid-state geodynamics, prebiotic and biotic chemistry, and statistical inference 'retrieval' methods.</p>","tags":["astronomy","exoplanets","planetary science","geophysics","atmospheric science","geochemistry","geodynamics","planetary evolution","lava planets"]},{"location":"paper/paper.html#background","title":"Background","text":"<p>Advances in astronomical instrumentation, such as the launch of the James Webb Space Telescope (JWST), now enable the spectral characterization of low-mass extrasolar planets, in particular so-called super-Earths and sub-Neptunes [@kempton24], which have no Solar System analogues. Many of these exoplanets orbit very close to their star, are highly irradiated, and have eccentric orbits that drive tidal heating [@zhu21]. These conditions create thermodynamic regimes that are potentially similar to those created by the climatic and geodynamic state of the primitive Earth after its formation that are likely universal to low-mass planets. The short period planet population therefore enables direct observational access to highly energetic phases of planetary evolution, for instance magma ocean stages and runaway greenhouse states. These extreme geodynamic and climatic regimes governed the early, formative, phases of the terrestrial planets, but are inaccessible to direct observation at present day in the Solar System [@lichtenberg23].</p> <p>Characterizing the thermodynamic and climatic properties of these exoplanets in fully or partially molten regimes will yield critical insights on the conditions that governed the formation of the earliest atmospheres of the terrestrial planets and built the background environment of the origin of life as we know it [@lichtenberg25]. Resolving the physical origin of the diversity in the observed exoplanet population is central to predicting the initial, abiotic, landscape of terrestrial worlds. Achieving key insights in this direction will require contextual interpretation on the level of both individual planets and population-level trends, such as the radius valley or prevalence of secondary atmospheres on low-mass exoplanets. Only by enhancement of our understanding of the planetary context that frameworks like PROTEUS provide can we build up a quantitative picture of abiotic planetary processes that will allow the eventual robust identification of life on other worlds [@apai25; @seager2025].</p>","tags":["astronomy","exoplanets","planetary science","geophysics","atmospheric science","geochemistry","geodynamics","planetary evolution","lava planets"]},{"location":"paper/paper.html#statement-of-need","title":"Statement of need","text":"<p>The atmospheric, surficial, and geologic conditions during magma ocean epochs arise from feedback between multiple coupled and non-linear processes, which include mantle melting and crystallization, geochemical evolution, outgassing, greenhouse forcing, condensation, and atmospheric escape [@lichtenberg23]. In- and outgassing of atmospheric volatiles and energy transfer through the planetary mantle and atmosphere create interconnected feedback loops that lead to hysteresis of the planetary climate and interior on billion-year timescales [@nicholls25c].</p> <p>This emergent property of coupled interior-atmosphere systems is illustrated by the discussion surrounding the 'runaway greenhouse' climate state for Earth and Venus, and for extrasolar planets. If low-mass planets are modelled in a steady-state with a pre-exisiting water ocean and freely chosen atmospheric compositions [@yang13; @way16; @selsis23; @madhusudhan23], it is found that such planets can retain their water inventory and remain 'habitable' on geologic timescales. However, if modelled as starting in a hot magma ocean state, as predicted by planetary formation scenarios and supported by empirical evidence from the Solar System, these solutions are not recovered [@hamano13; @schaefer16; @kite20a; @kite20b; @lichtenberg21a; @dorn21; @shorttle24; @nicholls25c; @boer25]; instead long-lived magma oceans are found to be defining features of planetary evolution, in particular under the extreme stellar irradiation of the currently known exoplanet population [@lichtenberg25].</p> <p>One key reason for this model divergence is the emerging feedback loop between molten mantle and atmosphere: atmospheric volatiles, including H\\(_2\\)O, N and S species, are highly soluble in magmatic fluids [@suer23; @sossi23; @schaefer16]. As a result, a planet's silicate mantle, when molten, acts as a significant sink of atmospheric volatiles. This mantle sink is selective, drawing volatiles out of the atmosphere according to their solubility in magmas [@dorn21; @shorttle24; @nicholls25c]. In this way, magma oceans change the energy transfer through the atmosphere by affecting its pressure, opacity, and scattering properties, in turn changing the heat loss or gain of the planet through secular cooling and stellar irradiation. Time-sensitive effects, such as continuing accretion [@itcovitz22; @lichtenberg22; @wogan24b], internal redox processes [@wordsworth18; @kite20b; @lichtenberg21b; @schaefer24], photoevaporation induced by the host star [@rogers21; @cherubim25], or mantle crystallisation [@schaefer_magma_2018], will affect the global planetary equilibrium over time.</p> <p>Planets with similar atmospheric properties may thus harbour order of magnitude different bulk volatile fractions if their interior (core and mantle) phase state is different. This presents a critical degeneracy for astronomical observations aiming to infer compositional and thermodynamic properties of exoplanets from telescopic data. On the other hand, evolutionary hysteresis processes may contribute to resolving observational degeneracies: the magma solubilities of C, N and S species are highly sensitive to the compositional properties of the planetary mantle [@suer23; @lichtenberg21a; @shorttle24; @nicholls25a], hence planets with different geochemistries may be distinguished by matching astronomical observations with time-resolved solutions that connect geochemical with atmospheric considerations over geological timescales. Providing these geophysical and climatic predictions and enabling quantitative comparison with empirical data from exoplanet astronomy is the primary purpose of the PROTEUS framework in its present form.</p>","tags":["astronomy","exoplanets","planetary science","geophysics","atmospheric science","geochemistry","geodynamics","planetary evolution","lava planets"]},{"location":"paper/paper.html#framework-modularisation","title":"Framework &amp; modularisation","text":"<p>Improved precision in analytical and observational methods, for example through high-resolution spectra and more precise masses and radii of exoplanets, motivates correspondingly more sophisticated physical and chemical models to interpret them. Life as a hypothesis of last resort [@sagan1993] demands abiotic models that capture the true diversity and evolutionary contingency of abiotic processes. Hence, we need to achieve a level of model complexity that is scalable to the data quality and the question being asked of the data. As modern research software environments grow correspondingly, they become more difficult to maintain and verify. With a growing user and developer base, sufficient documentation and tutorials become challenging to update on a continuously evolving basis.  From an organisational perspective, term-limited research projects and changing institutions of researchers present challenges for code consistency. The PROTEUS framework attempts to tackle these scientific, technical, and social challenges by modularising its software ecosystem: physical and chemical processes and sub-systems are isolated and maintained in separate <code>git</code> repositories, each with their own verification system through automated testing and documentation.</p> <p>The description in this manuscript relates to PROTEUS version 25.07.31, however, we encourage the reader to refer to the most up-to-date release version at any time. PROTEUS uses TOML to structure its configuration files, providing a human-readable input format. Input parameters are verified, some are conditional to others. Most of the modules are Python packages and easy to install and use standalone via <code>pip</code>. PROTEUS provides functionality to run singular simulations and grids of parameter sweeps to enable exploration of parameter sensitivity and population studies. Larger input data files, such as tabulated equations of state, opacities, and stellar spectra, are stored at Open Science Framework and Zenodo and automatically downloaded and verified when the user runs the code for the first time. Installation, usage, configuration, and contributing information is outlined in the Documentation. PROTEUS runs natively on Linux and MacOS computer systems, and has been tested to run on larger computer cluster architectures, interfacing with queuing managers, such as <code>slurm</code>.</p> <p>From a software engineering perspective, PROTEUS aims to externalise all modelled physics and chemistry to interoperable modules. Some of the advantages of this approach are:</p> <ul> <li>Modules can be updated and maintained independently. Each module is self-sufficient and can be executed standalone, which enhances developer experience and usability.</li> <li>Modules can be combined in different ways to create different approaches, which enables the framework to be adapted to a wide range of research questions.</li> <li>Modules can be exchanged with other modules to test the sensitivity of different approaches to the same problem, which enables a more robust understanding of the underlying physics and chemistry.</li> </ul> <p>PROTEUS is thus in principle interoperable with a variety of external computer codes that fit into the framework designation. In some instances, this enables integration and extension of pre-existing codes, preventing researchers from continuously 'reinventing the wheel' of their scientific domain.</p> <p></p> <p>\\autoref{fig:schematic} shows the current state of the PROTEUS framework at the time of submission, including its ecosystem of modules, as previously introduced in @lichtenberg21a; @nicholls24; @nicholls25a; @nicholls25c. Several of the currently existing modules (in addition to the PROTEUS framework itself) have been developed from scratch for their primary use as module within PROTEUS. Other modules are specialised codes, which were originally developed standalone, and have been adapted and extended to work with the PROTEUS framework.</p> <p>Modules are grouped into five main categories: (i) interior, (ii) atmosphere, (iii) interface, (iv) environment, and (v) interpretation.</p> <p>Interior modules (i) compute the interior structure as well as the thermal and chemical evolution of the planetary mantle and core, such as energy transport, melting, and crystallization (red boxes in \\autoref{fig:schematic}). These include:</p> <ul> <li>Aragog and SPIDER [@bower18; @sastre25], which describe the interior heat transport of partially molten planets using a temperature and an entropy formalism, respectively.</li> <li>LovePy [@hay19; @nicholls25c], which simulates mixed-phase tidal heating in the planetary mantle.</li> <li>Zalmoxis [@pascal26], which calculates the interior (core and mantle) structure and gravity profile.</li> </ul> <p>Atmosphere modules (ii) compute the energy balance and composition of the planetary atmosphere, including radiative transfer, and atmospheric chemistry (blue boxes in \\autoref{fig:schematic}). These include:</p> <ul> <li>AGNI [@nicholls25a; @nicholls25b], which describes the atmosphere energy balance using a radiative-convective model and surface reflection properties from laboratory data [@hammond25].</li> <li>JANUS [@graham21; @graham22], which treats the atmosphere energy balance using a multicomponent non\u2010dilute pseudoadiabat.</li> <li>SOCRATES [@manners2024fast], which calculates radiative fluxes from atmospheric temperature and composition.</li> <li>FASTCHEM [@kitzmann24], which models equilibrium atmospheric chemistry; implemented as post-processing option.</li> <li>VULCAN [@tsai17; @tsai21], which simulates disequilibrium atmospheric chemistry; implemented as post-processing option.</li> </ul> <p>Interface modules (iii) compute exchange between two or more planetary layers, including surface-atmosphere interactions and mass loss to space (green boxes in \\autoref{fig:schematic}). These include:</p> <ul> <li>CALLIOPE [@bower22; @shorttle24; @nicholls25a], which simulates the redox-, temperature-, and pressure-controlled in- and outgassing of C-H-N-O-S volatile elements.</li> <li>ZEPHYRUS [@postolec25], which calculates the escape of the atmosphere to space.</li> </ul> <p>Environment modules (iv) compute the evolution of the host star, including its luminosity and spectral energy distribution (yellow boxes in \\autoref{fig:schematic}):</p> <ul> <li>MORS [@johstone21], which models the evolution of rotation, luminosity, and high energy emission of stars.</li> </ul> <p>Interpretation modules (v) compute observational properties of the planet, such as emission and transmission spectra, planet-to-star contrast ratio, and bulk density (purple boxes in \\autoref{fig:schematic}):</p> <ul> <li>PLATON [@zhang19; @zhang20], which simulates synthetic telescopic observations of exoplanets; implemented as post-processing option.</li> </ul> <p>While module categories (iv) and (v) so far only contain one module, we anticipate extending these categories in the future with capabilities related to orbital dynamics, accretion, and telescope simulators. All module repositories are linked to the central PROTEUS framework repository, which provides a single entry point for users to access the entire PROTEUS ecosystem.</p>","tags":["astronomy","exoplanets","planetary science","geophysics","atmospheric science","geochemistry","geodynamics","planetary evolution","lava planets"]},{"location":"paper/paper.html#discussion-of-similar-codes","title":"Discussion of similar codes","text":"<p>With the advent of increasing observational precision in exoplanet characterization, and the focus on smaller planets that approach Earth-like radii and densities, there has been increased development of coupled atmosphere-interior simulation codes. Our discussion here focusses specifically on the key traits of (a) time evolution of the planet, (b) coupling between the interior and atmosphere (i.e., a change in one system must dynamically affect other system properties), and (c) the planetary mantle and atmosphere must be described in some fashion that enables quantification of mantle crystallization timescales and changes in atmospheric pressure and/or composition. With this definition, a growing number of codes, mostly proprietary, have been developed over the past few years [@schaefer16; @hamano13; @bower22; @lichtenberg21a; @krissansentotton21; @tang24; @cherubim25; @barnes20; @kite20a; @maurice24; @lebrun13; @salvador17; @carone25; @farhat25; @sahu25]. The majority of these codes are built on the principles developed by @elkinstanton08, but each have their own unique implementations and methodologies.</p> <p>It is critically important for the exploration of the exoplanet census and refined understanding of the deep history of the terrestrial planets that a variety of independent models are developed, optimally in an open source fashion, so that individual approaches can be compared against one another, and the community can learn from each other and thus produce better and more robust science. A detailed comparison with these codes would go beyond the scope of this article. Hence, we here limit our discussion to the traits that we believe are the unique capabilities and implementation aspects of PROTEUS. These are:</p> <ul> <li>Its modularised approach, with the software engineering and technical advantages described above.</li> <li>The ability to spatially model the planet (so far in 1-D) from the core-mantle boundary to the top of the atmosphere. Critically, this enables the quantification of thermal evolution scenarios, which depend on the energetics of the interior and atmosphere through the transport of material and energy by melt, solid, and gas phases.</li> <li>The wide variety of geochemistries that can be modelled, which are expressed through the redox state at the mantle-atmosphere interface and planetary volatile content, and result in order-of-magnitude variations in atmospheric compositions, which are chemically and energetically self-consistently resolved in the atmospheric modules.</li> <li>The dynamic resolution of interior and atmospheric energy transfers regimes: radiative and convective layers in the atmosphere; two-phase energy and compositional transfer in conductive and (turbulent) convective regimes of the mantle are resolved.</li> <li>True multi-phase evolution of the mantle, where melt and solid phases are resolved on individual nodes, affecting energy transfer and chemical properties.</li> <li>Interconnected atmospheric escape that couples to the planetary interior; i.e., the escaping reservoir is dynamically linked and/or disconnected from the volatile reservoir in the deep interior, depending on evolutionary state and redox properties.</li> <li>Time-resolved evolution of the stellar spectrum and energy flux for a wide array of stellar types directly imprint on atmospheric energy transfer and escape.</li> <li>Inclusion of equilibrium and disequilibrium chemistry in the atmosphere.</li> <li>The inclusion of realistic, measured surface reflection properties for solid and molten surface conditions.</li> <li>On-the-fly computation of observational properties, such as emission and transmission spectrum, planet-to-star contrast ratio, bulk density, and other observational properties of interest.</li> <li>Automated testing of individual modules and the PROTEUS framework as a whole using the GitHub continuous integration platform.</li> <li>A usable and growing set of documentation and tutorials.</li> </ul>","tags":["astronomy","exoplanets","planetary science","geophysics","atmospheric science","geochemistry","geodynamics","planetary evolution","lava planets"]},{"location":"paper/paper.html#verification-documentation","title":"Verification &amp; documentation","text":"<p>PROTEUS implements automated testing and documentation building practices. We use GitHub Actions to automatically run a suite of unit tests, each time code is committed to the public repository or a pull request is opened. The growing test base covers both individual modules within their respective repositories, as well as the PROTEUS framework as a whole. Tests are split into numerical tests, which ensure the numerical integrity, and physical tests, which compare the code against analytical and numerical results, and empirical data from the scientific literature. The documentation and tutorials for PROTEUS can be accessed online.</p>","tags":["astronomy","exoplanets","planetary science","geophysics","atmospheric science","geochemistry","geodynamics","planetary evolution","lava planets"]},{"location":"paper/paper.html#acknowledgements","title":"Acknowledgements","text":"<p>TL acknowledges support from the Netherlands eScience Center (PROTEUS project, NLESC.OEC.2023.017), the Branco Weiss Foundation, the Alfred P. Sloan Foundation (AEThER project, G202114194), and the United States National Aeronautic and Space Administration\u2019s Nexus for Exoplanet System Science research coordination network (Alien Earths project, 80NSSC21K0593). RC and OS acknowledge support from the United Kingdom Science and Technology Facilities Council (grant numbers ST/Y509139/1 and UKRI1184).</p>","tags":["astronomy","exoplanets","planetary science","geophysics","atmospheric science","geochemistry","geodynamics","planetary evolution","lava planets"]},{"location":"paper/paper.html#references","title":"References","text":"","tags":["astronomy","exoplanets","planetary science","geophysics","atmospheric science","geochemistry","geodynamics","planetary evolution","lava planets"]}]}